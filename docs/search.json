[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "",
    "text": "Preface\nThis is a collection of the notebooks making up module 4 in the HSMA programme.\nLinks to the lecture videos and slides can be found below, including for sessions 4A, 4C and 4H, which did not have notebooks.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a---an-introduction-to-machine-learning",
    "href": "index.html#a---an-introduction-to-machine-learning",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4A - An Introduction to Machine Learning",
    "text": "4A - An Introduction to Machine Learning\n\nIn this session we introduce some of the core concepts of AI and Machine Learning, including the concepts of features and labels, overfitting and underfitting and assessing model performance. We also explore some of the different types of machine learning, and practice our understanding of these new concepts by seeing if we can unpick patterns in Dan’s film preferences for “Dan’s Desert Island DVDs”.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#b---logistic-regression",
    "href": "index.html#b---logistic-regression",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4B - Logistic Regression",
    "text": "4B - Logistic Regression\n\nIn this session we’ll begin exploring some of the Machine Learning approaches that we can use, starting with Logistic Regression - a way of fusing together traditional linear regression models with a logistic function to create a powerful classifier model. You’ll see how these models can be implemented in Python, and practice using both the Titanic dataset you saw earlier in the course, as well as a new stroke patient dataset.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#c---ethics-in-ai",
    "href": "index.html#c---ethics-in-ai",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4C - Ethics in AI",
    "text": "4C - Ethics in AI\n\nIn this session we’ll explore some of the key ethical considerations that are fundamental to any machine learning work, as we explore what can (and will) go wrong.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#d---decision-trees-and-random-forests",
    "href": "index.html#d---decision-trees-and-random-forests",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4D - Decision Trees and Random Forests",
    "text": "4D - Decision Trees and Random Forests\n\nIn this session we’ll begin looking at how decision trees are built and how we can use the sklearn implementation of decision trees on our own datasets.\nWe also recap sensitivity (recall), specificity and precision and how to calculate these in sklearn.\n\nNext, we’ll take a look at how we can avoid some of the problems of decision trees by using an ensemble method - random forests.\nWe also find out an easier way of calculating sensitivity, specificity and precision in one function, as well as hearing about a new metric called f1 score, and we learn about the confusion matrix, which is a powerful tool for helping to break down how different models perform.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#e---boosted-trees-for-classification-and-regression",
    "href": "index.html#e---boosted-trees-for-classification-and-regression",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4E - Boosted Trees for Classification and Regression",
    "text": "4E - Boosted Trees for Classification and Regression\n\nIn this session we’ll take a look at a family of models called boosted trees. These are a very powerful type of algorithm that perform extremely well on tabular datasets. The session touches on XGBoost, AdaBoost, CatBoost, Histogram-based gradient boosting classifiers and LightGBM.\n\nNext, we take a look at how decision trees, random forests and boosted trees can also be used when you want to predict a numeric value instead of classifying a sample as a member of one group or another.\nWe also touch on some key parts of data preprocessing so we can work with a new dataset of patient length of stay in the final exercise, covering how to OneHot encode categorical variables to make this data usable with machine learning libraries.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#f---neural-networks",
    "href": "index.html#f---neural-networks",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4F - Neural Networks",
    "text": "4F - Neural Networks\n\nIn this session we’ll be looking at a subfield of AI that has dominated many of the big advancements in AI over the last few years - Deep Learning - as we introduce Neural Networks.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#g---explainable-ai",
    "href": "index.html#g---explainable-ai",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4G - Explainable AI",
    "text": "4G - Explainable AI\n\nIn this part of the Explainable AI session, we explore\n\nwhy explainability is importantant in AI models\nwhat we mean by explainability\nthe difference between correlation and causation\n\n\nNext, we explore\n\nhow we can extract feature importance from a logistic regression model\nhow to interpret the coefficients from logistic regression models\nthe relationship between odds, log odds and probability\n\n\nThen, we move onto\n\nfeature importance for tree-based models with Mean Decrease in Importance (MDI)\nmodel-agnostic feature importance with permutation feature importance (PFI)\n\n\nIn this part of the Explainable AI session, we explore\n\nthe partial dependence plot (PDP)\nthe individual conditional expectation plot (ICE)\nways of enhancing these plots\n\n\nIn this part of the Explainable AI session, we explore\n\nwhat Shapley values are\nhow the shap library allows us to look at global and local feature importance\nhow to create and interpret different shap plots\n\n\nIn the final part of the Explainable AI session, we explore\n\nwhy calculating prediction uncertainty may be useful\nhow to calculate and show prediction uncertainty",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#h---reinforcement-learning",
    "href": "index.html#h---reinforcement-learning",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4H - Reinforcement Learning",
    "text": "4H - Reinforcement Learning\nNote that we’d recommend not looking at the slides until after the first time the reinforcement learning game is played manually.\n\nIn this session we take a look at Reinforcement Learning in a session that will be very different to any you’ve experienced thus far.\nApp Link: https://bergam0t.github.io/ReinforcementLearningGame/\nApp Github Repository: https://github.com/Bergam0t/ReinforcementLearningGame",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#i---synthetic-data-using-smote",
    "href": "index.html#i---synthetic-data-using-smote",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4I - Synthetic Data using SMOTE",
    "text": "4I - Synthetic Data using SMOTE\n\nIn this session we take a look at synthetic data - how to create our own fake but realistic data when we want to augment an underrepresented class, or just use the data instead of our real data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#j---optimising-ml-imputation-feature-engineering-selection-hyperparameters",
    "href": "index.html#j---optimising-ml-imputation-feature-engineering-selection-hyperparameters",
    "title": "HSMA - Machine Learning Notebooks",
    "section": "4J - Optimising ML: Imputation, Feature Engineering & Selection, Hyperparameters",
    "text": "4J - Optimising ML: Imputation, Feature Engineering & Selection, Hyperparameters\n\nUnfortunately the first 5 minutes or so of the lecture was not recorded\nCovering a range of ways to improve your model’s performance, including:\n\nMissing Data Imputation with SimpleImputer and IterativeImputer\nFeature Selection with SequentialFeatureSelector (forward and backward selection) and SelectFromModel (feature importance selection with model coefficients or mean decrease in impurity)\nFeature Engineering\nDataset Splits (train/test/validation, k-fold)\nDealing with Imbalanced Datasets with model parameters\nHyperparameter tuning with exhaustive gridsearch, randomised gridsearch, and the Optuna framework\n\nAdditional areas in the slides, but not covered in the video, are:\n\nensemble models\nsklearn pipelines\nautomatic model selection with the flaml library\nmodel calibration curves (reliability plots)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_1.html",
    "href": "4b_log_reg_tutorial_1.html",
    "title": "1  An Introduction to Classification with Machine Learning",
    "section": "",
    "text": "2 An introduction to classification with machine learning\nIn classification tasks we seek to classify a ‘case’ into one or more classes, given one or more input features. This may be extended to enquiring about probability of classification. Examples of classification include:\nReflection: 1. Can you think of three occasions where people make classifications? 1. Can you think of an example where it might be useful to provide automated classification?",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Classification with Machine Learning</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_1.html#regression-and-logistic-regression",
    "href": "4b_log_reg_tutorial_1.html#regression-and-logistic-regression",
    "title": "1  An Introduction to Classification with Machine Learning",
    "section": "2.1 Regression and logistic regression",
    "text": "2.1 Regression and logistic regression\nWith ordinary regression we are trying to predict a value given one or more input features.\n\n(Flashcard images from machinelearningflashcards.com)\nWith logistic regression we are trying to the predict the probability, given one or more inputs, that an example belongs to a particular class (e.g. pass vs fail in an exam). Our training data has the actual class (which we turn into 0 or 1), but the model returns a probability of a new example being in either class 0 or 1. The logistic regression fit limits the range of output to between 0 and 1 (whereas a linear regression could predict outside of this range).\n\n\\[P = \\dfrac{e^{a+bX}}{1+e^{a+bX}}\\]",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Classification with Machine Learning</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_1.html#import-libraries",
    "href": "4b_log_reg_tutorial_1.html#import-libraries",
    "title": "1  An Introduction to Classification with Machine Learning",
    "section": "2.2 Import libraries",
    "text": "2.2 Import libraries\n\nimport matplotlib.pyplot as plt\nimport numpy as np",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Classification with Machine Learning</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_1.html#data-scaling",
    "href": "4b_log_reg_tutorial_1.html#data-scaling",
    "title": "1  An Introduction to Classification with Machine Learning",
    "section": "2.3 Data scaling",
    "text": "2.3 Data scaling\nIt is usual to scale input features in machine learning, so that all features are on a similar scale. Consider these two features (we will create artifical data).\n\n# Create two sets of data with different means and standard deviations\nnp.random.seed(123) # The random seed simply \"fixes\" the random number generation to make it reproducible\nx1 = np.random.normal(50,10,size=1000)\nx2 = np.random.normal(150,30,size=1000)\n\n\n# Set up single plot\nfig, ax = plt.subplots(figsize=(8,5))\n# Add histogram of x1\nax.hist(x1, bins=50, alpha=0.5, color='b', label='x1')\n# Add histogram of x2\nax.hist(x2, bins=50, alpha=0.5, color='r', label='x2')\n# Add labels\nax.set_xlabel('value')\nax.set_ylabel('count')\n# Add legend\nax.legend()\n# Finalise and show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n2.3.1 MinMax Normalisation\nWith MinMax normalisation we scale all values between 0 and 1.\n\\[z = \\frac{x-min(x)}{max(x) - min(x)}\\]\nA less common alternative is to scale between -1 and 1.\n\\[z = -1 + 2\\frac{x-min(x)}{max(x) - min(x)}\\]\nHere we will use 0-1 normalisation. Here we do this manually, but for real data we can use a MinMaxScaler (as seen in the lecture) :\n\nx1_norm = (x1 - x1.min()) / (x1.max() - x1.min())\nx2_norm = (x2 - x2.min()) / (x2.max() - x2.min())\n\n\n\n2.3.2 Standardisation\nWith standardisation we scale data such that all features have a mean of 0 and standard deviation of 1. To do this we simply subtract by the mean and divide by the standard deviation.\n\\[z = \\frac{x-\\mu}{\\sigma}\\]\nAgain, here we do this manually, but for real data we’d typically use a StandardScaler, as seen in the lecture :\n\nx1_std = (x1 - x1.mean()) / x1.std()\nx2_std = (x2 - x2.mean()) / x2.std()\n\n\n\n2.3.3 Plotting the transformations\n\n# Set up three subplots (12 x 5 inch plot)\nfig, axs = plt.subplots(1, 3, figsize=(12,5))\n\n# Plot original data in axs[0]\naxs[0].hist(x1, bins=50, alpha=0.5, color='b', label='x1')\naxs[0].hist(x2, bins=50, alpha=0.5, color='r', label='x2')\naxs[0].set_xlabel('value')\naxs[0].set_ylabel('count')\naxs[0].legend()\naxs[0].set_title('Original data')\n\n# Plot normalised data in axs[1]\naxs[1].hist(x1_norm, bins=50, alpha=0.5, color='b', label='x1 norm')\naxs[1].hist(x2_norm, bins=50, alpha=0.5, color='r', label='x2 norm')\naxs[1].set_xlabel('value')\naxs[1].set_ylabel('count')\naxs[1].set_title('MinMax Normalised data')\n\n# Plot standardised data in axs[2]\naxs[2].hist(x1_std, bins=50, alpha=0.5, color='b', label='x1 norm')\naxs[2].hist(x2_std, bins=50, alpha=0.5, color='r', label='x2 norm')\naxs[2].set_xlabel('value')\naxs[2].set_ylabel('count')\naxs[2].set_title('Standardised data')\n\n# Adjust padding between subplots and show figure\nfig.tight_layout(pad=1.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.3.4 Notes on scaling\nMost commonly, with classification: * Decision trees and random forests require no scaling * Linear regression and support vector machines use standardised input data * Neural networks use MinMax normalised input data\nLater we will see that when we split data into training and test sets we will scale based on min/max or mean/std of the training set data only. We will also use the scaling methods provided by the SciKit-Learn library rather than using our own.",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Classification with Machine Learning</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html",
    "href": "4b_log_reg_tutorial_2.html",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "",
    "text": "2.1 Logistic regression\nIn this example we will use logistic regression (see https://en.wikipedia.org/wiki/Logistic_regression).\nFor an introductory video on logistic regression see: https://www.youtube.com/watch?v=yIYKR4sgzI8\nLogistic regression takes a range of features (which we will normalise/standardise to put on the same scale) and returns a probability that a certain classification (survival in this case) is true.\nWe will go through the following steps:",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#logistic-regression",
    "href": "4b_log_reg_tutorial_2.html#logistic-regression",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "",
    "text": "Download and save pre-processed data\nSplit data into features (X) and label (y)\nSplit data into training and test sets (we will test on data that has not been used to fit the model)\nStandardise data\nFit a logistic regression model (from sklearn)\nPredict survival of the test set, and assess accuracy\nReview model coefficients (weights) to see importance of features\nShow probability of survival for passengers",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#load-modules",
    "href": "4b_log_reg_tutorial_2.html#load-modules",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.2 Load modules",
    "text": "2.2 Load modules\nA standard Anaconda install of Python (https://www.anaconda.com/distribution/) contains all the necessary modules. Use your base environment if in doubt.\n\nimport numpy as np\nimport pandas as pd\n# Import machine learning methods\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#load-data",
    "href": "4b_log_reg_tutorial_2.html#load-data",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.3 Load data",
    "text": "2.3 Load data\nThe section below downloads pre-processed data, and saves it to a subfolder (from where this code is run). If data has already been downloaded that cell may be skipped.\nCode that was used to pre-process the data ready for machine learning may be found at: https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)\n\n\ndata = pd.read_csv('data/processed_data.csv')\n# Make all data 'float' type\ndata = data.astype(float)",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#examine-loaded-data",
    "href": "4b_log_reg_tutorial_2.html#examine-loaded-data",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.4 Examine loaded data",
    "text": "2.4 Examine loaded data\nThe data is in the form of a Pandas DataFrame, so we have column headers providing information of what is contained in each column.\nWe will use the DataFrame .head() method to show the first few rows of the imported DataFrame. By default this shows the first 5 rows. Here we will look at the first 10.\n\ndata.head(10)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n0\n1.0\n0.0\n3.0\n22.0\n1.0\n0.0\n7.2500\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n2.0\n1.0\n1.0\n38.0\n1.0\n0.0\n71.2833\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n3.0\n1.0\n3.0\n26.0\n0.0\n0.0\n7.9250\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n4.0\n1.0\n1.0\n35.0\n1.0\n0.0\n53.1000\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n5.0\n0.0\n3.0\n35.0\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n5\n6.0\n0.0\n3.0\n28.0\n0.0\n0.0\n8.4583\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n6\n7.0\n0.0\n1.0\n54.0\n0.0\n0.0\n51.8625\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n7\n8.0\n0.0\n3.0\n2.0\n3.0\n1.0\n21.0750\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n8\n9.0\n1.0\n3.0\n27.0\n0.0\n2.0\n11.1333\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n9\n10.0\n1.0\n2.0\n14.0\n1.0\n0.0\n30.0708\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n10 rows × 26 columns\n\n\n\n\nNote that in the above, “Imputed” column names indicate whether that feature was blank in the original data and was filled in (“imputed”) in the preprocessing (so “AgeImputed” value of 1.0 means the age was missing, 0.0 means it wasn’t). You can’t have missing cells in Machine Learning, so you have to decide what to do when you have missing data. This is part of the pre-processing step you’ll need to do with real data, and there’s more information about this in the pre-processing notebook here : https://michaelallen1966.github.io/titanic/01_preprocessing.html\nWe can also show a summary of the data with the .describe() method.\n\ndata.describe()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n...\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.361582\n0.523008\n0.381594\n32.204208\n0.198653\n0.002245\n0.771044\n...\n0.002245\n0.016835\n0.052750\n0.066218\n0.037037\n0.035915\n0.014590\n0.004489\n0.001122\n0.771044\n\n\nstd\n257.353842\n0.486592\n0.836071\n13.019697\n1.102743\n0.806057\n49.693429\n0.399210\n0.047351\n0.420397\n...\n0.047351\n0.128725\n0.223659\n0.248802\n0.188959\n0.186182\n0.119973\n0.066890\n0.033501\n0.420397\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n22.000000\n0.000000\n0.000000\n7.910400\n0.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n0.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n75%\n668.500000\n1.000000\n3.000000\n35.000000\n1.000000\n0.000000\n31.000000\n0.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 26 columns\n\n\n\n\nThe first column is a passenger index number. We will remove this, as this is not part of the original Titanic passenger data, and will not help us train our model.\n\n# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n# We drop passenger ID as it is not original data\n# inplace=True means change the dataframe itself - don't create a copy with this column dropped\n\ndata.drop('PassengerId', inplace=True, axis=1)",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#looking-at-a-summary-of-passengers-who-survived-or-did-not-survive",
    "href": "4b_log_reg_tutorial_2.html#looking-at-a-summary-of-passengers-who-survived-or-did-not-survive",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.5 Looking at a summary of passengers who survived or did not survive",
    "text": "2.5 Looking at a summary of passengers who survived or did not survive\nBefore running machine learning models, it is always good to have a look at your data. Here we will separate passengers who survived from those who died, and we will have a look at differences in features.\nWe will use a mask to select and filter passengers. The mask applies Boolean values (True and False) to entries depending on a given condition. Below, we use this to create a mask that has True values for any rows where the Survived value is 1.0 (ie where the patient survived), and then store only those rows in a separate dataframe called “survived”. Then we do the same thing but for those who died.\n\nmask = data['Survived'] == 1 # Mask for passengers who survive\nsurvived = data[mask] # filter using mask\n\nmask = data['Survived'] == 0 # Mask for passengers who died\ndied = data[mask] # filter using mask\n\nNow let’s look at average (mean) values for each feature for those who survived and those who died. We can make comparing them easier by putting these values in a new DataFrame so we can look at them side by side. What do you notice? What features do you think might have influenced survival?\n\nsummary = pd.DataFrame() # New empty DataFrame\nsummary['survived'] = survived.mean()\nsummary['died'] = died.mean()\n\n\nsummary\n\n\n\n\n\n\n\n\n\nsurvived\ndied\n\n\n\n\nSurvived\n1.000000\n0.000000\n\n\nPclass\n1.950292\n2.531876\n\n\nAge\n28.291433\n30.028233\n\n\nSibSp\n0.473684\n0.553734\n\n\nParch\n0.464912\n0.329690\n\n\nFare\n48.395408\n22.117887\n\n\nAgeImputed\n0.152047\n0.227687\n\n\nEmbarkedImputed\n0.005848\n0.000000\n\n\nCabinLetterImputed\n0.602339\n0.876138\n\n\nCabinNumber\n18.961988\n6.074681\n\n\nCabinNumberImputed\n0.611111\n0.885246\n\n\nmale\n0.318713\n0.852459\n\n\nEmbarked_C\n0.271930\n0.136612\n\n\nEmbarked_Q\n0.087719\n0.085610\n\n\nEmbarked_S\n0.634503\n0.777778\n\n\nEmbarked_missing\n0.005848\n0.000000\n\n\nCabinLetter_A\n0.020468\n0.014572\n\n\nCabinLetter_B\n0.102339\n0.021858\n\n\nCabinLetter_C\n0.102339\n0.043716\n\n\nCabinLetter_D\n0.073099\n0.014572\n\n\nCabinLetter_E\n0.070175\n0.014572\n\n\nCabinLetter_F\n0.023392\n0.009107\n\n\nCabinLetter_G\n0.005848\n0.003643\n\n\nCabinLetter_T\n0.000000\n0.001821\n\n\nCabinLetter_missing\n0.602339\n0.876138",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#divide-into-x-features-and-y-labels",
    "href": "4b_log_reg_tutorial_2.html#divide-into-x-features-and-y-labels",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.6 Divide into X (features) and y (labels)",
    "text": "2.6 Divide into X (features) and y (labels)\nWe will separate out our features (the data we use to make a prediction) from our label (what we are trying to predict - survival here). By convention our features are called X (usually upper case to denote multiple features), and the label (survived or not) y.\n\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#divide-into-training-and-tets-sets",
    "href": "4b_log_reg_tutorial_2.html#divide-into-training-and-tets-sets",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.7 Divide into training and tets sets",
    "text": "2.7 Divide into training and tets sets\nWhen we test a machine learning model we should always test it on data that has not been used to train the model. We will use sklearn’s train_test_split method to randomly split the data: 75% for training, and 25% for testing.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\nLet’s have a look at the standard deviation and the mean of the feature values in the training data (Standard Deviation for each feature shown first, then mean for each feature)\n\nX_train.std(), X_train.mean()\n\n(Pclass                  0.831431\n Age                    12.963059\n SibSp                   1.147940\n Parch                   0.803459\n Fare                   46.558286\n AgeImputed              0.411525\n EmbarkedImputed         0.054677\n CabinLetterImputed      0.421493\n CabinNumber            25.933916\n CabinNumberImputed      0.413581\n male                    0.477421\n Embarked_C              0.377799\n Embarked_Q              0.288276\n Embarked_S              0.442443\n Embarked_missing        0.054677\n CabinLetter_A           0.143351\n CabinLetter_B           0.219955\n CabinLetter_C           0.240201\n CabinLetter_D           0.186250\n CabinLetter_E           0.197088\n CabinLetter_F           0.121524\n CabinLetter_G           0.066915\n CabinLetter_T           0.038691\n CabinLetter_missing     0.421493\n dtype: float64,\n Pclass                  2.318862\n Age                    29.352171\n SibSp                   0.555389\n Parch                   0.387725\n Fare                   31.497535\n AgeImputed              0.215569\n EmbarkedImputed         0.002994\n CabinLetterImputed      0.769461\n CabinNumber            10.673653\n CabinNumberImputed      0.781437\n male                    0.649701\n Embarked_C              0.172156\n Embarked_Q              0.091317\n Embarked_S              0.733533\n Embarked_missing        0.002994\n CabinLetter_A           0.020958\n CabinLetter_B           0.050898\n CabinLetter_C           0.061377\n CabinLetter_D           0.035928\n CabinLetter_E           0.040419\n CabinLetter_F           0.014970\n CabinLetter_G           0.004491\n CabinLetter_T           0.001497\n CabinLetter_missing     0.769461\n dtype: float64)",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#standardise-data",
    "href": "4b_log_reg_tutorial_2.html#standardise-data",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.8 Standardise data",
    "text": "2.8 Standardise data\nWe can see above that there are quite different scales across different features in this data. For example, passenger class is on a very different scale numerically than the fare paid.\nWe want all of our features to be on roughly the same scale. This generally leads to a better model, and also allows us to more easily compare the importance of different features.\nWe will use standardisation to scale our feature values, where we use the mean and standard deviation of the training set of data to normalise the data. We subtract the mean of the training set values, and divide by the standard deviation of the training data. Note that the mean and standard deviation of the training data are used to standardise the test set data as well.\nHere we will use sklearn’s StandardScaler method. This method also copes with problems we might otherwise have (such as if one feature has zero standard deviation in the training set). We write a little function so whenever we need to standardise some data, we can just call this function, pass in the training and test feature (X) data, and it’ll return the same data but standardised.\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.fit_transform(X_train)\n    test_std=sc.fit_transform(X_test)\n\n    return train_std, test_std\n\nNow let’s call this function and use it to standardise our training and test data.\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#fit-logistic-regression-model",
    "href": "4b_log_reg_tutorial_2.html#fit-logistic-regression-model",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.9 Fit logistic regression model",
    "text": "2.9 Fit logistic regression model\nNow we will fit a logistic regression model, using sklearn’s LogisticRegression method. Our machine learning model fitting (training) is only two lines of code! By using the name model for our logistic regression model we will make our model more interchangeable later on if we wanted to try a different kind of model, for example.\n\nmodel = LogisticRegression()\nmodel.fit(X_train_std,y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#predict-values",
    "href": "4b_log_reg_tutorial_2.html#predict-values",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.10 Predict values",
    "text": "2.10 Predict values\nNow we can use the trained model to predict survival. We will test the accuracy of both the training and test data sets.\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train_std)\ny_pred_test = model.predict(X_test_std)",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#calculate-accuracy",
    "href": "4b_log_reg_tutorial_2.html#calculate-accuracy",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.11 Calculate accuracy",
    "text": "2.11 Calculate accuracy\nIn this example we will measure accuracy simply as the proportion of passengers where we make the correct prediction. In later examples we will look at other measures of accuracy which explore false positives and false negatives in more detail.\n\n# The shorthand below says to check each predicted y value against the actual\n# y value in the training data.  This gives a list of True and False values\n# for each prediction, where True indicates the predicted value matches the\n# actual value.  Then we take the mean of these Boolean values, which gives\n# us a proportion (where if all values were True, the proportion would be 1.0)\n# If you want to see why that works, just uncomment the following line of code\n# to see what y_pred_train == y_train is doing.\n# print (y_pred_train == y_train)\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train}')\nprint (f'Accuracy of predicting test data = {accuracy_test}')\n\nAccuracy of predicting training data = 0.8203592814371258\nAccuracy of predicting test data = 0.7982062780269058\n\n\nNot bad - about 80% accuracy. You will probably see that accuracy of predicting the training set is usually higher than the test set. Because we are only testing one random sample, you may occasionally see otherwise. In later examples we will look at the best way to repeat multiple tests, and look at what to do if the accuracy of the training set is significantly higher than the test set (a problem called ‘overfitting’).",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#examining-the-model-coefficients-weights",
    "href": "4b_log_reg_tutorial_2.html#examining-the-model-coefficients-weights",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.12 Examining the model coefficients (weights)",
    "text": "2.12 Examining the model coefficients (weights)\nNot all features are equally important. And some may be of little or no use at all, unnecessarily increasing the complexity of the model. In later examples we will look at selecting features which add value to the model (or removing features that don’t).\nHere we will look at the importance of features – how they affect our estimation of survival. These are known as the model coefficients (if you come from a traditional statistics background), or model weights (if you come from a machine learning background).\nBecause we have standardised our input data the magnitude of the weights may be compared as an indicator of their influence in the model. Weights with higher negative numbers mean that that feature correlates with reduced chance of survival. Weights with higher positive numbers mean that that feature correlates with increased chance of survival. Those weights with values closer to zero (either positive or negative) have less influence in the model.\nWe access the model weights my examining the model coef_ attribute. The model may predict more than one outcome label, in which case we have weights for each label. Because we are predicting a single label here (survive or not), the weights are found in the first element ([0]) of the coef_ attribute.\n\nco_eff = model.coef_[0]\nco_eff\n\narray([-0.84475058, -0.55123105, -0.40925875, -0.01028495,  0.01404105,\n       -0.21700132,  0.09049331,  0.01086364, -0.04374518, -0.38007729,\n       -1.27635266,  0.09730836,  0.14421872, -0.18824044,  0.09049331,\n       -0.04918584,  0.04665105, -0.13362438,  0.0071312 ,  0.13648839,\n        0.10288958, -0.14729349, -0.16976434,  0.01086364])\n\n\nSo we have an array of model weights.\nNot very readable for us mere humans is it?!\nWe will transfer the weights array to a Pandas DataFrame. The array order is in the same order of the list of features of X, so we will put that those into the DataFrame as well. And we will sort by influence in the model. Because both large negative and positive values are more influential in the model we will take the absolute value of the weight (ie remove any negative sign), and then sort by that absolute value. That will give us a more readable table of most influential features in the model.\n\nco_eff_df = pd.DataFrame() # create empty DataFrame\nco_eff_df['feature'] = list(X) # Get feature names from X\nco_eff_df['co_eff'] = co_eff\nco_eff_df['abs_co_eff'] = np.abs(co_eff)\nco_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=True)\n\nLet’s look at the DataFrame. What do you conclude?\n\nco_eff_df\n\n\n\n\n\n\n\n\n\nfeature\nco_eff\nabs_co_eff\n\n\n\n\n10\nmale\n-1.276353\n1.276353\n\n\n0\nPclass\n-0.844751\n0.844751\n\n\n1\nAge\n-0.551231\n0.551231\n\n\n2\nSibSp\n-0.409259\n0.409259\n\n\n9\nCabinNumberImputed\n-0.380077\n0.380077\n\n\n5\nAgeImputed\n-0.217001\n0.217001\n\n\n13\nEmbarked_S\n-0.188240\n0.188240\n\n\n22\nCabinLetter_T\n-0.169764\n0.169764\n\n\n21\nCabinLetter_G\n-0.147293\n0.147293\n\n\n12\nEmbarked_Q\n0.144219\n0.144219\n\n\n19\nCabinLetter_E\n0.136488\n0.136488\n\n\n17\nCabinLetter_C\n-0.133624\n0.133624\n\n\n20\nCabinLetter_F\n0.102890\n0.102890\n\n\n11\nEmbarked_C\n0.097308\n0.097308\n\n\n6\nEmbarkedImputed\n0.090493\n0.090493\n\n\n14\nEmbarked_missing\n0.090493\n0.090493\n\n\n15\nCabinLetter_A\n-0.049186\n0.049186\n\n\n16\nCabinLetter_B\n0.046651\n0.046651\n\n\n8\nCabinNumber\n-0.043745\n0.043745\n\n\n4\nFare\n0.014041\n0.014041\n\n\n7\nCabinLetterImputed\n0.010864\n0.010864\n\n\n23\nCabinLetter_missing\n0.010864\n0.010864\n\n\n3\nParch\n-0.010285\n0.010285\n\n\n18\nCabinLetter_D\n0.007131\n0.007131\n\n\n\n\n\n\n\n\nNow - the actual order of features, and the weights for each, will be different each time you fit the model, as it’s random and we haven’t put in place any controls. So don’t worry if yours differs a bit. But, you’ll likely find that amongst the top weighted features are :\n\nmale (being male reduces probability of survival)\nPclass (lower class passengers, who have a higher class number, reduces probability of survival)\nage (being older reduces probability of survival)\nCabinNumberImputed (cabin number is missing, which may mean they didn’t have a cabin - likely lower class)",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_log_reg_tutorial_2.html#show-predicted-probabilities",
    "href": "4b_log_reg_tutorial_2.html#show-predicted-probabilities",
    "title": "2  A logistic regression model for predicting survival on the titanic",
    "section": "2.13 Show predicted probabilities",
    "text": "2.13 Show predicted probabilities\nThe predicted probabilities are for the two alternative classes 0 (does not survive) or 1 (survive).\nOrdinarily we do not see these probabilities - the predict method used above applies a cut-off of 0.5 to classify passengers into survived or not, but we can see the individual probabilities for each passenger if desired.\nIn a later example we will see how we can use such probabilities to adjust the sensitivity of our model to detecting survivors or non-survivors.\nEach passenger has two values. These are the probability of not surviving (first value) or surviving (second value). Because we only have two possible classes we only need to look at one. Multiple values are important when there are more than one class being predicted.\n\n# Show first ten predicted classes\nclasses = model.predict(X_test_std)\nclasses[0:10]\n\narray([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.])\n\n\n\n# Show first ten predicted probabilities\n# (note how the values relate to the classes predicted above)\nprobabilities = model.predict_proba(X_test_std)\nprobabilities[0:10]\n\narray([[0.9585624 , 0.0414376 ],\n       [0.98719408, 0.01280592],\n       [0.83298478, 0.16701522],\n       [0.94523796, 0.05476204],\n       [0.04687281, 0.95312719],\n       [0.5156704 , 0.4843296 ],\n       [0.91270338, 0.08729662],\n       [0.89453433, 0.10546567],\n       [0.5000722 , 0.4999278 ],\n       [0.08915059, 0.91084941]])",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A logistic regression model for predicting survival on the titanic</span>"
    ]
  },
  {
    "objectID": "4b_ex_1_solution.html",
    "href": "4b_ex_1_solution.html",
    "title": "3  Stroke Thromobolysis Dataset: Logistic Regression Exercise (Solution)",
    "section": "",
    "text": "The data loaded in this exercise is for seven acute stroke units, and whether a patient receives clost-busting treatment for stroke. There are lots of features, and a description of the features can be found in the file stroke_data_feature_descriptions.csv.\nTrain a Logistic Regression model to try to predict whether or not a stroke patient receives clot-busting treatment. Use the prompts below to write each section of code.\nWhat do you conclude are the most important features for predicting whether a patient receives clot busting treatment? Can you improve accuracy by changing the size of your train / test split? If you have time, perhaps consider dropping some features from your data based on your outputs (in the same way you dropped passengerID in the Titanic example). Don’t forget you’ll need to rerun all subsequent cells if you make changes like that.\n\nimport pandas as pd\nimport numpy as np\n# Import machine learning methods\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Download data\n# (not required if running locally and have previously downloaded data)\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '2004_titanic/master/jupyter_notebooks/data/hsma_stroke.csv'\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data to data subfolder\n    data.to_csv(data_directory + 'hsma_stroke.csv', index=False)\n\n# Load data\ndata = pd.read_csv('data/hsma_stroke.csv')\n# Make all data 'float' type\ndata = data.astype(float)\n# Show data\ndata.head()\n\n\n\n\n\n\n\n\n\nClotbuster given\nHosp_1\nHosp_2\nHosp_3\nHosp_4\nHosp_5\nHosp_6\nHosp_7\nMale\nAge\n...\nS2NihssArrivalFacialPalsy\nS2NihssArrivalMotorArmLeft\nS2NihssArrivalMotorArmRight\nS2NihssArrivalMotorLegLeft\nS2NihssArrivalMotorLegRight\nS2NihssArrivalLimbAtaxia\nS2NihssArrivalSensory\nS2NihssArrivalBestLanguage\nS2NihssArrivalDysarthria\nS2NihssArrivalExtinctionInattention\n\n\n\n\n0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n63.0\n...\n3.0\n4.0\n0.0\n4.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n85.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n91.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n90.0\n...\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n69.0\n...\n2.0\n0.0\n4.0\n1.0\n4.0\n0.0\n1.0\n2.0\n2.0\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n# Look at overview of data\ndata.describe()\n\n\n\n\n\n\n\n\n\nClotbuster given\nHosp_1\nHosp_2\nHosp_3\nHosp_4\nHosp_5\nHosp_6\nHosp_7\nMale\nAge\n...\nS2NihssArrivalFacialPalsy\nS2NihssArrivalMotorArmLeft\nS2NihssArrivalMotorArmRight\nS2NihssArrivalMotorLegLeft\nS2NihssArrivalMotorLegRight\nS2NihssArrivalLimbAtaxia\nS2NihssArrivalSensory\nS2NihssArrivalBestLanguage\nS2NihssArrivalDysarthria\nS2NihssArrivalExtinctionInattention\n\n\n\n\ncount\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n...\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n\n\nmean\n0.403330\n0.159506\n0.142320\n0.154672\n0.165414\n0.055854\n0.113319\n0.208915\n0.515575\n74.553706\n...\n1.114930\n1.002148\n0.963480\n0.963480\n0.910849\n0.216971\n0.610097\n0.944146\n0.739527\n0.566595\n\n\nstd\n0.490698\n0.366246\n0.349472\n0.361689\n0.371653\n0.229701\n0.317068\n0.406643\n0.499892\n12.280576\n...\n0.930527\n1.479211\n1.441594\n1.406501\n1.380606\n0.522643\n0.771932\n1.121379\n0.731083\n0.794000\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n40.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n67.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n76.000000\n...\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n83.000000\n...\n2.000000\n2.000000\n2.000000\n2.000000\n2.000000\n0.000000\n1.000000\n2.000000\n1.000000\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n100.000000\n...\n3.000000\n4.000000\n4.000000\n4.000000\n4.000000\n2.000000\n2.000000\n3.000000\n2.000000\n2.000000\n\n\n\n\n8 rows × 51 columns\n\n\n\n\n\n# Look at mean feature values for those who were given a clotbuster vs those\n# that weren't\nmask = data['Clotbuster given'] == 1\ngiven = data[mask]\n\nmask = data['Clotbuster given'] == 0\nnot_given = data[mask]\n\nsummary = pd.DataFrame()\nsummary['given'] = given.mean()\nsummary['not given'] = not_given.mean()\n\nsummary\n\n\n\n\n\n\n\n\n\ngiven\nnot given\n\n\n\n\nClotbuster given\n1.000000\n0.000000\n\n\nHosp_1\n0.203728\n0.129613\n\n\nHosp_2\n0.122503\n0.155716\n\n\nHosp_3\n0.182423\n0.135914\n\n\nHosp_4\n0.137150\n0.184518\n\n\nHosp_5\n0.067909\n0.047705\n\n\nHosp_6\n0.123835\n0.106211\n\n\nHosp_7\n0.162450\n0.240324\n\n\nMale\n0.515313\n0.515752\n\n\nAge\n73.303595\n75.398740\n\n\n80+\n0.346205\n0.393339\n\n\nOnset Time Known Type_BE\n0.149134\n0.352835\n\n\nOnset Time Known Type_NK\n0.010652\n0.015302\n\n\nOnset Time Known Type_P\n0.840213\n0.631863\n\n\n# Comorbidities\n1.053262\n1.258326\n\n\n2+ comorbidotes\n0.298269\n0.393339\n\n\nCongestive HF\n0.041278\n0.047705\n\n\nHypertension\n0.464714\n0.470747\n\n\nAtrial Fib\n0.186418\n0.239424\n\n\nDiabetes\n0.151798\n0.172817\n\n\nTIA\n0.209055\n0.327633\n\n\nCo-mordity\n0.676431\n0.732673\n\n\nAntiplatelet_0\n0.097204\n0.144014\n\n\nAntiplatelet_1\n0.079893\n0.079208\n\n\nAntiplatelet_NK\n0.822903\n0.776778\n\n\nAnticoag before stroke_0\n0.122503\n0.083708\n\n\nAnticoag before stroke_1\n0.046605\n0.129613\n\n\nAnticoag before stroke_NK\n0.830892\n0.786679\n\n\nStroke severity group_1. No stroke symtpoms\n0.002663\n0.041404\n\n\nStroke severity group_2. Minor\n0.061252\n0.396040\n\n\nStroke severity group_3. Moderate\n0.619174\n0.333033\n\n\nStroke severity group_4. Moderate to severe\n0.182423\n0.105311\n\n\nStroke severity group_5. Severe\n0.134487\n0.124212\n\n\nStroke Type_I\n1.000000\n0.803780\n\n\nStroke Type_PIH\n0.000000\n0.196220\n\n\nS2RankinBeforeStroke\n0.360852\n0.791179\n\n\nS2NihssArrival\n12.515313\n9.032403\n\n\nS2NihssArrivalLocQuestions\n0.850866\n0.605761\n\n\nS2NihssArrivalLocCommands\n0.394141\n0.360936\n\n\nS2NihssArrivalBestGaze\n0.521971\n0.341134\n\n\nS2NihssArrivalVisual\n0.809587\n0.495050\n\n\nS2NihssArrivalFacialPalsy\n1.407457\n0.917192\n\n\nS2NihssArrivalMotorArmLeft\n1.215712\n0.857786\n\n\nS2NihssArrivalMotorArmRight\n1.073236\n0.889289\n\n\nS2NihssArrivalMotorLegLeft\n1.165113\n0.827183\n\n\nS2NihssArrivalMotorLegRight\n0.977364\n0.865887\n\n\nS2NihssArrivalLimbAtaxia\n0.214381\n0.218722\n\n\nS2NihssArrivalSensory\n0.762983\n0.506751\n\n\nS2NihssArrivalBestLanguage\n1.181092\n0.783978\n\n\nS2NihssArrivalDysarthria\n0.902796\n0.629163\n\n\nS2NihssArrivalExtinctionInattention\n0.762983\n0.433843\n\n\n\n\n\n\n\n\n\n# Divide into features and labels\nX = data.drop('Clotbuster given', axis=1)\ny = data['Clotbuster given']\n\n\n# Divide into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\n# Standardise data\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.fit_transform(X_train)\n    test_std=sc.fit_transform(X_test)\n\n    return train_std, test_std\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)\n\n\n# Fit (train) Logistic Regression model\nmodel = LogisticRegression()\nmodel.fit(X_train_std, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Predict training and test labels, and calculate accuracy\ny_pred_train = model.predict(X_train_std)\ny_pred_test = model.predict(X_test_std)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train}')\nprint (f'Accuracy of predicting test data = {accuracy_test}')\n\nAccuracy of predicting training data = 0.8202005730659025\nAccuracy of predicting test data = 0.7939914163090128\n\n\n\n# Examine feature weights and sort by most influential\nco_eff = model.coef_[0]\n\nco_eff_df = pd.DataFrame()\nco_eff_df['feature'] = list(X)\nco_eff_df['co_eff'] = co_eff\nco_eff_df['abs_co_eff'] = np.abs(co_eff)\nco_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=True)\n\nco_eff_df\n\n\n\n\n\n\n\n\n\nfeature\nco_eff\nabs_co_eff\n\n\n\n\n32\nStroke Type_I\n1.121156\n1.121156\n\n\n33\nStroke Type_PIH\n-1.121156\n1.121156\n\n\n28\nStroke severity group_2. Minor\n-0.709358\n0.709358\n\n\n29\nStroke severity group_3. Moderate\n0.594452\n0.594452\n\n\n35\nS2NihssArrival\n-0.468261\n0.468261\n\n\n34\nS2RankinBeforeStroke\n-0.458139\n0.458139\n\n\n47\nS2NihssArrivalBestLanguage\n0.407136\n0.407136\n\n\n8\nAge\n-0.351431\n0.351431\n\n\n10\nOnset Time Known Type_BE\n-0.311359\n0.311359\n\n\n17\nAtrial Fib\n-0.304925\n0.304925\n\n\n12\nOnset Time Known Type_P\n0.296688\n0.296688\n\n\n16\nHypertension\n0.284943\n0.284943\n\n\n41\nS2NihssArrivalMotorArmLeft\n0.270736\n0.270736\n\n\n27\nStroke severity group_1. No stroke symtpoms\n-0.269664\n0.269664\n\n\n24\nAnticoag before stroke_0\n0.268555\n0.268555\n\n\n25\nAnticoag before stroke_1\n-0.265632\n0.265632\n\n\n36\nS2NihssArrivalLocQuestions\n0.259637\n0.259637\n\n\n40\nS2NihssArrivalFacialPalsy\n0.249051\n0.249051\n\n\n49\nS2NihssArrivalExtinctionInattention\n0.216644\n0.216644\n\n\n30\nStroke severity group_4. Moderate to severe\n0.194853\n0.194853\n\n\n19\nTIA\n-0.193125\n0.193125\n\n\n42\nS2NihssArrivalMotorArmRight\n0.186265\n0.186265\n\n\n38\nS2NihssArrivalBestGaze\n0.180682\n0.180682\n\n\n37\nS2NihssArrivalLocCommands\n-0.175957\n0.175957\n\n\n3\nHosp_4\n-0.171598\n0.171598\n\n\n21\nAntiplatelet_0\n0.155566\n0.155566\n\n\n14\n2+ comorbidotes\n-0.145232\n0.145232\n\n\n0\nHosp_1\n0.122579\n0.122579\n\n\n5\nHosp_6\n0.116022\n0.116022\n\n\n23\nAntiplatelet_NK\n-0.110145\n0.110145\n\n\n39\nS2NihssArrivalVisual\n0.093206\n0.093206\n\n\n1\nHosp_2\n0.071554\n0.071554\n\n\n45\nS2NihssArrivalLimbAtaxia\n0.067437\n0.067437\n\n\n43\nS2NihssArrivalMotorLegLeft\n0.065901\n0.065901\n\n\n9\n80+\n0.064134\n0.064134\n\n\n13\n# Comorbidities\n-0.062878\n0.062878\n\n\n4\nHosp_5\n-0.047460\n0.047460\n\n\n6\nHosp_7\n-0.046642\n0.046642\n\n\n46\nS2NihssArrivalSensory\n0.044662\n0.044662\n\n\n11\nOnset Time Known Type_NK\n0.042662\n0.042662\n\n\n2\nHosp_3\n-0.034359\n0.034359\n\n\n7\nMale\n0.032534\n0.032534\n\n\n48\nS2NihssArrivalDysarthria\n0.031809\n0.031809\n\n\n20\nCo-mordity\n-0.029874\n0.029874\n\n\n22\nAntiplatelet_1\n-0.026909\n0.026909\n\n\n31\nStroke severity group_5. Severe\n-0.019006\n0.019006\n\n\n15\nCongestive HF\n0.016522\n0.016522\n\n\n18\nDiabetes\n0.013792\n0.013792\n\n\n44\nS2NihssArrivalMotorLegRight\n-0.009994\n0.009994\n\n\n26\nAnticoag before stroke_NK\n-0.006123\n0.006123",
    "crumbs": [
      "4B - Logistic Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stroke Thromobolysis Dataset: Logistic Regression Exercise (Solution)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html",
    "href": "4d_decision_tree_titanic.html",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "",
    "text": "4.1 Divide into X (features) and y (labels)\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#divide-into-training-and-tets-sets",
    "href": "4d_decision_tree_titanic.html#divide-into-training-and-tets-sets",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "4.2 Divide into training and tets sets",
    "text": "4.2 Divide into training and tets sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#fit-decision-tree-model",
    "href": "4d_decision_tree_titanic.html#fit-decision-tree-model",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "4.3 Fit decision tree model",
    "text": "4.3 Fit decision tree model\n\nmodel = DecisionTreeClassifier() # Create a Decision Tree Model\nmodel = model.fit(X_train,y_train) # Fit the model using our training data",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#predict-values",
    "href": "4d_decision_tree_titanic.html#predict-values",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "4.4 Predict values",
    "text": "4.4 Predict values\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#calculate-accuracy",
    "href": "4d_decision_tree_titanic.html#calculate-accuracy",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "4.5 Calculate accuracy",
    "text": "4.5 Calculate accuracy\n\n# The shorthand below says to check each predicted y value against the actual\n# y value in the training data.  This gives a list of True and False values\n# for each prediction, where True indicates the predicted value matches the\n# actual value.  Then we take the mean of these Boolean values, which gives\n# us a proportion (where if all values were True, the proportion would be 1.0)\n# If you want to see why that works, just uncomment the following line of code\n# to see what y_pred_train == y_train is doing.\n# print (y_pred_train == y_train)\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train:3f}')\nprint (f'Accuracy of predicting test data = {accuracy_test:3f}')\n\nAccuracy of predicting training data = 0.983533\nAccuracy of predicting test data = 0.766816\n\n\n\n# Show first ten predicted classes\nclasses = model.predict(X_test)\nclasses[0:10]\n\narray([0., 1., 0., 1., 1., 1., 1., 0., 0., 1.])\n\n\n\n# Show first ten predicted probabilities\nprobabilities = model.predict_proba(X_test)\nprobabilities[0:10]\n\narray([[1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.16666667, 0.83333333],\n       [1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ]])",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#plot-tree",
    "href": "4d_decision_tree_titanic.html#plot-tree",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "4.6 Plot tree",
    "text": "4.6 Plot tree\n\nfig = plot_tree(model)\n\n\n\n\n\n\n\n\n\nfig = plot_tree(\n    model,\n    feature_names=data.drop('Survived',axis=1).columns.tolist(),\n    class_names=['Died', 'Survived'],\n    filled=True\n    )",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#following-nodes",
    "href": "4d_decision_tree_titanic.html#following-nodes",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "4.7 Following Nodes",
    "text": "4.7 Following Nodes\nhttps://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#post-pruning",
    "href": "4d_decision_tree_titanic.html#post-pruning",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "6.1 Post pruning",
    "text": "6.1 Post pruning\nhttps://ranvir.xyz/blog/practical-approach-to-tree-pruning-using-sklearn/\nhttps://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n\nimport matplotlib.pyplot as plt\n\n\nmodel = DecisionTreeClassifier()\npath = model.cost_complexity_pruning_path(X_train, y_train)\n\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\nplt.figure(figsize=(10, 6))\nplt.plot(ccp_alphas, impurities)\nplt.xlabel(\"effective alpha\")\nplt.ylabel(\"total impurity of leaves\")\n\nText(0, 0.5, 'total impurity of leaves')\n\n\n\n\n\n\n\n\n\n\nmodels = []\n\nfor ccp_alpha in ccp_alphas:\n    model = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    model.fit(X_train, y_train)\n    models.append(model)\n\n\ntree_depths = [model.tree_.max_depth for model in models]\nplt.figure(figsize=(10,  6))\nplt.plot(ccp_alphas[:-1], tree_depths[:-1])\nplt.xlabel(\"effective alpha\")\nplt.ylabel(\"total depth\")\n\nText(0, 0.5, 'total depth')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\n\nacc_scores = [accuracy_score(y_test, model.predict(X_test)) for model in models]\n\ntree_depths = [model.tree_.max_depth for model in models]\nplt.figure(figsize=(10,  6))\nplt.grid()\nplt.plot(ccp_alphas[:-1], acc_scores[:-1])\nplt.xlabel(\"effective alpha\")\nplt.ylabel(\"Accuracy scores\")\n\nText(0, 0.5, 'Accuracy scores')\n\n\n\n\n\n\n\n\n\nFinal model from this approach\n\ntrain_and_run_dt(DecisionTreeClassifier(random_state=0, ccp_alpha=0.0045))\n\nAccuracy of predicting training data = 0.832\nAccuracy of predicting test data = 0.803\nPrecision on test data = 0.803\nRecall on test data = 0.803\nSpecificity on test data = 0.803\n\n\nc:\\HSMA\\_HSMA 6\\Sammi's Sessions\\h6_4d_decision_trees_random_forests\\h6_4d_decision_trees_random_forests\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1561: UserWarning:\n\nNote that pos_label (set to 0) is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#exploring-metrics-in-our-lr-and-dt",
    "href": "4d_decision_tree_titanic.html#exploring-metrics-in-our-lr-and-dt",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "6.2 Exploring metrics in our lr and dt",
    "text": "6.2 Exploring metrics in our lr and dt\n\n6.2.1 Decision Tree\n\ndecision_tree_model = model = DecisionTreeClassifier(max_depth=6)\ndecision_tree_model = decision_tree_model.fit(X_train,y_train)\ny_pred_train_dt = decision_tree_model.predict(X_train)\ny_pred_test_dt = decision_tree_model.predict(X_test)\n\n\nroc_curve = RocCurveDisplay.from_estimator(\n    decision_tree_model, X_test, y_test\n)\n\nfig = roc_curve.figure_\nax = roc_curve.ax_\n\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n\n\n\n\n\n\n\n\n\nnp.random.seed(42)\n\ndecision_tree_model = DecisionTreeClassifier(max_depth=6)\ndecision_tree_model = decision_tree_model.fit(X_train,y_train)\n\ny_pred_train_dt = decision_tree_model.predict(X_train)\ny_pred_test_dt = decision_tree_model.predict(X_test)\n\nroc_curve_dt = RocCurveDisplay.from_estimator(\n    decision_tree_model, X_test, y_test\n)\n\nfig = roc_curve_dt.figure_\nax = roc_curve_dt.ax_\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n\n\n\n\n\n\n\n\n\nconfusion_matrix_dt = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test_dt\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n)\n\nconfusion_matrix_dt.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nconfusion_matrix_dt_normalised = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test_dt,\n        normalize='true'\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n)\n\nconfusion_matrix_dt_normalised.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(precision_recall_fscore_support(\n        y_true=y_test,\n        y_pred=y_pred_test_dt,\n        average=\"binary\"\n        ))\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n0.769231\n\n\n1\n0.561798\n\n\n2\n0.649351\n\n\n3\nNaN\n\n\n\n\n\n\n\n\n\n\n6.2.2 Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.fit_transform(X_train)\n    test_std=sc.fit_transform(X_test)\n\n    return train_std, test_std\n\nX_train_standardised, X_test_standardised = standardise_data(X_train, X_test)\n\nlogistic_regression_model = LogisticRegression()\nlogistic_regression_model = logistic_regression_model.fit(X_train_standardised,y_train)\n\ny_pred_train_lr = logistic_regression_model.predict(X_train_standardised)\ny_pred_test_lr = logistic_regression_model.predict(X_test_standardised)\n\naccuracy_train = np.mean(y_pred_train_lr == y_train)\naccuracy_test = np.mean(y_pred_test_lr == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train}')\nprint (f'Accuracy of predicting test data = {accuracy_test}')\n\nAccuracy of predicting training data = 0.8083832335329342\nAccuracy of predicting test data = 0.8116591928251121\n\n\n\nroc_curve_lr = RocCurveDisplay.from_estimator(\n    logistic_regression_model, X_test_standardised, y_test\n)\n\nfig = roc_curve_lr.figure_\nax = roc_curve_lr.ax_\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n\n\n\n\n\n\n\n\n\nconfusion_matrix_lr = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test_lr,\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n)\n\nconfusion_matrix_lr.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nconfusion_matrix_lr_normalised = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test_lr,\n        normalize='true',\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n)\n\nconfusion_matrix_lr_normalised.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(classification_report(\n        y_true=y_test,\n        y_pred=y_pred_test_lr,\n        target_names=[\"Died\", \"Survived\"],\n        output_dict=True\n))\n\n\n\n\n\n\n\n\n\nDied\nSurvived\naccuracy\nmacro avg\nweighted avg\n\n\n\n\nprecision\n0.823944\n0.790123\n0.811659\n0.807034\n0.810446\n\n\nrecall\n0.873134\n0.719101\n0.811659\n0.796118\n0.811659\n\n\nf1-score\n0.847826\n0.752941\n0.811659\n0.800384\n0.809957\n\n\nsupport\n134.000000\n89.000000\n0.811659\n223.000000\n223.000000\n\n\n\n\n\n\n\n\n\nprecision, recall, fbeta, support = precision_recall_fscore_support(\n        y_true=y_test,\n        y_pred=y_pred_test_lr,\n        average=\"binary\"\n        )",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_titanic.html#compare-confusion-matrices",
    "href": "4d_decision_tree_titanic.html#compare-confusion-matrices",
    "title": "4  Decision Trees for Classification (Titanic Dataset)",
    "section": "6.3 Compare confusion matrices",
    "text": "6.3 Compare confusion matrices\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nconfusion_matrix_dt.plot(ax=ax1)\nax1.title.set_text('Decision Tree')\n\nconfusion_matrix_lr.plot(ax=ax2)\nax2.title.set_text('Logistic Regression')\n\n\n\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nconfusion_matrix_dt_normalised.plot(ax=ax1)\nax1.title.set_text('Decision Tree - Normalised')\n\nconfusion_matrix_lr_normalised.plot(ax=ax2)\nax2.title.set_text('Logistic Regression - Normalised')",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Decision Trees for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_stroke_exercise_SOLUTION.html",
    "href": "4d_decision_tree_stroke_exercise_SOLUTION.html",
    "title": "5  Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "5.1 Core - Fitting and Evaluating a Decision Tree\nRun the code below to import the dataset and the libraries we need.\nimport pandas as pd\nimport numpy as np\n\n# import preprocessing functions\nfrom sklearn.model_selection import train_test_split\n\n# Import machine learning model of interest\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# Import package to investigate our loaded dataframe\nfrom ydata_profiling import ProfileReport\n\n# Import functions for evaluating model\nfrom sklearn.metrics import recall_score, precision_score\n\n# Imports relating to logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Imports relating to plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Download data\n# (not required if running locally and have previously downloaded data)\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '2004_titanic/master/jupyter_notebooks/data/hsma_stroke.csv'\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data to data subfolder\n    data.to_csv(data_directory + 'hsma_stroke.csv', index=False)\n\n# Load data\ndata = pd.read_csv('data/hsma_stroke.csv')\n# Make all data 'float' type\ndata = data.astype(float)\n# Show data\ndata.head()\n\n\n\n\n\n\n\n\n\nClotbuster given\nHosp_1\nHosp_2\nHosp_3\nHosp_4\nHosp_5\nHosp_6\nHosp_7\nMale\nAge\n...\nS2NihssArrivalFacialPalsy\nS2NihssArrivalMotorArmLeft\nS2NihssArrivalMotorArmRight\nS2NihssArrivalMotorLegLeft\nS2NihssArrivalMotorLegRight\nS2NihssArrivalLimbAtaxia\nS2NihssArrivalSensory\nS2NihssArrivalBestLanguage\nS2NihssArrivalDysarthria\nS2NihssArrivalExtinctionInattention\n\n\n\n\n0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n63.0\n...\n3.0\n4.0\n0.0\n4.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n1\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n85.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n91.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n90.0\n...\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n4\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n69.0\n...\n2.0\n0.0\n4.0\n1.0\n4.0\n0.0\n1.0\n2.0\n2.0\n1.0\n\n\n\n\n5 rows × 51 columns\nLook at an overview of the data by running the code below.\nWe’re going to use a library we haven’t covered before to give a quick summary of the dataframe.\nYou used this data last week, so it should feel familiar to you.\nDo you prefer this method or the code you used last week in the logistic regression exercise?\nprofile = ProfileReport(data)\n\nprofile.to_notebook_iframe()\nLoad in the ‘stroke_data_feature_descriptions’ dataframe and view that too - you can just view the whole dataframe with pandas rather than using the ProfileReport.\nHint: it’s in the same folder as the hsma_stroke.csv dataset we imported above.\nstroke_data_feature_descriptions_df = pd.read_csv('../datasets/stroke_data_feature_descriptions.csv')\n\nstroke_data_feature_descriptions_df\n\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\n0\n# Comorbidities\nNumber of comorbidities\n\n\n1\n2+ comorbidotes\nIf the patient had at least two comorbidities\n\n\n2\n80+\nIf the patient is aged 80 or over\n\n\n3\nAge\nAge of patient\n\n\n4\nAnticoag before stroke_0\nDid not take anticoagulants before stroke\n\n\n5\nAnticoag before stroke_1\nDid take anticoagulants before stroke\n\n\n6\nAnticoag before stroke_NK\nNot known if was taking anticoagulants before ...\n\n\n7\nAntiplatelet_0\nDid not receive antiplatelet treatment\n\n\n8\nAntiplatelet_1\nDid receive antiplatelet treatment\n\n\n9\nAntiplatelet_NK\nNot known if received antiplatelet treatment\n\n\n10\nAtrial Fib\nPatient has atrial fibrillation\n\n\n11\nCo-mordity\nIf the patient has any comorbidities at all\n\n\n12\nCongestive HF\nPatient has congestive heart failure\n\n\n13\nDiabetes\nPatient has diabetes\n\n\n14\nHosp_1\nTaken to hospital 1\n\n\n15\nHosp_2\nTaken to hospital 2\n\n\n16\nHosp_3\nTaken to hospital 3\n\n\n17\nHosp_4\nTaken to hospital 4\n\n\n18\nHosp_5\nTaken to hospital 5\n\n\n19\nHosp_6\nTaken to hospital 6\n\n\n20\nHosp_7\nTaken to hospital 7\n\n\n21\nHypertension\nPatient has hypertension\n\n\n22\nMale\nPatient is male\n\n\n23\nOnset Time Known Type_BE\nOnset time type is Best Estimate\n\n\n24\nOnset Time Known Type_NK\nOnset time type is Not Known\n\n\n25\nOnset Time Known Type_P\nOnset time type is Precise\n\n\n26\nS2NihssArrival\nStroke severity (NIHSS score) on arrival : tot...\n\n\n27\nS2NihssArrivalBestGaze\nStroke severity (NIHSS score) on arrival : eye...\n\n\n28\nS2NihssArrivalBestLanguage\nStroke severity (NIHSS score) on arrival : com...\n\n\n29\nS2NihssArrivalDysarthria\nStroke severity (NIHSS score) on arrival : slu...\n\n\n30\nS2NihssArrivalExtinctionInattention\nStroke severity (NIHSS score) on arrival : abi...\n\n\n31\nS2NihssArrivalFacialPalsy\nStroke severity (NIHSS score) on arrival : fac...\n\n\n32\nS2NihssArrivalLimbAtaxia\nStroke severity (NIHSS score) on arrival : lim...\n\n\n33\nS2NihssArrivalLocCommands\nStroke severity (NIHSS score) on arrival : lev...\n\n\n34\nS2NihssArrivalLocQuestions\nStroke severity (NIHSS score) on arrival : lev...\n\n\n35\nS2NihssArrivalMotorArmLeft\nStroke severity (NIHSS score) on arrival : mov...\n\n\n36\nS2NihssArrivalMotorArmRight\nStroke severity (NIHSS score) on arrival : mov...\n\n\n37\nS2NihssArrivalMotorLegLeft\nStroke severity (NIHSS score) on arrival : mov...\n\n\n38\nS2NihssArrivalMotorLegRight\nStroke severity (NIHSS score) on arrival : mov...\n\n\n39\nS2NihssArrivalSensory\nStroke severity (NIHSS score) on arrival : sen...\n\n\n40\nS2NihssArrivalVisual\nStroke severity (NIHSS score) on arrival : bli...\n\n\n41\nS2RankinBeforeStroke\nPre-stroke disability level (Modified Rankin S...\n\n\n42\nStroke severity group_1. No stroke symtpoms\nStroke severity 1 - no symptoms\n\n\n43\nStroke severity group_2. Minor\nStroke severity 2 - minor\n\n\n44\nStroke severity group_3. Moderate\nStroke severity 3 - moderate\n\n\n45\nStroke severity group_4. Moderate to severe\nStroke severity 4 - moderate to severe\n\n\n46\nStroke severity group_5. Severe\nStroke severity 5 - severe\n\n\n47\nStroke Type_I\nIschemic stroke\n\n\n48\nStroke Type_PIH\nPregnancy-induced Hypertension stroke\n\n\n49\nTIA\nStroke was a transient ischaemic attack (\"mini...\nDivide the main stroke dataset into features and labels.\nRemember - we’re trying to predict whether patients are given clotbusting treatment or not.\nWhat column contains that information?\nX = data.drop('Clotbuster given', axis=1)\ny = data['Clotbuster given']\nSplit the data into training and testing sets.\nStart with a train/test split of 80/20.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=127)\nFit a Decision Tree model.\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier()\nUse the trained model to predict labels in both training and test sets, and calculate and compare accuracy.\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 74.799%\nCalculate the additional model metrics for the test data only.\nReturn the ‘micro’ average in each case.\nprecision_score_test = precision_score(y_test, y_pred_test, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nPrecision score for testing data = 74.799%\nRecall (sensitivity) score for testing data = 74.799%\nSpecificity score for testing data = 78.404%\n# we learn this in a later session - but a nice way to compare and contrast\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))\n\n              precision    recall  f1-score   support\n\n         0.0       0.78      0.78      0.78       215\n         1.0       0.70      0.71      0.70       158\n\n    accuracy                           0.75       373\n   macro avg       0.74      0.74      0.74       373\nweighted avg       0.75      0.75      0.75       373\nPlot the decision tree.\nfig, ax = plt.subplots(figsize=(14,10))\n\nfig = plot_tree(\n    model,\n    feature_names = X.columns.tolist(),\n    class_names=[\"Not Given Clotbuster\", \"Given Clotbuster\"],\n    filled = True,\n    ax=ax\n)\n\nplt.show()",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_stroke_exercise_SOLUTION.html#core---fitting-and-evaluating-a-decision-tree",
    "href": "4d_decision_tree_stroke_exercise_SOLUTION.html#core---fitting-and-evaluating-a-decision-tree",
    "title": "5  Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "precision\nspecificity\nrecall (sensitivity)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_stroke_exercise_SOLUTION.html#extension---refining-your-decision-tree",
    "href": "4d_decision_tree_stroke_exercise_SOLUTION.html#extension---refining-your-decision-tree",
    "title": "5  Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)",
    "section": "5.2 Extension - Refining Your Decision Tree",
    "text": "5.2 Extension - Refining Your Decision Tree\nLet’s experiment by changing a few parameters.\n\n5.2.1 Maximum Depth\nTry changing the value of the ‘max_depth’ parameter when setting up your DecisionTreeClassifier.\nOutput the - accuracy (train and test) - precision (test) - specificity (test) - and recall (sensitivity) (test)\nof this new classifier.\nWe’ve switched to ‘macro’ average from here on in - take a look at the second half of session 4D for more detail on this.\n\nmodel = DecisionTreeClassifier(max_depth=5)\nmodel.fit(X_train, y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\nprecision_score_test = precision_score(y_test, y_pred_test, average='macro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='macro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nAccuracy of predicting training data = 80.591%\nAccuracy of predicting testing data = 80.965%\nPrecision score for testing data = 80.767%\nRecall (sensitivity) score for testing data = 81.475%\nSpecificity score for testing data = 87.500%\n\n\n\nmodel = DecisionTreeClassifier(max_depth=3)\nmodel.fit(X_train, y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\nprecision_score_test = precision_score(y_test, y_pred_test, average='macro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='macro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nAccuracy of predicting training data = 78.308%\nAccuracy of predicting testing data = 77.748%\nPrecision score for testing data = 77.338%\nRecall (sensitivity) score for testing data = 77.845%\nSpecificity score for testing data = 83.000%\n\n\nThis is getting very tiresome - let’s write a function!\n\ndef fit_dt_model(model):\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    precision_score_test = precision_score(y_test, y_pred_test, average='micro')\n    recall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\n    specificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n    # Note that the lecture slides show this displayed as a float using :3f\n    # We can instead use :.3% to format the number as a percentage to 3 decimal places.\n    print(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\n    print(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\n    print(f\"Precision score for testing data = {precision_score_test:.3%}\")\n    print(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\n    print(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nfit_dt_model(model = DecisionTreeClassifier(max_depth=8, random_state=42))\n\nAccuracy of predicting training data = 86.367%\nAccuracy of predicting testing data = 78.820%\nPrecision score for testing data = 78.820%\nRecall (sensitivity) score for testing data = 78.820%\nSpecificity score for testing data = 83.010%\n\n\n\nfit_dt_model(model = DecisionTreeClassifier(max_depth=4, random_state=42))\n\nAccuracy of predicting training data = 79.449%\nAccuracy of predicting testing data = 80.161%\nPrecision score for testing data = 80.161%\nRecall (sensitivity) score for testing data = 80.161%\nSpecificity score for testing data = 80.786%\n\n\n\n\n5.2.2 Minimum Samples\nTry changing the values of ‘min_samples_split’ (the default value is 2).\n\nfit_dt_model(model =DecisionTreeClassifier(min_samples_split=6, random_state=42))\n\nAccuracy of predicting training data = 95.097%\nAccuracy of predicting testing data = 74.531%\nPrecision score for testing data = 74.531%\nRecall (sensitivity) score for testing data = 74.531%\nSpecificity score for testing data = 76.549%\n\n\n\nfit_dt_model(model = DecisionTreeClassifier(min_samples_split=4, random_state=42))\n\nAccuracy of predicting training data = 97.381%\nAccuracy of predicting testing data = 76.408%\nPrecision score for testing data = 76.408%\nRecall (sensitivity) score for testing data = 76.408%\nSpecificity score for testing data = 79.535%\n\n\nNow try adjusting ‘min_samples_leaf’ (the default is 1).\n\nfit_dt_model(model = DecisionTreeClassifier(min_samples_leaf=6, random_state=42))\n\nAccuracy of predicting training data = 87.710%\nAccuracy of predicting testing data = 77.748%\nPrecision score for testing data = 77.748%\nRecall (sensitivity) score for testing data = 77.748%\nSpecificity score for testing data = 77.049%\n\n\n\nfit_dt_model(model = DecisionTreeClassifier(min_samples_split=3, random_state=42))\n\nAccuracy of predicting training data = 99.127%\nAccuracy of predicting testing data = 73.727%\nPrecision score for testing data = 73.727%\nRecall (sensitivity) score for testing data = 73.727%\nSpecificity score for testing data = 76.233%\n\n\n\n\n5.2.3 Split Criterion\nCompare the performance when using\n\nGini Impurity\nEntropy\nLog Loss\n\n\nprint(\"**Gini**\")\nfit_dt_model(model = DecisionTreeClassifier(criterion=\"gini\", random_state=42))\n\n**Gini**\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 75.335%\nPrecision score for testing data = 75.335%\nRecall (sensitivity) score for testing data = 75.335%\nSpecificity score for testing data = 78.605%\n\n\n\nprint(\"**Entropy**\")\nfit_dt_model(model = DecisionTreeClassifier(criterion=\"entropy\", random_state=42))\n\n**Entropy**\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 77.212%\nPrecision score for testing data = 77.212%\nRecall (sensitivity) score for testing data = 77.212%\nSpecificity score for testing data = 79.545%\n\n\n\nprint(\"**Log Loss**\")\nfit_dt_model(model = DecisionTreeClassifier(criterion=\"log_loss\", random_state=42))\n\n**Log Loss**\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 77.212%\nPrecision score for testing data = 77.212%\nRecall (sensitivity) score for testing data = 77.212%\nSpecificity score for testing data = 79.545%",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_stroke_exercise_SOLUTION.html#comparing-performance-with-a-logistic-regression-model",
    "href": "4d_decision_tree_stroke_exercise_SOLUTION.html#comparing-performance-with-a-logistic-regression-model",
    "title": "5  Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)",
    "section": "5.3 Comparing Performance with a Logistic Regression Model",
    "text": "5.3 Comparing Performance with a Logistic Regression Model\nCopy your code in from last week’s logistic regression exercise (or write this in from scratch - there isn’t much that is different to the decision tree model!).\nRemember - you will need to standardise the data for the logistic regression model!\nLook at these additional metrics as well:\n\nprecision\nspecificity\nrecall (sensitivity)\n\n\nscaler = StandardScaler()\n\nX_train_stand = scaler.fit_transform(X_train)\nX_test_stand = scaler.fit_transform(X_test)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_stand, y_train)\ny_pred_train = model.predict(X_train_stand)\ny_pred_test = model.predict(X_test_stand)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\nprecision_score_test = precision_score(y_test, y_pred_test, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nAccuracy of predicting training data = 81.531%\nAccuracy of predicting testing data = 82.574%\nPrecision score for testing data = 82.574%\nRecall (sensitivity) score for testing data = 82.574%\nSpecificity score for testing data = 83.784%\n\n\nUse the cell below to write out an interpretation of the performance of the logistic regression model and the decision tree.\nThink about the presence of false positives and false negatives.\nWhich might you be more interested in minimizing in this model?\nHint - giving thrombolysis to good candidates for it can lead to less disability after stroke and improved outcomes. However, there is a risk that giving thrombolysis to the wrong person could lead to additional bleeding on the brain and worse outcomes. What might you want to balance?\nNo answer given",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_decision_tree_stroke_exercise_SOLUTION.html#challenge-exercises",
    "href": "4d_decision_tree_stroke_exercise_SOLUTION.html#challenge-exercises",
    "title": "5  Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)",
    "section": "5.4 Challenge Exercises",
    "text": "5.4 Challenge Exercises\n\n5.4.1 Bonus Exercise 1\nHave a read of this article on feature importance in decision trees: Article Link\nIn particular, make sure you read the section “Pros and cons of using Gini importance” so you can understand some of the things you need to keep in mind when looking at feature importance in trees.\nWe can access the feature importance by running the following code:\n\n# modify this code to point towards your decision tree model object (make sure that object\n# was using the gini index as the criteria)\nmodel_dt = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\nmodel_dt = model_dt.fit(X_train, y_train)\n\nfeature_importances_dt = model_dt.feature_importances_\n\nfeature_importances_dt\n\narray([0.01364131, 0.00456272, 0.00586152, 0.01439673, 0.00110657,\n       0.01397118, 0.01249914, 0.00889   , 0.08702842, 0.00186827,\n       0.05481699, 0.        , 0.01165528, 0.01608714, 0.00869807,\n       0.00445253, 0.01247073, 0.        , 0.00643961, 0.01070718,\n       0.01356352, 0.0064616 , 0.00336373, 0.00394369, 0.00672135,\n       0.01659303, 0.        , 0.        , 0.00157772, 0.        ,\n       0.00463295, 0.        , 0.        , 0.1380051 , 0.0602852 ,\n       0.2124907 , 0.01367527, 0.01015513, 0.00578212, 0.01669722,\n       0.02725587, 0.02481184, 0.02105913, 0.02376351, 0.02466254,\n       0.01465469, 0.00871107, 0.02615918, 0.003855  , 0.02196545])\n\n\nHow does this compare to the feature importance for your logistic regression?\nHINT: This is quite different from the approach used in the model.\nYou’ll need to look back at the exercises form last week.\n\n# modify this code to point towards your logistic regression model object\nscaler = StandardScaler()\n\nX_train_stand = scaler.fit_transform(X_train)\nX_test_stand = scaler.fit_transform(X_test)\n\nmodel_lr = LogisticRegression()\nmodel_lr = model_lr.fit(X_train_stand, y_train)\n\n# Examine feature weights and sort by most influential\nco_eff = model_lr.coef_[0]\n\nco_eff_df = pd.DataFrame()\nco_eff_df['feature'] = list(X)\nco_eff_df['co_eff'] = co_eff\nco_eff_df['abs_co_eff'] = np.abs(co_eff)\nco_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=True)\n\nco_eff_df\n\n\n\n\n\n\n\n\n\nfeature\nco_eff\nabs_co_eff\n\n\n\n\n33\nStroke Type_PIH\n-1.138691\n1.138691\n\n\n32\nStroke Type_I\n1.138691\n1.138691\n\n\n28\nStroke severity group_2. Minor\n-0.655217\n0.655217\n\n\n29\nStroke severity group_3. Moderate\n0.558470\n0.558470\n\n\n34\nS2RankinBeforeStroke\n-0.507321\n0.507321\n\n\n47\nS2NihssArrivalBestLanguage\n0.458131\n0.458131\n\n\n10\nOnset Time Known Type_BE\n-0.290595\n0.290595\n\n\n25\nAnticoag before stroke_1\n-0.274249\n0.274249\n\n\n12\nOnset Time Known Type_P\n0.266564\n0.266564\n\n\n27\nStroke severity group_1. No stroke symtpoms\n-0.248087\n0.248087\n\n\n8\nAge\n-0.244158\n0.244158\n\n\n24\nAnticoag before stroke_0\n0.235502\n0.235502\n\n\n16\nHypertension\n0.235309\n0.235309\n\n\n17\nAtrial Fib\n-0.230893\n0.230893\n\n\n42\nS2NihssArrivalMotorArmRight\n0.219366\n0.219366\n\n\n49\nS2NihssArrivalExtinctionInattention\n0.195696\n0.195696\n\n\n37\nS2NihssArrivalLocCommands\n-0.189860\n0.189860\n\n\n40\nS2NihssArrivalFacialPalsy\n0.187638\n0.187638\n\n\n30\nStroke severity group_4. Moderate to severe\n0.185069\n0.185069\n\n\n44\nS2NihssArrivalMotorLegRight\n-0.181887\n0.181887\n\n\n36\nS2NihssArrivalLocQuestions\n0.168651\n0.168651\n\n\n0\nHosp_1\n0.168301\n0.168301\n\n\n21\nAntiplatelet_0\n0.161005\n0.161005\n\n\n35\nS2NihssArrival\n-0.142778\n0.142778\n\n\n19\nTIA\n-0.138909\n0.138909\n\n\n43\nS2NihssArrivalMotorLegLeft\n0.131229\n0.131229\n\n\n14\n2+ comorbidotes\n-0.129622\n0.129622\n\n\n20\nCo-mordity\n-0.124540\n0.124540\n\n\n41\nS2NihssArrivalMotorArmLeft\n0.120298\n0.120298\n\n\n5\nHosp_6\n0.116469\n0.116469\n\n\n46\nS2NihssArrivalSensory\n0.114362\n0.114362\n\n\n45\nS2NihssArrivalLimbAtaxia\n-0.099983\n0.099983\n\n\n3\nHosp_4\n-0.093776\n0.093776\n\n\n2\nHosp_3\n-0.093303\n0.093303\n\n\n23\nAntiplatelet_NK\n-0.092551\n0.092551\n\n\n18\nDiabetes\n0.090665\n0.090665\n\n\n11\nOnset Time Known Type_NK\n0.069936\n0.069936\n\n\n7\nMale\n0.068703\n0.068703\n\n\n38\nS2NihssArrivalBestGaze\n0.065874\n0.065874\n\n\n22\nAntiplatelet_1\n-0.059578\n0.059578\n\n\n4\nHosp_5\n-0.057283\n0.057283\n\n\n9\n80+\n-0.053481\n0.053481\n\n\n31\nStroke severity group_5. Severe\n-0.047326\n0.047326\n\n\n48\nS2NihssArrivalDysarthria\n0.044316\n0.044316\n\n\n6\nHosp_7\n-0.044271\n0.044271\n\n\n39\nS2NihssArrivalVisual\n0.042245\n0.042245\n\n\n15\nCongestive HF\n0.030292\n0.030292\n\n\n26\nAnticoag before stroke_NK\n0.021401\n0.021401\n\n\n1\nHosp_2\n0.010540\n0.010540\n\n\n13\n# Comorbidities\n-0.001275\n0.001275\n\n\n\n\n\n\n\n\nCan you create two graphs showing feature importance for the two models?\nInstead of using the plot code used in the linked article, try looking up the barh function from matplotlib.\nTry ordering your plot so that the features with the most importance are at the top.\n\n# Sort the feature importances from greatest to least using the sorted indices\nsorted_indices = feature_importances_dt.argsort()[::-1]\n\nsorted_feature_names =[X.columns[i] for i in sorted_indices]\n\nsorted_importances = feature_importances_dt[sorted_indices]\n\n# Create a bar plot of the feature importances\nfig, ax = plt.subplots(figsize=(15,10))\nax.barh(width=sorted_importances, y=sorted_feature_names)\nax.invert_yaxis()\nax = plt.title(\"Feature Importance - Stroke Dataset - Decision Tree\")\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15,10))\nax.barh(width=co_eff_df['abs_co_eff'], y=co_eff_df['feature'])\nax.invert_yaxis()\nax = plt.title(\"Absolute Feature Importance - Stroke Dataset - Logistic Regression\")\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15,10))\nax.barh(width=co_eff_df['co_eff'], y=co_eff_df['feature'])\nax.invert_yaxis()\nax = plt.title(\"Feature Importance - Stroke Dataset - Logistic Regression\")\n\n\n\n\n\n\n\n\n\n\n5.4.2 Bonus Exercise 2\nCan you improve accuracy of your decision tree model by changing the size of your train / test split?\nNOTE - the examples below just show the impact of changing the train/test split.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\nfit_dt_model(model = DecisionTreeClassifier(random_state=42))\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 74.866%\nPrecision score for testing data = 74.866%\nRecall (sensitivity) score for testing data = 74.866%\nSpecificity score for testing data = 81.250%\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nfit_dt_model(model = DecisionTreeClassifier(random_state=42))\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 76.923%\nPrecision score for testing data = 76.923%\nRecall (sensitivity) score for testing data = 76.923%\nSpecificity score for testing data = 80.588%\n\n\n\n\n5.4.3 Bonus Exercise 3\nTry dropping some features from your data.\nCan you improve the performance of your model this way?\nNOTE: This solution just shows selecting a subset of features - not the best ones necessarily!\n\nX.columns\n\nIndex(['Hosp_1', 'Hosp_2', 'Hosp_3', 'Hosp_4', 'Hosp_5', 'Hosp_6', 'Hosp_7',\n       'Male', 'Age', '80+', 'Onset Time Known Type_BE',\n       'Onset Time Known Type_NK', 'Onset Time Known Type_P',\n       '# Comorbidities', '2+ comorbidotes', 'Congestive HF', 'Hypertension',\n       'Atrial Fib', 'Diabetes', 'TIA', 'Co-mordity', 'Antiplatelet_0',\n       'Antiplatelet_1', 'Antiplatelet_NK', 'Anticoag before stroke_0',\n       'Anticoag before stroke_1', 'Anticoag before stroke_NK',\n       'Stroke severity group_1. No stroke symtpoms',\n       'Stroke severity group_2. Minor', 'Stroke severity group_3. Moderate',\n       'Stroke severity group_4. Moderate to severe',\n       'Stroke severity group_5. Severe', 'Stroke Type_I', 'Stroke Type_PIH',\n       'S2RankinBeforeStroke', 'S2NihssArrival', 'S2NihssArrivalLocQuestions',\n       'S2NihssArrivalLocCommands', 'S2NihssArrivalBestGaze',\n       'S2NihssArrivalVisual', 'S2NihssArrivalFacialPalsy',\n       'S2NihssArrivalMotorArmLeft', 'S2NihssArrivalMotorArmRight',\n       'S2NihssArrivalMotorLegLeft', 'S2NihssArrivalMotorLegRight',\n       'S2NihssArrivalLimbAtaxia', 'S2NihssArrivalSensory',\n       'S2NihssArrivalBestLanguage', 'S2NihssArrivalDysarthria',\n       'S2NihssArrivalExtinctionInattention'],\n      dtype='object')\n\n\n\nfit_dt_model(model = DecisionTreeClassifier(max_depth=5, random_state=42))\n\nAccuracy of predicting training data = 80.583%\nAccuracy of predicting testing data = 80.680%\nPrecision score for testing data = 80.680%\nRecall (sensitivity) score for testing data = 80.680%\nSpecificity score for testing data = 89.895%\n\n\n\n# This isn't necessarily the best subset of features to use - it's just an example!\nX_reduced = X[['S2NihssArrival', 'Stroke Type_PIH', 'Age', 'S2RankinBeforeStroke']]\n\nX_reduced_train, X_reduced_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.1, random_state=42)\n\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\nmodel.fit(X_reduced_train, y_train)\ny_pred_train = model.predict(X_reduced_train)\ny_pred_test = model.predict(X_reduced_test)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\nprecision_score_test = precision_score(y_test, y_pred_test, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nAccuracy of predicting training data = 79.821%\nAccuracy of predicting testing data = 77.540%\nPrecision score for testing data = 77.540%\nRecall (sensitivity) score for testing data = 77.540%\nSpecificity score for testing data = 86.408%",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercise Solution: Decision Trees (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html",
    "href": "4d_random_forest_titanic.html",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "",
    "text": "6.1 Divide into X (features) and y (labels)\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#divide-into-training-and-tets-sets",
    "href": "4d_random_forest_titanic.html#divide-into-training-and-tets-sets",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "6.2 Divide into training and tets sets",
    "text": "6.2 Divide into training and tets sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#fit-random-forest-model",
    "href": "4d_random_forest_titanic.html#fit-random-forest-model",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "6.3 Fit random forest model",
    "text": "6.3 Fit random forest model\n\nmodel = RandomForestClassifier(random_state=42)\nmodel = model.fit(X_train,y_train)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#predict-values",
    "href": "4d_random_forest_titanic.html#predict-values",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "6.4 Predict values",
    "text": "6.4 Predict values\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#calculate-accuracy",
    "href": "4d_random_forest_titanic.html#calculate-accuracy",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "6.5 Calculate accuracy",
    "text": "6.5 Calculate accuracy\n\n# The shorthand below says to check each predicted y value against the actual\n# y value in the training data.  This gives a list of True and False values\n# for each prediction, where True indicates the predicted value matches the\n# actual value.  Then we take the mean of these Boolean values, which gives\n# us a proportion (where if all values were True, the proportion would be 1.0)\n# If you want to see why that works, just uncomment the following line of code\n# to see what y_pred_train == y_train is doing.\n# print (y_pred_train == y_train)\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train}')\nprint (f'Accuracy of predicting test data = {accuracy_test}')\n\nAccuracy of predicting training data = 0.9835329341317365\nAccuracy of predicting test data = 0.8026905829596412\n\n\n\n# Show first ten predicted classes\nclasses = model.predict(X_test)\nclasses[0:10]\n\narray([0., 0., 0., 1., 0., 1., 1., 0., 1., 1.])\n\n\n\n# Show first ten predicted probabilities\nprobabilities = model.predict_proba(X_test)\nprobabilities[0:10]\n\narray([[0.76     , 0.24     ],\n       [0.98     , 0.02     ],\n       [0.94     , 0.06     ],\n       [0.04     , 0.96     ],\n       [0.67     , 0.33     ],\n       [0.08     , 0.92     ],\n       [0.1572482, 0.8427518],\n       [0.93     , 0.07     ],\n       [0.32     , 0.68     ],\n       [0.12     , 0.88     ]])",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#calculate-f1-score",
    "href": "4d_random_forest_titanic.html#calculate-f1-score",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "6.6 Calculate F1 Score",
    "text": "6.6 Calculate F1 Score\n\nf1_score(y_test, y_pred_test, average=None)\n\narray([0.83703704, 0.75      ])\n\n\n\nf1_score(y_test, y_pred_test, average='micro')\n\n0.8026905829596412\n\n\n\nf1_score(y_test, y_pred_test, average='macro')\n\n0.7935185185185185\n\n\n\nf1_score(y_test, y_pred_test, average='weighted')\n\n0.8023002823451255",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#plot-tree",
    "href": "4d_random_forest_titanic.html#plot-tree",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "6.7 Plot tree",
    "text": "6.7 Plot tree\nhttps://stackoverflow.com/questions/40155128/plot-trees-for-a-random-forest-in-python-with-scikit-learn\n\nfig, axes = plt.subplots(nrows = 1, ncols = 5, figsize = (10,2), dpi=900)\nfor index in range(0, 5):\n    plot_tree(model.estimators_[index],\n    feature_names=data.drop('Survived',axis=1).columns.tolist(),\n    class_names=['Died', 'Survived'],\n                   filled = True,\n                   ax = axes[index]);\n\n    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_titanic.html#hyperparameters",
    "href": "4d_random_forest_titanic.html#hyperparameters",
    "title": "6  Random Forests for Classification (Titanic Dataset)",
    "section": "7.1 Hyperparameters",
    "text": "7.1 Hyperparameters\n\n7.1.1 n estimators (trees per forest)\n\naccuracy_results = []\n\nfor i in range(10, 500, 10):\n    model = model = RandomForestClassifier(n_estimators=i, random_state=42)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'n_estimators': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='n_estimators'),\n        x='n_estimators', y='value', color='variable')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\npd.DataFrame(accuracy_results).set_index(\"n_estimators\").sort_values(by=[\"accuracy_test\"], ascending=False)\n\n\n\n\n\n\n\n\n\naccuracy_train\naccuracy_test\n\n\nn_estimators\n\n\n\n\n\n\n30\n0.980539\n0.807175\n\n\n40\n0.980539\n0.807175\n\n\n50\n0.982036\n0.802691\n\n\n20\n0.980539\n0.802691\n\n\n10\n0.976048\n0.798206\n\n\n60\n0.983533\n0.798206\n\n\n70\n0.983533\n0.798206\n\n\n80\n0.983533\n0.798206\n\n\n90\n0.983533\n0.798206\n\n\n120\n0.983533\n0.793722\n\n\n100\n0.983533\n0.793722\n\n\n110\n0.983533\n0.793722\n\n\n210\n0.983533\n0.793722\n\n\n450\n0.983533\n0.789238\n\n\n440\n0.983533\n0.789238\n\n\n260\n0.983533\n0.789238\n\n\n240\n0.983533\n0.789238\n\n\n230\n0.983533\n0.789238\n\n\n220\n0.983533\n0.789238\n\n\n250\n0.983533\n0.789238\n\n\n200\n0.983533\n0.789238\n\n\n180\n0.983533\n0.789238\n\n\n170\n0.983533\n0.789238\n\n\n160\n0.983533\n0.789238\n\n\n150\n0.983533\n0.789238\n\n\n190\n0.983533\n0.789238\n\n\n130\n0.983533\n0.789238\n\n\n140\n0.983533\n0.789238\n\n\n420\n0.983533\n0.784753\n\n\n400\n0.983533\n0.784753\n\n\n410\n0.983533\n0.784753\n\n\n460\n0.983533\n0.784753\n\n\n430\n0.983533\n0.784753\n\n\n380\n0.983533\n0.784753\n\n\n470\n0.983533\n0.784753\n\n\n480\n0.983533\n0.784753\n\n\n390\n0.983533\n0.784753\n\n\n350\n0.983533\n0.784753\n\n\n370\n0.983533\n0.784753\n\n\n360\n0.983533\n0.784753\n\n\n340\n0.983533\n0.784753\n\n\n330\n0.983533\n0.784753\n\n\n320\n0.983533\n0.784753\n\n\n310\n0.983533\n0.784753\n\n\n300\n0.983533\n0.784753\n\n\n290\n0.983533\n0.784753\n\n\n280\n0.983533\n0.784753\n\n\n270\n0.983533\n0.784753\n\n\n490\n0.983533\n0.784753\n\n\n\n\n\n\n\n\n\n\n7.1.2 n estimators (trees per forest) - with max depth of 8\n\naccuracy_results = []\n\nfor i in range(10, 200, 10):\n    model = RandomForestClassifier(n_estimators=i, random_state=42, max_depth=8)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'n_estimators': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='n_estimators'),\n        x='n_estimators', y='value', color='variable')\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\npd.DataFrame(accuracy_results).set_index(\"n_estimators\").sort_values(by=[\"accuracy_test\"], ascending=False)\n\n\n\n\n\n\n\n\n\naccuracy_train\naccuracy_test\n\n\nn_estimators\n\n\n\n\n\n\n190\n0.919162\n0.829596\n\n\n180\n0.922156\n0.829596\n\n\n170\n0.923653\n0.829596\n\n\n150\n0.922156\n0.829596\n\n\n140\n0.922156\n0.829596\n\n\n90\n0.919162\n0.829596\n\n\n110\n0.920659\n0.825112\n\n\n160\n0.923653\n0.825112\n\n\n130\n0.920659\n0.825112\n\n\n10\n0.892216\n0.825112\n\n\n80\n0.920659\n0.825112\n\n\n70\n0.920659\n0.825112\n\n\n50\n0.919162\n0.825112\n\n\n100\n0.920659\n0.825112\n\n\n120\n0.920659\n0.820628\n\n\n20\n0.902695\n0.816143\n\n\n40\n0.913174\n0.816143\n\n\n60\n0.920659\n0.811659\n\n\n30\n0.914671\n0.811659\n\n\n\n\n\n\n\n\n\nnp.random.seed(42)\n\nbest_n_estimators = pd.DataFrame(accuracy_results).sort_values(by=[\"accuracy_test\"], ascending=False).head(1)['n_estimators'].values[0]\n\nmodel = RandomForestClassifier(n_estimators=best_n_estimators, random_state=42, max_depth=8)\nmodel.fit(X_train,y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nroc_curve = RocCurveDisplay.from_estimator(\n    model, X_test, y_test\n)\n\nfig = roc_curve.figure_\nax = roc_curve.ax_\n\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n).plot()\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test,\n        normalize='true'\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n).plot()",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Forests for Classification (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_stroke_exercise_SOLUTION.html",
    "href": "4d_random_forest_stroke_exercise_SOLUTION.html",
    "title": "7  Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "7.1 Core Tasks\nRun the code below to import the dataset and the libraries we need.\nimport pandas as pd\nimport numpy as np\n\n# import preprocessing functions\nfrom sklearn.model_selection import train_test_split\n\n# Import machine learning model of interest\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import package to investigate our loaded dataframe\nfrom ydata_profiling import ProfileReport\n\n# Import functions for evaluating model\nfrom sklearn.metrics import recall_score, precision_score, f1_score, classification_report, \\\n                            confusion_matrix, ConfusionMatrixDisplay, auc, roc_curve\nfrom sklearn.inspection import permutation_importance\n\n# Imports relating to logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Imports relating to plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Download data\n# (not required if running locally and have previously downloaded data)\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '2004_titanic/master/jupyter_notebooks/data/hsma_stroke.csv'\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data to data subfolder\n    data.to_csv(data_directory + 'hsma_stroke.csv', index=False)\n\n# Load data\ndata = pd.read_csv('data/hsma_stroke.csv')\n# Make all data 'float' type\ndata = data.astype(float)\nLook at an overview of the data. Choose whichever method you like.\n(e.g. something like the ‘head’ or ‘describe’ method from pandas.)\ndata.describe()\n\n\n\n\n\n\n\n\n\nClotbuster given\nHosp_1\nHosp_2\nHosp_3\nHosp_4\nHosp_5\nHosp_6\nHosp_7\nMale\nAge\n...\nS2NihssArrivalFacialPalsy\nS2NihssArrivalMotorArmLeft\nS2NihssArrivalMotorArmRight\nS2NihssArrivalMotorLegLeft\nS2NihssArrivalMotorLegRight\nS2NihssArrivalLimbAtaxia\nS2NihssArrivalSensory\nS2NihssArrivalBestLanguage\nS2NihssArrivalDysarthria\nS2NihssArrivalExtinctionInattention\n\n\n\n\ncount\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n...\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n\n\nmean\n0.403330\n0.159506\n0.142320\n0.154672\n0.165414\n0.055854\n0.113319\n0.208915\n0.515575\n74.553706\n...\n1.114930\n1.002148\n0.963480\n0.963480\n0.910849\n0.216971\n0.610097\n0.944146\n0.739527\n0.566595\n\n\nstd\n0.490698\n0.366246\n0.349472\n0.361689\n0.371653\n0.229701\n0.317068\n0.406643\n0.499892\n12.280576\n...\n0.930527\n1.479211\n1.441594\n1.406501\n1.380606\n0.522643\n0.771932\n1.121379\n0.731083\n0.794000\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n40.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n67.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n76.000000\n...\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n83.000000\n...\n2.000000\n2.000000\n2.000000\n2.000000\n2.000000\n0.000000\n1.000000\n2.000000\n1.000000\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n100.000000\n...\n3.000000\n4.000000\n4.000000\n4.000000\n4.000000\n2.000000\n2.000000\n3.000000\n2.000000\n2.000000\n\n\n\n\n8 rows × 51 columns\nLoad in the ‘stroke_data_feature_descriptions’ dataframe and view that too - you can just view the whole dataframe with pandas rather than using the ProfileReport.\nHint: it’s in the same folder as the hsma_stroke.csv dataset we imported above.\nstroke_data_feature_descriptions_df = pd.read_csv('../datasets/stroke_data_feature_descriptions.csv')\n\nstroke_data_feature_descriptions_df\n\n\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\n0\n# Comorbidities\nNumber of comorbidities\n\n\n1\n2+ comorbidotes\nIf the patient had at least two comorbidities\n\n\n2\n80+\nIf the patient is aged 80 or over\n\n\n3\nAge\nAge of patient\n\n\n4\nAnticoag before stroke_0\nDid not take anticoagulants before stroke\n\n\n5\nAnticoag before stroke_1\nDid take anticoagulants before stroke\n\n\n6\nAnticoag before stroke_NK\nNot known if was taking anticoagulants before ...\n\n\n7\nAntiplatelet_0\nDid not receive antiplatelet treatment\n\n\n8\nAntiplatelet_1\nDid receive antiplatelet treatment\n\n\n9\nAntiplatelet_NK\nNot known if received antiplatelet treatment\n\n\n10\nAtrial Fib\nPatient has atrial fibrillation\n\n\n11\nCo-mordity\nIf the patient has any comorbidities at all\n\n\n12\nCongestive HF\nPatient has congestive heart failure\n\n\n13\nDiabetes\nPatient has diabetes\n\n\n14\nHosp_1\nTaken to hospital 1\n\n\n15\nHosp_2\nTaken to hospital 2\n\n\n16\nHosp_3\nTaken to hospital 3\n\n\n17\nHosp_4\nTaken to hospital 4\n\n\n18\nHosp_5\nTaken to hospital 5\n\n\n19\nHosp_6\nTaken to hospital 6\n\n\n20\nHosp_7\nTaken to hospital 7\n\n\n21\nHypertension\nPatient has hypertension\n\n\n22\nMale\nPatient is male\n\n\n23\nOnset Time Known Type_BE\nOnset time type is Best Estimate\n\n\n24\nOnset Time Known Type_NK\nOnset time type is Not Known\n\n\n25\nOnset Time Known Type_P\nOnset time type is Precise\n\n\n26\nS2NihssArrival\nStroke severity (NIHSS score) on arrival : tot...\n\n\n27\nS2NihssArrivalBestGaze\nStroke severity (NIHSS score) on arrival : eye...\n\n\n28\nS2NihssArrivalBestLanguage\nStroke severity (NIHSS score) on arrival : com...\n\n\n29\nS2NihssArrivalDysarthria\nStroke severity (NIHSS score) on arrival : slu...\n\n\n30\nS2NihssArrivalExtinctionInattention\nStroke severity (NIHSS score) on arrival : abi...\n\n\n31\nS2NihssArrivalFacialPalsy\nStroke severity (NIHSS score) on arrival : fac...\n\n\n32\nS2NihssArrivalLimbAtaxia\nStroke severity (NIHSS score) on arrival : lim...\n\n\n33\nS2NihssArrivalLocCommands\nStroke severity (NIHSS score) on arrival : lev...\n\n\n34\nS2NihssArrivalLocQuestions\nStroke severity (NIHSS score) on arrival : lev...\n\n\n35\nS2NihssArrivalMotorArmLeft\nStroke severity (NIHSS score) on arrival : mov...\n\n\n36\nS2NihssArrivalMotorArmRight\nStroke severity (NIHSS score) on arrival : mov...\n\n\n37\nS2NihssArrivalMotorLegLeft\nStroke severity (NIHSS score) on arrival : mov...\n\n\n38\nS2NihssArrivalMotorLegRight\nStroke severity (NIHSS score) on arrival : mov...\n\n\n39\nS2NihssArrivalSensory\nStroke severity (NIHSS score) on arrival : sen...\n\n\n40\nS2NihssArrivalVisual\nStroke severity (NIHSS score) on arrival : bli...\n\n\n41\nS2RankinBeforeStroke\nPre-stroke disability level (Modified Rankin S...\n\n\n42\nStroke severity group_1. No stroke symtpoms\nStroke severity 1 - no symptoms\n\n\n43\nStroke severity group_2. Minor\nStroke severity 2 - minor\n\n\n44\nStroke severity group_3. Moderate\nStroke severity 3 - moderate\n\n\n45\nStroke severity group_4. Moderate to severe\nStroke severity 4 - moderate to severe\n\n\n46\nStroke severity group_5. Severe\nStroke severity 5 - severe\n\n\n47\nStroke Type_I\nIschemic stroke\n\n\n48\nStroke Type_PIH\nPregnancy-induced Hypertension stroke\n\n\n49\nTIA\nStroke was a transient ischaemic attack (\"mini...\nDivide the main stroke dataset into features and labels.\nRemember - we’re trying to predict whether patients are given clotbusting treatment or not.\nWhat column contains that information?\nX = data.drop('Clotbuster given', axis=1)\ny = data['Clotbuster given']\nSplit the data into training and testing sets.\nStart with a train/test split of 80/20.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nFit a random forest model.\nmodel = RandomForestClassifier(random_state=42)\nmodel = model.fit(X_train, y_train)\nUse the trained model to predict labels in both training and test sets, and calculate and compare accuracy.\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 81.233%\nCalculate the additional model metrics for the test data only.\nReturn the ‘micro’ average in each case.\nprecision_score_test = precision_score(y_test, y_pred_test, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\nf1_score_test = f1_score(y_test, y_pred_test, average='micro')\n\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\nprint(f\"f1 score for testing data = {f1_score_test:.3%}\")\n\nPrecision score for testing data = 81.233%\nRecall (sensitivity) score for testing data = 81.233%\nSpecificity score for testing data = 84.255%\nf1 score for testing data = 81.233%\nRepeat this using the classification_report function, returning the output as a dataframe.\npd.DataFrame(classification_report(\n    y_true = y_test,\n    y_pred = y_pred_test,\n    target_names=[\"Not Given Clotbuster\", \"Given Clotbuster\"],\n    output_dict=True\n))\n\n\n\n\n\n\n\n\n\nNot Given Clotbuster\nGiven Clotbuster\naccuracy\nmacro avg\nweighted avg\n\n\n\n\nprecision\n0.842553\n0.760870\n0.812332\n0.801711\n0.811456\n\n\nrecall\n0.857143\n0.739437\n0.812332\n0.798290\n0.812332\n\n\nf1-score\n0.849785\n0.750000\n0.812332\n0.799893\n0.811797\n\n\nsupport\n231.000000\n142.000000\n0.812332\n373.000000\n373.000000\nPlot a confusion matrix for your model.\nconfusion_matrix_rf = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true = y_test,\n        y_pred=y_pred_test,\n    ),\n    display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n)\n\nconfusion_matrix_rf.plot()\n\nplt.show()\nPlot a normalized confusion matrix for your model.\nconfusion_matrix_rf_normalised = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true = y_test,\n        y_pred=y_pred_test,\n        normalize='true'\n    ),\n    display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n)\n\nconfusion_matrix_rf_normalised.plot()\n\nplt.show()",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_stroke_exercise_SOLUTION.html#core-tasks",
    "href": "4d_random_forest_stroke_exercise_SOLUTION.html#core-tasks",
    "title": "7  Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "precision\nspecificity\nrecall (sensitivity)\nf1",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_stroke_exercise_SOLUTION.html#part-2---refining-your-random-forest",
    "href": "4d_random_forest_stroke_exercise_SOLUTION.html#part-2---refining-your-random-forest",
    "title": "7  Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)",
    "section": "7.2 Part 2 - Refining Your Random Forest",
    "text": "7.2 Part 2 - Refining Your Random Forest\nLet’s experiment by changing a few parameters.\nAfter changing the parameters, look at the model metrics like accuracy, precision, and recall.\nTweak the parameters to see what model performance you can achieve.\n\ndef fit_model(model):\n    model = model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    precision_score_test = precision_score(y_test, y_pred_test, average='micro')\n    recall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\n    specificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n    # Note that the lecture slides show this displayed as a float using :3f\n    # We can instead use :.3% to format the number as a percentage to 3 decimal places.\n    print(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\n    print(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\n    print(f\"Precision score for testing data = {precision_score_test:.3%}\")\n    print(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\n    print(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\n\n7.2.1 Maximum Depth\n\nfit_model(model = RandomForestClassifier(max_depth=8, random_state=42))\n\nAccuracy of predicting training data = 89.389%\nAccuracy of predicting testing data = 83.646%\nPrecision score for testing data = 83.646%\nRecall (sensitivity) score for testing data = 83.646%\nSpecificity score for testing data = 88.991%\n\n\n\nfit_model(model = RandomForestClassifier(max_depth=4, random_state=42))\n\nAccuracy of predicting training data = 81.733%\nAccuracy of predicting testing data = 82.574%\nPrecision score for testing data = 82.574%\nRecall (sensitivity) score for testing data = 82.574%\nSpecificity score for testing data = 87.727%\n\n\n\nfit_model(model = RandomForestClassifier(max_depth=3, random_state=42))\n\nAccuracy of predicting training data = 80.188%\nAccuracy of predicting testing data = 80.429%\nPrecision score for testing data = 80.429%\nRecall (sensitivity) score for testing data = 80.429%\nSpecificity score for testing data = 85.268%\n\n\n\nfit_model(model = RandomForestClassifier(max_depth=14, random_state=42))\n\nAccuracy of predicting training data = 98.791%\nAccuracy of predicting testing data = 81.769%\nPrecision score for testing data = 81.769%\nRecall (sensitivity) score for testing data = 81.769%\nSpecificity score for testing data = 84.681%\n\n\n\n\n7.2.2 Number of Trees\n\nfit_model(model = RandomForestClassifier(n_estimators=50, random_state=42))\n\nAccuracy of predicting training data = 99.933%\nAccuracy of predicting testing data = 81.501%\nPrecision score for testing data = 81.501%\nRecall (sensitivity) score for testing data = 81.501%\nSpecificity score for testing data = 84.034%\n\n\n\nfit_model(model = RandomForestClassifier(n_estimators=500, random_state=42))\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 82.306%\nPrecision score for testing data = 82.306%\nRecall (sensitivity) score for testing data = 82.306%\nSpecificity score for testing data = 85.106%\n\n\n\nfit_model(model = RandomForestClassifier(n_estimators=500, max_depth=8, random_state=42))\n\nAccuracy of predicting training data = 89.657%\nAccuracy of predicting testing data = 82.038%\nPrecision score for testing data = 82.038%\nRecall (sensitivity) score for testing data = 82.038%\nSpecificity score for testing data = 87.273%",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_stroke_exercise_SOLUTION.html#part-3---comparing-performance-with-a-decision-tree-model",
    "href": "4d_random_forest_stroke_exercise_SOLUTION.html#part-3---comparing-performance-with-a-decision-tree-model",
    "title": "7  Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)",
    "section": "7.3 Part 3 - Comparing Performance with a Decision Tree Model",
    "text": "7.3 Part 3 - Comparing Performance with a Decision Tree Model\nCopy your code in from the previous exercise on decision trees.\nIf you tuned your decision tree, you can bring in the best-performing of your decision tree models.\n\nmodel_decision_tree = DecisionTreeClassifier(random_state=42)\nmodel_decision_tree = model_decision_tree.fit(X_train, y_train)\ny_pred_train_dt = model_decision_tree.predict(X_train)\ny_pred_test_dt = model_decision_tree.predict(X_test)\n\nLook at all of the metrics.\n\nprecision\nspecificity\nrecall (sensitivity)\nf1\n\n\naccuracy_train = np.mean(y_pred_train_dt == y_train)\naccuracy_test = np.mean(y_pred_test_dt == y_test)\n\nprecision_score_test = precision_score(y_test, y_pred_test_dt, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test_dt, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test_dt, pos_label=0)\nf1_score_test = f1_score(y_test, y_pred_test_dt, average='micro')\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\nprint(f\"f1 score for testing data = {f1_score_test:.3%}\")\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 78.016%\nPrecision score for testing data = 78.016%\nRecall (sensitivity) score for testing data = 78.016%\nSpecificity score for testing data = 82.819%\nf1 score for testing data = 78.016%\n\n\nPlot a confusion matrix for the decision tree model.\n\nconfusion_matrix_dt = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true = y_test,\n        y_pred=y_pred_test_dt,\n    ),\n    display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n)\n\nconfusion_matrix_dt.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\nPlot a normalised confusion matrix for the decision tree model.\n\nconfusion_matrix_dt_normalised = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true = y_test,\n        y_pred=y_pred_test_dt,\n        normalize=\"true\"\n    ),\n    display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n)\n\nconfusion_matrix_dt_normalised.plot()\n\nplt.show()",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_stroke_exercise_SOLUTION.html#extension",
    "href": "4d_random_forest_stroke_exercise_SOLUTION.html#extension",
    "title": "7  Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)",
    "section": "7.4 Extension",
    "text": "7.4 Extension\n\n7.4.1 ROC and AUC\nCreate receiver operating curves (ROC), labelled with the area under the curve (AUC).\n\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nroc_auc = auc(fpr, tpr)\nroc_auc\n\n0.9049448204377781\n\n\n\nfig = plt.figure(figsize=(6,6))\n\n# Plot ROC\nax = fig.add_subplot()\nax.plot(fpr, tpr, color='orange')\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('Receiver Operator Characteristic Curve')\ntext = f'AUC: {roc_auc:.3f}'\nax.text(0.64,0.07, text,\n         bbox=dict(facecolor='white', edgecolor='black'))\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.4.2 Comparing Performance with a Logistic Regression Model\nCopy your code in from last week’s logistic regression exercise.\nRemember - you will need to standardise the data for the logistic regression model!\nLook at all of the metrics.\n\nprecision\nspecificity\nrecall (sensitivity)\nf1\n\n\nscaler = StandardScaler()\n\nX_train_stand = scaler.fit_transform(X_train)\nX_test_stand = scaler.fit_transform(X_test)\n\nmodel_lr = LogisticRegression()\nmodel_lr = model_lr.fit(X_train_stand, y_train)\n\ny_pred_train_lr = model_lr.predict(X_train_stand)\ny_pred_test_lr = model_lr.predict(X_test_stand)\n\naccuracy_train = np.mean(y_pred_train_lr == y_train)\naccuracy_test = np.mean(y_pred_test_lr == y_test)\nprecision_score_test = precision_score(y_test, y_pred_test_lr, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test_lr, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test_lr, pos_label=0)\nf1_score_test = f1_score(y_test, y_pred_test, average='micro')\n\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\nprint(f\"f1 score for testing data = {f1_score_test:.3%}\")\n\nAccuracy of predicting training data = 81.934%\nAccuracy of predicting testing data = 82.038%\nPrecision score for testing data = 82.038%\nRecall (sensitivity) score for testing data = 82.038%\nSpecificity score for testing data = 88.318%\nf1 score for testing data = 81.233%\n\n\nPlot a confusion matrix for the logistic regression model.\n\nconfusion_matrix_lr = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true = y_test,\n        y_pred=y_pred_test_lr,\n    ),\n    display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n)\n\nconfusion_matrix_lr.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\nPlot a normalised confusion matrix for the logistic regression model.\n\nconfusion_matrix_lr_normalised = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true = y_test,\n        y_pred=y_pred_test_lr,\n        normalize='true'\n    ),\n    display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n)\n\nconfusion_matrix_lr_normalised.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.4.3 Comparing all of the models\nIn the previous exercise, we compared the performance of the logistic regression model and the decision tree model.\nNow consider the random forest too.\nCompare and contrast the confusion matrices for each fo these.\nIf one of these models were to be selected, which model would you recommend to put into use, and why?\nRemember: giving thrombolysis to good candidates for it can lead to less disability after stroke and improved outcomes. However, there is a risk that giving thrombolysis to the wrong person could lead to additional bleeding on the brain and worse outcomes. What might you want to balance?\nYou can write your answer into the empty cell below.",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4d_random_forest_stroke_exercise_SOLUTION.html#challenge",
    "href": "4d_random_forest_stroke_exercise_SOLUTION.html#challenge",
    "title": "7  Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)",
    "section": "7.5 Challenge",
    "text": "7.5 Challenge\n\n7.5.1 Challenge Exercise 1\nTry plotting all of your confusion matrices onto a single matplotlib figure.\nMake sure you give each of these a title.\nHint: You’ll need to create multiple matplotlib subplots.\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 5))\nconfusion_matrix_rf.plot(ax=ax1)\nax1.title.set_text('Random Forest')\n\nconfusion_matrix_dt.plot(ax=ax2)\nax2.title.set_text('Decision Tree')\n\nconfusion_matrix_lr.plot(ax=ax3)\nax3.title.set_text('Logistic Regression')\n\n\n\n\n\n\n\n\nNow do the same for the normalised confusion matrices.\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 5))\nconfusion_matrix_rf_normalised.plot(ax=ax1)\nax1.title.set_text('Random Forest')\n\nconfusion_matrix_dt_normalised.plot(ax=ax2)\nax2.title.set_text('Decision Tree')\n\nconfusion_matrix_lr_normalised.plot(ax=ax3)\nax3.title.set_text('Logistic Regression')\n\n\n\n\n\n\n\n\nNow do the same for your ROC curves.\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 5))\n\ndef plot_roc(model, ax, title, X_test):\n\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n    roc_auc = auc(fpr, tpr)\n\n    ax.plot(fpr, tpr, color='orange')\n    ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver Operator Characteristic Curve')\n    text = f'AUC: {roc_auc:.3f}'\n    ax.text(0.64,0.07, text,\n            bbox=dict(facecolor='white', edgecolor='black'))\n    ax.grid(True)\n\nplot_roc(model, ax1, \"Random Forest\", X_test)\nplot_roc(model_decision_tree, ax2, \"Decision Tree\", X_test)\nplot_roc(model_lr, ax3, \"Logistic Regression\", X_test_stand)\n\n\n\n\n\n\n\n\n\n\n7.5.2 Challenge Exercise 2\nUsing a random forest gives us another way to look at feature importance in our datasets.\nTake a look at this example from the scikit learn documentation. https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\nCalculate the feature importance using both methods (mean decrease in impurity, and feature permutation) for the dataset we have been working with in this exercise.\nDo they produce the same ranking of feature importance?\n\nmodel_rf = RandomForestClassifier(random_state=42)\nmodel_rf.fit(X_train, y_train)\n\nfeature_names = X.columns.tolist()\n\nimportances = model_rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\n\nforest_importances = pd.Series(importances, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nforest_importances.plot.barh(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n# Permutation feature importance\nfrom sklearn.inspection import permutation_importance\nmodel_rf = RandomForestClassifier(random_state=42)\nmodel_rf.fit(X_train, y_train)\n\nfeature_names = X.columns.tolist()\n\nresult = permutation_importance(\n    model_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\nforest_importances = pd.Series(result.importances_mean, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nforest_importances.plot.barh(yerr=result.importances_std, ax=ax)\nax.set_title(\"Feature importances using permutation on full model\")\nax.set_ylabel(\"Mean accuracy decrease\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.5.3 Challenge Exercise 3\nCan you improve accuracy of your random forest by changing the size of your train / test split?\nNOTE: This just shows trying out some different train/test splits!\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nfit_model(model = RandomForestClassifier(random_state=42))\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 81.932%\nPrecision score for testing data = 81.932%\nRecall (sensitivity) score for testing data = 81.932%\nSpecificity score for testing data = 84.503%\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n\nfit_model(model = RandomForestClassifier(random_state=42))\n\nAccuracy of predicting training data = 100.000%\nAccuracy of predicting testing data = 82.353%\nPrecision score for testing data = 82.353%\nRecall (sensitivity) score for testing data = 82.353%\nSpecificity score for testing data = 85.000%\n\n\n\n\n7.5.4 Challenge Exercise 4\nTry dropping some features from your data.\nCan you improve the performance of your random forest this way?\nNOTE: This just shows how to choose a subset of features - not the best subset! We cover that in a later session.\n\n# This isn't necessarily the best subset of features to use - it's just an example!\nX_reduced = X[['S2NihssArrival', 'Stroke Type_PIH', 'Age', 'S2RankinBeforeStroke']]\n\nX_reduced_train, X_reduced_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.1, random_state=42)\n\nmodel = RandomForestClassifier(max_depth=5, random_state=42)\nmodel = model.fit(X_reduced_train, y_train)\ny_pred_train = model.predict(X_reduced_train)\ny_pred_test = model.predict(X_reduced_test)\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\nprecision_score_test = precision_score(y_test, y_pred_test, average='micro')\nrecall_sensitivity_score_test = recall_score(y_test, y_pred_test, average='micro')\nspecificity_score_test = precision_score(y_test, y_pred_test, pos_label=0)\n\n# Note that the lecture slides show this displayed as a float using :3f\n# We can instead use :.3% to format the number as a percentage to 3 decimal places.\nprint(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\nprint(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")\nprint(f\"Precision score for testing data = {precision_score_test:.3%}\")\nprint(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\nprint(f\"Specificity score for testing data = {specificity_score_test:.3%}\")\n\nAccuracy of predicting training data = 80.537%\nAccuracy of predicting testing data = 79.144%\nPrecision score for testing data = 79.144%\nRecall (sensitivity) score for testing data = 79.144%\nSpecificity score for testing data = 88.235%",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercise Solution: Random Forests (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html",
    "href": "4e_boosting_titanic.html",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "",
    "text": "8.1 Divide into X (features) and y (labels)\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#divide-into-training-and-tets-sets",
    "href": "4e_boosting_titanic.html#divide-into-training-and-tets-sets",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "8.2 Divide into training and tets sets",
    "text": "8.2 Divide into training and tets sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#fit-decision-tree-model",
    "href": "4e_boosting_titanic.html#fit-decision-tree-model",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "8.3 Fit decision tree model",
    "text": "8.3 Fit decision tree model\n\nmodel = XGBClassifier(random_state=42)\nmodel = model.fit(X_train,y_train)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#predict-values",
    "href": "4e_boosting_titanic.html#predict-values",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "8.4 Predict values",
    "text": "8.4 Predict values\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#calculate-accuracy",
    "href": "4e_boosting_titanic.html#calculate-accuracy",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "8.5 Calculate accuracy",
    "text": "8.5 Calculate accuracy\n\n# The shorthand below says to check each predicted y value against the actual\n# y value in the training data.  This gives a list of True and False values\n# for each prediction, where True indicates the predicted value matches the\n# actual value.  Then we take the mean of these Boolean values, which gives\n# us a proportion (where if all values were True, the proportion would be 1.0)\n# If you want to see why that works, just uncomment the following line of code\n# to see what y_pred_train == y_train is doing.\n# print (y_pred_train == y_train)\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train}')\nprint (f'Accuracy of predicting test data = {accuracy_test}')\n\nAccuracy of predicting training data = 0.9730538922155688\nAccuracy of predicting test data = 0.8071748878923767\n\n\n\n# Show first ten predicted classes\nclasses = model.predict(X_test)\nclasses[0:10]\n\narray([0, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n\n\n\n# Show first ten predicted probabilities\nprobabilities = model.predict_proba(X_test)\nprobabilities[0:10]\n\narray([[0.86352104, 0.13647896],\n       [0.7883349 , 0.2116651 ],\n       [0.5199659 , 0.48003414],\n       [0.00145131, 0.9985487 ],\n       [0.07196659, 0.9280334 ],\n       [0.01444411, 0.9855559 ],\n       [0.22169638, 0.7783036 ],\n       [0.99416023, 0.00583977],\n       [0.67432725, 0.32567278],\n       [0.00534749, 0.9946525 ]], dtype=float32)",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#calculate-other-metrics",
    "href": "4e_boosting_titanic.html#calculate-other-metrics",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "8.6 Calculate other metrics",
    "text": "8.6 Calculate other metrics\n\nf1_score_xg = f1_score(y_test, y_pred_test, average='macro')\nprecision_score_xg = precision_score(y_test, y_pred_test, average='macro')\nrecall_score_xg = recall_score(y_test, y_pred_test, average='macro')\n\nprint (f'Accuracy of predicting test data = {accuracy_test}')\nprint (f'f1 score = {f1_score_xg}')\nprint (f'precision score = {recall_score_xg}')\nprint (f'recall score = {recall_score_xg}')\n\nAccuracy of predicting test data = 0.8071748878923767\nf1 score = 0.7978070637849236\nprecision score = 0.7961596511822908\nrecall score = 0.7961596511822908\n\n\n\nprint(classification_report(y_test, y_pred_test))\n\n              precision    recall  f1-score   support\n\n         0.0       0.83      0.85      0.84       134\n         1.0       0.77      0.74      0.75        89\n\n    accuracy                           0.81       223\n   macro avg       0.80      0.80      0.80       223\nweighted avg       0.81      0.81      0.81       223",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#hyperparameters",
    "href": "4e_boosting_titanic.html#hyperparameters",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "10.1 Hyperparameters",
    "text": "10.1 Hyperparameters\n\n10.1.1 n estimators (trees per forest)\n\naccuracy_results = []\n\nfor i in range(10, 500, 10):\n    model = model = XGBClassifier(n_estimators=i, random_state=42, nthread=-1)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'n_estimators': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='n_estimators'),\n        x='n_estimators', y='value', color='variable')\n\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed\n\n\n                        \n                                            \n\n\n\npd.DataFrame(accuracy_results).set_index(\"n_estimators\").sort_values(by=[\"accuracy_test\"], ascending=False)\n\n\n\n\n\n\n\n\n\naccuracy_train\naccuracy_test\n\n\nn_estimators\n\n\n\n\n\n\n10\n0.901198\n0.847534\n\n\n20\n0.931138\n0.834081\n\n\n30\n0.941617\n0.829596\n\n\n40\n0.950599\n0.820628\n\n\n50\n0.956587\n0.820628\n\n\n70\n0.965569\n0.820628\n\n\n60\n0.965569\n0.816143\n\n\n80\n0.965569\n0.811659\n\n\n130\n0.976048\n0.811659\n\n\n230\n0.980539\n0.811659\n\n\n220\n0.980539\n0.807175\n\n\n280\n0.980539\n0.807175\n\n\n270\n0.980539\n0.807175\n\n\n260\n0.980539\n0.807175\n\n\n240\n0.980539\n0.807175\n\n\n250\n0.980539\n0.807175\n\n\n90\n0.970060\n0.807175\n\n\n100\n0.973054\n0.807175\n\n\n360\n0.980539\n0.802691\n\n\n350\n0.980539\n0.802691\n\n\n340\n0.980539\n0.802691\n\n\n320\n0.980539\n0.802691\n\n\n310\n0.980539\n0.802691\n\n\n300\n0.980539\n0.802691\n\n\n290\n0.980539\n0.802691\n\n\n390\n0.980539\n0.802691\n\n\n110\n0.974551\n0.802691\n\n\n120\n0.974551\n0.802691\n\n\n210\n0.979042\n0.802691\n\n\n200\n0.979042\n0.802691\n\n\n190\n0.979042\n0.802691\n\n\n400\n0.980539\n0.798206\n\n\n450\n0.980539\n0.798206\n\n\n470\n0.980539\n0.798206\n\n\n480\n0.980539\n0.798206\n\n\n440\n0.980539\n0.798206\n\n\n430\n0.980539\n0.798206\n\n\n420\n0.980539\n0.798206\n\n\n170\n0.979042\n0.798206\n\n\n330\n0.980539\n0.798206\n\n\n380\n0.980539\n0.798206\n\n\n180\n0.979042\n0.798206\n\n\n140\n0.977545\n0.798206\n\n\n150\n0.979042\n0.798206\n\n\n160\n0.979042\n0.798206\n\n\n490\n0.980539\n0.798206\n\n\n410\n0.980539\n0.793722\n\n\n370\n0.980539\n0.793722\n\n\n460\n0.980539\n0.793722\n\n\n\n\n\n\n\n\n\n\n10.1.2 n estimators (trees per forest) - with max depth of 5\n\naccuracy_results = []\n\nfor i in range(10, 200, 10):\n    model = XGBClassifier(n_estimators=i, random_state=42, max_depth=5, nthread=-1)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'n_estimators': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='n_estimators'),\n        x='n_estimators', y='value', color='variable')\n\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed\n\n\n                        \n                                            \n\n\n\npd.DataFrame(accuracy_results).set_index(\"n_estimators\").sort_values(by=[\"accuracy_test\"], ascending=False)\n\n\n\n\n\n\n\n\n\naccuracy_train\naccuracy_test\n\n\nn_estimators\n\n\n\n\n\n\n30\n0.926647\n0.843049\n\n\n20\n0.913174\n0.838565\n\n\n40\n0.937126\n0.829596\n\n\n50\n0.941617\n0.825112\n\n\n10\n0.899701\n0.820628\n\n\n80\n0.962575\n0.820628\n\n\n100\n0.964072\n0.820628\n\n\n60\n0.952096\n0.820628\n\n\n70\n0.956587\n0.816143\n\n\n90\n0.964072\n0.816143\n\n\n110\n0.968563\n0.811659\n\n\n120\n0.970060\n0.811659\n\n\n130\n0.973054\n0.807175\n\n\n140\n0.971557\n0.802691\n\n\n190\n0.977545\n0.802691\n\n\n150\n0.974551\n0.798206\n\n\n160\n0.976048\n0.798206\n\n\n170\n0.977545\n0.798206\n\n\n180\n0.977545\n0.798206\n\n\n\n\n\n\n\n\n\nnp.random.seed(42)\n\nbest_n_estimators = pd.DataFrame(accuracy_results).sort_values(by=[\"accuracy_test\"], ascending=False).head(1)['n_estimators'].values[0]\n\nmodel = RandomForestClassifier(n_estimators=best_n_estimators, random_state=42, max_depth=8)\nmodel.fit(X_train,y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\nroc_curve = RocCurveDisplay.from_estimator(\n    model, X_test, y_test\n)\n\nfig = roc_curve.figure_\nax = roc_curve.ax_\n\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n).plot()\n\n\n\n\n\n\n\n\n\nConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_test,\n        y_pred=y_pred_test,\n        normalize='true'\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n).plot()",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#learning-rate-eta",
    "href": "4e_boosting_titanic.html#learning-rate-eta",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "10.2 Learning Rate (ETA)",
    "text": "10.2 Learning Rate (ETA)\n\nimport numpy as np\n\n\naccuracy_results = []\n\nfor i in np.arange(0.005, 0.2, 0.01):\n    model = XGBClassifier(learning_rate=i, random_state=42, nthread=-1)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'learning_rate': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='learning_rate'),\n        x='learning_rate', y='value', color='variable')\n\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#min-child-weight",
    "href": "4e_boosting_titanic.html#min-child-weight",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "10.3 Min child weight",
    "text": "10.3 Min child weight\n\naccuracy_results = []\n\nfor i in range(2, 15):\n    model = XGBClassifier(min_child_weight=i, random_state=42, nthread=-1)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'min_child_weight': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='min_child_weight'),\n        x='min_child_weight', y='value', color='variable')\n\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#subsample",
    "href": "4e_boosting_titanic.html#subsample",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "10.4 Subsample",
    "text": "10.4 Subsample\nhttps://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n\naccuracy_results = []\n\nfor i in np.arange(0.05, 1, 0.05):\n    model = XGBClassifier(subsample=i, random_state=42, nthread=-1)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'subsample': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='subsample'),\n        x='subsample', y='value', color='variable')\n\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#colsample",
    "href": "4e_boosting_titanic.html#colsample",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "10.5 Colsample",
    "text": "10.5 Colsample\n\naccuracy_results = []\n\nfor i in np.arange(0.05, 1, 0.05):\n    model = XGBClassifier(colsample_bytree=i, random_state=42, nthread=-1)\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    accuracy_train = np.mean(y_pred_train == y_train)\n    accuracy_test = np.mean(y_pred_test == y_test)\n    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'colsample_bytree': i})\n\npx.line(pd.DataFrame(accuracy_results).melt(id_vars='colsample_bytree'),\n        x='colsample_bytree', y='value', color='variable')\n\nValueError: Mime type rendering requires nbformat&gt;=4.2.0 but it is not installed",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_boosting_titanic.html#num-boost-round",
    "href": "4e_boosting_titanic.html#num-boost-round",
    "title": "8  Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)",
    "section": "10.6 Num boost round",
    "text": "10.6 Num boost round",
    "crumbs": [
      "4D/4E - Tree-based models for classification",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boosted Trees for Classification: XGBoost, Catboost, AdaBoost, LightGBM, HistogramGradientBoosting (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html",
    "href": "4e_tree_based_models_regression.html",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "",
    "text": "9.1 The dataset\nhttps://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\nTen baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n# Load the diabetes dataset\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True, as_frame=True, scaled=False)\n\ndiabetes_X.head()\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n0\n59.0\n2.0\n32.1\n101.0\n157.0\n93.2\n38.0\n4.0\n4.8598\n87.0\n\n\n1\n48.0\n1.0\n21.6\n87.0\n183.0\n103.2\n70.0\n3.0\n3.8918\n69.0\n\n\n2\n72.0\n2.0\n30.5\n93.0\n156.0\n93.6\n41.0\n4.0\n4.6728\n85.0\n\n\n3\n24.0\n1.0\n25.3\n84.0\n198.0\n131.4\n40.0\n5.0\n4.8903\n89.0\n\n\n4\n50.0\n1.0\n23.0\n101.0\n192.0\n125.4\n52.0\n4.0\n4.2905\n80.0\ndiabetes_y.head()\n\n0    151.0\n1     75.0\n2    141.0\n3    206.0\n4    135.0\nName: target, dtype: float64\n# Split the data into training/testing sets\ndiabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = train_test_split(\n    diabetes_X, diabetes_y,\n    test_size = 0.25,\n    random_state=42\n    )\ndiabetes_X_train\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n16\n47.0\n1.0\n30.3\n109.0\n207.0\n100.2\n70.0\n3.00\n5.2149\n98.0\n\n\n408\n66.0\n1.0\n21.7\n126.0\n212.0\n127.8\n45.0\n4.71\n5.2781\n101.0\n\n\n432\n51.0\n1.0\n31.5\n93.0\n231.0\n144.0\n49.0\n4.70\n5.2523\n117.0\n\n\n316\n53.0\n2.0\n27.7\n95.0\n190.0\n101.8\n41.0\n5.00\n5.4638\n101.0\n\n\n3\n24.0\n1.0\n25.3\n84.0\n198.0\n131.4\n40.0\n5.00\n4.8903\n89.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n106\n22.0\n1.0\n19.3\n82.0\n156.0\n93.2\n52.0\n3.00\n3.9890\n71.0\n\n\n270\n50.0\n2.0\n29.2\n119.0\n162.0\n85.2\n54.0\n3.00\n4.7362\n95.0\n\n\n348\n57.0\n1.0\n24.5\n93.0\n186.0\n96.6\n71.0\n3.00\n4.5218\n91.0\n\n\n435\n45.0\n1.0\n24.2\n83.0\n177.0\n118.4\n45.0\n4.00\n4.2195\n82.0\n\n\n102\n23.0\n1.0\n29.0\n90.0\n216.0\n131.4\n65.0\n3.00\n4.5850\n91.0\n\n\n\n\n331 rows × 10 columns\ndiabetes_y_train\n\n16     166.0\n408    189.0\n432    173.0\n316    220.0\n3      206.0\n       ...  \n106    134.0\n270    202.0\n348    148.0\n435     64.0\n102    302.0\nName: target, Length: 331, dtype: float64\n# Explore distribution of values being predicted\ndiabetes_y_train.hist()\npd.DataFrame(diabetes_y_train).plot(kind=\"box\")",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#storing-results",
    "href": "4e_tree_based_models_regression.html#storing-results",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.2 Storing results",
    "text": "9.2 Storing results\nCreate a list to store results in.\n\nresults_list = []",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#linear-regression",
    "href": "4e_tree_based_models_regression.html#linear-regression",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.3 Linear Regression",
    "text": "9.3 Linear Regression\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.fit_transform(X_train)\n    test_std=sc.fit_transform(X_test)\n\n    return train_std, test_std\n\ndiabetes_X_train_standardised, diabetes_X_test_standardised = standardise_data(\n    diabetes_X_train,\n    diabetes_X_test\n    )\n\n\n# Use only first feature\nsingle_feat_train = diabetes_X_train_standardised[:, np.newaxis, 2]\nsingle_feat_test = diabetes_X_test_standardised[:, np.newaxis, 2]\n\n# Create linear regression object\nregr = LinearRegression()\n\n# Train the model using the training sets\nregr.fit(single_feat_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(single_feat_test)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'Linear Regression',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\n# Plot outputs\ndef plot_actual_vs_predicted(actual, predicted):\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.scatter(actual, predicted, color=\"black\")\n    ax.axline((1, 1), slope=1)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('True vs Predicted Values')\n    plt.show()\n\ndef plot_residuals(actual, predicted):\n    residuals = actual - predicted\n\n    plt.figure(figsize=(10, 5))\n    plt.hist(residuals, bins=20)\n    plt.axvline(x = 0, color = 'r')\n    plt.xlabel('Residual')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Residuals')\n    plt.show()\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nMean absolute error: 51.02\nMean absolute percentage error: 48.88%\nRoot Mean squared error: 62.00\nCoefficient of determination: 0.30\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#multiple-linear-regression",
    "href": "4e_tree_based_models_regression.html#multiple-linear-regression",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.4 Multiple Linear Regression",
    "text": "9.4 Multiple Linear Regression\n\n# Create linear regression object\nregr = LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train_standardised, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test_standardised)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'Multiple Linear Regression',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nMean absolute error: 42.42\nMean absolute percentage error: 40.74%\nRoot Mean squared error: 54.25\nCoefficient of determination: 0.47\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#decision-tree",
    "href": "4e_tree_based_models_regression.html#decision-tree",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.5 Decision Tree",
    "text": "9.5 Decision Tree\n\nregr_dt = DecisionTreeRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_dt.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr_dt.predict(diabetes_X_test)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'Decision Tree',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nMean absolute error: 57.55\nMean absolute percentage error: 50.68%\nRoot Mean squared error: 76.00\nCoefficient of determination: -0.04\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#random-forest",
    "href": "4e_tree_based_models_regression.html#random-forest",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.6 Random Forest",
    "text": "9.6 Random Forest\n\nregr_rf = RandomForestRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_rf.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr_rf.predict(diabetes_X_test)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'Random Forest',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nMean absolute error: 43.86\nMean absolute percentage error: 40.34%\nRoot Mean squared error: 54.96\nCoefficient of determination: 0.45\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#xgboost",
    "href": "4e_tree_based_models_regression.html#xgboost",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.7 XGBoost",
    "text": "9.7 XGBoost\n\nregr_xg = XGBRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_xg.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr_xg.predict(diabetes_X_test)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'XGBoost',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nMean absolute error: 46.59\nMean absolute percentage error: 39.84%\nRoot Mean squared error: 58.01\nCoefficient of determination: 0.39\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#light-gbm",
    "href": "4e_tree_based_models_regression.html#light-gbm",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.8 Light GBM",
    "text": "9.8 Light GBM\n\nregr_lgbm = LGBMRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_lgbm.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr_lgbm.predict(diabetes_X_test)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'Light GBM',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nMean absolute error: 45.91\nMean absolute percentage error: 40.71%\nRoot Mean squared error: 57.97\nCoefficient of determination: 0.39\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#catboost",
    "href": "4e_tree_based_models_regression.html#catboost",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "9.9 Catboost",
    "text": "9.9 Catboost\n\nregr_catboost = CatBoostRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_catboost.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr_catboost.predict(diabetes_X_test)\n\nprint(f\"Mean absolute error: {mean_absolute_error(diabetes_y_test, diabetes_y_pred):.2f}\")\n\n\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred):.2%}\" )\n\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n\nresults_list.append(\n    {'model': 'CatBoost',\n     'RMSE': root_mean_squared_error(diabetes_y_test, diabetes_y_pred),\n     'MAPE': mean_absolute_percentage_error(diabetes_y_test, diabetes_y_pred)}\n)\n\nplot_actual_vs_predicted(diabetes_y_test, diabetes_y_pred)\n\nLearning rate set to 0.034381\n0:  learn: 76.7698931   total: 1.5ms    remaining: 1.5s\n1:  learn: 75.7552747   total: 2.5ms    remaining: 1.25s\n2:  learn: 74.8136416   total: 3.54ms   remaining: 1.18s\n3:  learn: 73.9549588   total: 4.85ms   remaining: 1.21s\n4:  learn: 73.0782030   total: 6.35ms   remaining: 1.26s\n5:  learn: 72.1682178   total: 7.69ms   remaining: 1.27s\n6:  learn: 71.3682200   total: 8.78ms   remaining: 1.25s\n7:  learn: 70.5608211   total: 9.75ms   remaining: 1.21s\n8:  learn: 69.7224552   total: 10.9ms   remaining: 1.2s\n9:  learn: 68.9543606   total: 12.2ms   remaining: 1.21s\n10: learn: 68.2429798   total: 13.5ms   remaining: 1.21s\n11: learn: 67.5175992   total: 14.8ms   remaining: 1.22s\n12: learn: 66.8114535   total: 16.1ms   remaining: 1.22s\n13: learn: 66.1442549   total: 17.3ms   remaining: 1.22s\n14: learn: 65.5281514   total: 18.5ms   remaining: 1.22s\n15: learn: 64.8807317   total: 19.7ms   remaining: 1.21s\n16: learn: 64.3379117   total: 21.1ms   remaining: 1.22s\n17: learn: 63.7508639   total: 22.4ms   remaining: 1.22s\n18: learn: 63.2097439   total: 23.6ms   remaining: 1.22s\n19: learn: 62.7139608   total: 24.7ms   remaining: 1.21s\n20: learn: 62.2236234   total: 26ms remaining: 1.21s\n21: learn: 61.7820840   total: 27.1ms   remaining: 1.2s\n22: learn: 61.2214417   total: 28ms remaining: 1.19s\n23: learn: 60.7727226   total: 28.9ms   remaining: 1.18s\n24: learn: 60.3932562   total: 29.9ms   remaining: 1.16s\n25: learn: 59.9563855   total: 30.8ms   remaining: 1.15s\n26: learn: 59.5363233   total: 31.9ms   remaining: 1.15s\n27: learn: 59.1791749   total: 33.1ms   remaining: 1.15s\n28: learn: 58.6907023   total: 34.1ms   remaining: 1.14s\n29: learn: 58.2293662   total: 35ms remaining: 1.13s\n30: learn: 57.8130967   total: 36ms remaining: 1.13s\n31: learn: 57.4281744   total: 37.3ms   remaining: 1.13s\n32: learn: 57.1725832   total: 38.5ms   remaining: 1.13s\n33: learn: 56.7632606   total: 39.6ms   remaining: 1.13s\n34: learn: 56.4138620   total: 40.7ms   remaining: 1.12s\n35: learn: 56.0466281   total: 42ms remaining: 1.12s\n36: learn: 55.7211225   total: 43.1ms   remaining: 1.12s\n37: learn: 55.4148873   total: 44.2ms   remaining: 1.12s\n38: learn: 55.1553516   total: 45.2ms   remaining: 1.11s\n39: learn: 54.8464593   total: 46.2ms   remaining: 1.11s\n40: learn: 54.5206398   total: 47.3ms   remaining: 1.11s\n41: learn: 54.2016506   total: 48.3ms   remaining: 1.1s\n42: learn: 53.9583345   total: 49.2ms   remaining: 1.09s\n43: learn: 53.6716694   total: 50.7ms   remaining: 1.1s\n44: learn: 53.4208321   total: 51.9ms   remaining: 1.1s\n45: learn: 53.1852474   total: 53ms remaining: 1.1s\n46: learn: 52.8905478   total: 54ms remaining: 1.09s\n47: learn: 52.7149197   total: 55ms remaining: 1.09s\n48: learn: 52.5124637   total: 56.1ms   remaining: 1.09s\n49: learn: 52.3121632   total: 57.1ms   remaining: 1.08s\n50: learn: 52.1629426   total: 58ms remaining: 1.08s\n51: learn: 51.9156136   total: 59ms remaining: 1.07s\n52: learn: 51.6313901   total: 59.8ms   remaining: 1.07s\n53: learn: 51.3785559   total: 60.8ms   remaining: 1.06s\n54: learn: 51.2121959   total: 61.6ms   remaining: 1.06s\n55: learn: 51.0247514   total: 62.4ms   remaining: 1.05s\n56: learn: 50.7958459   total: 63.4ms   remaining: 1.05s\n57: learn: 50.6322565   total: 64.2ms   remaining: 1.04s\n58: learn: 50.4693849   total: 65.1ms   remaining: 1.04s\n59: learn: 50.2729021   total: 66.1ms   remaining: 1.03s\n60: learn: 50.0652141   total: 67ms remaining: 1.03s\n61: learn: 49.9072643   total: 67.9ms   remaining: 1.03s\n62: learn: 49.7088928   total: 68.8ms   remaining: 1.02s\n63: learn: 49.4991804   total: 69.7ms   remaining: 1.02s\n64: learn: 49.3220877   total: 70.5ms   remaining: 1.01s\n65: learn: 49.1778938   total: 71.3ms   remaining: 1.01s\n66: learn: 49.0257537   total: 72.2ms   remaining: 1s\n67: learn: 48.8766285   total: 73ms remaining: 1s\n68: learn: 48.7340804   total: 73.8ms   remaining: 996ms\n69: learn: 48.6003848   total: 74.6ms   remaining: 991ms\n70: learn: 48.4638499   total: 75.4ms   remaining: 986ms\n71: learn: 48.3048779   total: 76.2ms   remaining: 982ms\n72: learn: 48.1774846   total: 77ms remaining: 978ms\n73: learn: 48.0137795   total: 77.8ms   remaining: 973ms\n74: learn: 47.8542608   total: 78.7ms   remaining: 971ms\n75: learn: 47.6948567   total: 79.7ms   remaining: 969ms\n76: learn: 47.5868712   total: 80.7ms   remaining: 967ms\n77: learn: 47.4238916   total: 81.8ms   remaining: 966ms\n78: learn: 47.2951391   total: 82.9ms   remaining: 966ms\n79: learn: 47.1670664   total: 83.8ms   remaining: 963ms\n80: learn: 46.9672861   total: 84.7ms   remaining: 961ms\n81: learn: 46.8544297   total: 85.6ms   remaining: 958ms\n82: learn: 46.7604885   total: 86.6ms   remaining: 956ms\n83: learn: 46.5980069   total: 87.4ms   remaining: 953ms\n84: learn: 46.4970350   total: 88.2ms   remaining: 950ms\n85: learn: 46.3157959   total: 89.1ms   remaining: 947ms\n86: learn: 46.1764093   total: 89.9ms   remaining: 943ms\n87: learn: 46.0119289   total: 90.8ms   remaining: 941ms\n88: learn: 45.9075921   total: 91.7ms   remaining: 938ms\n89: learn: 45.7041591   total: 92.7ms   remaining: 937ms\n90: learn: 45.6230779   total: 93.6ms   remaining: 935ms\n91: learn: 45.4441067   total: 94.5ms   remaining: 933ms\n92: learn: 45.3302847   total: 95.5ms   remaining: 931ms\n93: learn: 45.2266465   total: 96.5ms   remaining: 930ms\n94: learn: 45.1188503   total: 97.5ms   remaining: 929ms\n95: learn: 45.0070179   total: 98.5ms   remaining: 928ms\n96: learn: 44.9073280   total: 99.5ms   remaining: 926ms\n97: learn: 44.8299073   total: 100ms    remaining: 925ms\n98: learn: 44.7113215   total: 101ms    remaining: 923ms\n99: learn: 44.6274691   total: 102ms    remaining: 921ms\n100:    learn: 44.5478216   total: 103ms    remaining: 920ms\n101:    learn: 44.4544657   total: 104ms    remaining: 918ms\n102:    learn: 44.2905178   total: 105ms    remaining: 915ms\n103:    learn: 44.2331215   total: 106ms    remaining: 914ms\n104:    learn: 44.1630763   total: 107ms    remaining: 913ms\n105:    learn: 44.0799029   total: 108ms    remaining: 910ms\n106:    learn: 43.9771810   total: 109ms    remaining: 910ms\n107:    learn: 43.8666901   total: 110ms    remaining: 909ms\n108:    learn: 43.7729359   total: 111ms    remaining: 907ms\n109:    learn: 43.6575975   total: 112ms    remaining: 905ms\n110:    learn: 43.5374373   total: 113ms    remaining: 903ms\n111:    learn: 43.4217869   total: 114ms    remaining: 901ms\n112:    learn: 43.2726354   total: 115ms    remaining: 900ms\n113:    learn: 43.1591655   total: 116ms    remaining: 898ms\n114:    learn: 43.1050945   total: 117ms    remaining: 897ms\n115:    learn: 42.9959354   total: 117ms    remaining: 895ms\n116:    learn: 42.9189572   total: 118ms    remaining: 893ms\n117:    learn: 42.7879437   total: 119ms    remaining: 891ms\n118:    learn: 42.7335824   total: 120ms    remaining: 889ms\n119:    learn: 42.6647507   total: 121ms    remaining: 886ms\n120:    learn: 42.5460610   total: 122ms    remaining: 884ms\n121:    learn: 42.4575359   total: 123ms    remaining: 883ms\n122:    learn: 42.3488106   total: 124ms    remaining: 882ms\n123:    learn: 42.2746008   total: 125ms    remaining: 881ms\n124:    learn: 42.1997445   total: 126ms    remaining: 879ms\n125:    learn: 42.0779892   total: 126ms    remaining: 877ms\n126:    learn: 41.9835915   total: 128ms    remaining: 876ms\n127:    learn: 41.8873330   total: 129ms    remaining: 876ms\n128:    learn: 41.8151306   total: 130ms    remaining: 875ms\n129:    learn: 41.7505622   total: 131ms    remaining: 874ms\n130:    learn: 41.6331956   total: 132ms    remaining: 874ms\n131:    learn: 41.5887267   total: 133ms    remaining: 872ms\n132:    learn: 41.4726460   total: 134ms    remaining: 871ms\n133:    learn: 41.4357049   total: 135ms    remaining: 869ms\n134:    learn: 41.3683909   total: 135ms    remaining: 867ms\n135:    learn: 41.3529582   total: 136ms    remaining: 862ms\n136:    learn: 41.2664667   total: 137ms    remaining: 860ms\n137:    learn: 41.1269202   total: 137ms    remaining: 858ms\n138:    learn: 41.0353019   total: 138ms    remaining: 857ms\n139:    learn: 40.9380830   total: 139ms    remaining: 856ms\n140:    learn: 40.8300088   total: 140ms    remaining: 855ms\n141:    learn: 40.7284181   total: 141ms    remaining: 853ms\n142:    learn: 40.6691379   total: 142ms    remaining: 853ms\n143:    learn: 40.5847037   total: 144ms    remaining: 853ms\n144:    learn: 40.5070803   total: 145ms    remaining: 853ms\n145:    learn: 40.4541409   total: 146ms    remaining: 851ms\n146:    learn: 40.3628197   total: 146ms    remaining: 849ms\n147:    learn: 40.2947908   total: 147ms    remaining: 847ms\n148:    learn: 40.2383656   total: 148ms    remaining: 845ms\n149:    learn: 40.1418319   total: 149ms    remaining: 844ms\n150:    learn: 40.0635154   total: 150ms    remaining: 843ms\n151:    learn: 40.0195394   total: 151ms    remaining: 841ms\n152:    learn: 39.9984043   total: 152ms    remaining: 841ms\n153:    learn: 39.9361859   total: 153ms    remaining: 840ms\n154:    learn: 39.8484358   total: 154ms    remaining: 838ms\n155:    learn: 39.7381342   total: 155ms    remaining: 836ms\n156:    learn: 39.6576067   total: 155ms    remaining: 835ms\n157:    learn: 39.6234486   total: 156ms    remaining: 833ms\n158:    learn: 39.5537998   total: 157ms    remaining: 832ms\n159:    learn: 39.4925269   total: 158ms    remaining: 831ms\n160:    learn: 39.4231701   total: 159ms    remaining: 830ms\n161:    learn: 39.3822348   total: 160ms    remaining: 829ms\n162:    learn: 39.3088103   total: 161ms    remaining: 829ms\n163:    learn: 39.2205778   total: 162ms    remaining: 828ms\n164:    learn: 39.1371691   total: 163ms    remaining: 826ms\n165:    learn: 39.1192417   total: 164ms    remaining: 825ms\n166:    learn: 39.0395533   total: 165ms    remaining: 823ms\n167:    learn: 38.9513121   total: 166ms    remaining: 822ms\n168:    learn: 38.9051022   total: 167ms    remaining: 821ms\n169:    learn: 38.8698619   total: 168ms    remaining: 821ms\n170:    learn: 38.7690720   total: 169ms    remaining: 820ms\n171:    learn: 38.7107433   total: 170ms    remaining: 818ms\n172:    learn: 38.5816804   total: 171ms    remaining: 817ms\n173:    learn: 38.4737286   total: 172ms    remaining: 816ms\n174:    learn: 38.4367775   total: 173ms    remaining: 815ms\n175:    learn: 38.3782871   total: 174ms    remaining: 814ms\n176:    learn: 38.3075423   total: 175ms    remaining: 813ms\n177:    learn: 38.2641161   total: 176ms    remaining: 812ms\n178:    learn: 38.1671217   total: 177ms    remaining: 811ms\n179:    learn: 38.0724997   total: 178ms    remaining: 809ms\n180:    learn: 38.0517931   total: 179ms    remaining: 808ms\n181:    learn: 37.9349784   total: 179ms    remaining: 807ms\n182:    learn: 37.8350153   total: 180ms    remaining: 805ms\n183:    learn: 37.7654821   total: 181ms    remaining: 804ms\n184:    learn: 37.6858220   total: 182ms    remaining: 803ms\n185:    learn: 37.5583218   total: 183ms    remaining: 801ms\n186:    learn: 37.4254625   total: 184ms    remaining: 800ms\n187:    learn: 37.4015333   total: 185ms    remaining: 798ms\n188:    learn: 37.3565400   total: 186ms    remaining: 797ms\n189:    learn: 37.2798855   total: 187ms    remaining: 796ms\n190:    learn: 37.1737303   total: 188ms    remaining: 794ms\n191:    learn: 37.0917386   total: 189ms    remaining: 794ms\n192:    learn: 36.9876853   total: 190ms    remaining: 793ms\n193:    learn: 36.9347396   total: 191ms    remaining: 792ms\n194:    learn: 36.8560969   total: 192ms    remaining: 791ms\n195:    learn: 36.7820441   total: 193ms    remaining: 790ms\n196:    learn: 36.7382448   total: 193ms    remaining: 788ms\n197:    learn: 36.6888071   total: 194ms    remaining: 787ms\n198:    learn: 36.6077915   total: 196ms    remaining: 788ms\n199:    learn: 36.5194556   total: 197ms    remaining: 788ms\n200:    learn: 36.4391372   total: 198ms    remaining: 787ms\n201:    learn: 36.3613057   total: 199ms    remaining: 786ms\n202:    learn: 36.3233642   total: 200ms    remaining: 785ms\n203:    learn: 36.2923866   total: 201ms    remaining: 783ms\n204:    learn: 36.1969353   total: 202ms    remaining: 782ms\n205:    learn: 36.1045298   total: 202ms    remaining: 780ms\n206:    learn: 36.0651219   total: 203ms    remaining: 779ms\n207:    learn: 36.0293077   total: 204ms    remaining: 778ms\n208:    learn: 35.9293003   total: 205ms    remaining: 777ms\n209:    learn: 35.8540193   total: 206ms    remaining: 776ms\n210:    learn: 35.7344981   total: 207ms    remaining: 775ms\n211:    learn: 35.6745242   total: 208ms    remaining: 774ms\n212:    learn: 35.5702305   total: 209ms    remaining: 772ms\n213:    learn: 35.4581545   total: 210ms    remaining: 772ms\n214:    learn: 35.3961630   total: 211ms    remaining: 771ms\n215:    learn: 35.3470567   total: 212ms    remaining: 770ms\n216:    learn: 35.3164110   total: 213ms    remaining: 769ms\n217:    learn: 35.3022169   total: 214ms    remaining: 768ms\n218:    learn: 35.2201926   total: 215ms    remaining: 768ms\n219:    learn: 35.1870608   total: 216ms    remaining: 767ms\n220:    learn: 35.1522341   total: 217ms    remaining: 766ms\n221:    learn: 35.1144408   total: 218ms    remaining: 765ms\n222:    learn: 35.0458040   total: 219ms    remaining: 764ms\n223:    learn: 35.0073791   total: 220ms    remaining: 763ms\n224:    learn: 34.9968960   total: 221ms    remaining: 762ms\n225:    learn: 34.9134931   total: 222ms    remaining: 761ms\n226:    learn: 34.8522448   total: 223ms    remaining: 761ms\n227:    learn: 34.7182768   total: 224ms    remaining: 759ms\n228:    learn: 34.7080690   total: 225ms    remaining: 758ms\n229:    learn: 34.6158693   total: 226ms    remaining: 757ms\n230:    learn: 34.5994554   total: 227ms    remaining: 756ms\n231:    learn: 34.5231509   total: 228ms    remaining: 755ms\n232:    learn: 34.4521360   total: 229ms    remaining: 753ms\n233:    learn: 34.4377034   total: 230ms    remaining: 752ms\n234:    learn: 34.4019649   total: 231ms    remaining: 751ms\n235:    learn: 34.3384892   total: 231ms    remaining: 749ms\n236:    learn: 34.2648958   total: 232ms    remaining: 748ms\n237:    learn: 34.1924062   total: 233ms    remaining: 746ms\n238:    learn: 34.0597921   total: 234ms    remaining: 745ms\n239:    learn: 33.9936793   total: 235ms    remaining: 744ms\n240:    learn: 33.9363340   total: 236ms    remaining: 743ms\n241:    learn: 33.8403938   total: 237ms    remaining: 743ms\n242:    learn: 33.8253175   total: 238ms    remaining: 742ms\n243:    learn: 33.7073406   total: 239ms    remaining: 741ms\n244:    learn: 33.6338510   total: 240ms    remaining: 739ms\n245:    learn: 33.6226077   total: 241ms    remaining: 738ms\n246:    learn: 33.5647316   total: 242ms    remaining: 736ms\n247:    learn: 33.4532943   total: 242ms    remaining: 735ms\n248:    learn: 33.3957704   total: 243ms    remaining: 734ms\n249:    learn: 33.3218832   total: 244ms    remaining: 733ms\n250:    learn: 33.2823774   total: 245ms    remaining: 732ms\n251:    learn: 33.1949668   total: 246ms    remaining: 731ms\n252:    learn: 33.1268050   total: 247ms    remaining: 729ms\n253:    learn: 33.0295259   total: 248ms    remaining: 728ms\n254:    learn: 32.8956135   total: 249ms    remaining: 727ms\n255:    learn: 32.8307975   total: 250ms    remaining: 726ms\n256:    learn: 32.7931065   total: 251ms    remaining: 725ms\n257:    learn: 32.7541666   total: 252ms    remaining: 725ms\n258:    learn: 32.6455644   total: 253ms    remaining: 724ms\n259:    learn: 32.5994355   total: 254ms    remaining: 723ms\n260:    learn: 32.5175453   total: 255ms    remaining: 721ms\n261:    learn: 32.4368870   total: 256ms    remaining: 720ms\n262:    learn: 32.4094157   total: 256ms    remaining: 718ms\n263:    learn: 32.3119991   total: 257ms    remaining: 717ms\n264:    learn: 32.2503354   total: 258ms    remaining: 715ms\n265:    learn: 32.1916534   total: 259ms    remaining: 714ms\n266:    learn: 32.1189335   total: 260ms    remaining: 713ms\n267:    learn: 32.0524602   total: 261ms    remaining: 712ms\n268:    learn: 31.9945096   total: 261ms    remaining: 710ms\n269:    learn: 31.9432231   total: 262ms    remaining: 709ms\n270:    learn: 31.8558105   total: 263ms    remaining: 708ms\n271:    learn: 31.7245360   total: 264ms    remaining: 707ms\n272:    learn: 31.6810521   total: 265ms    remaining: 706ms\n273:    learn: 31.5905310   total: 267ms    remaining: 706ms\n274:    learn: 31.5814820   total: 268ms    remaining: 706ms\n275:    learn: 31.5287706   total: 269ms    remaining: 705ms\n276:    learn: 31.3911913   total: 269ms    remaining: 703ms\n277:    learn: 31.2843119   total: 270ms    remaining: 702ms\n278:    learn: 31.2729218   total: 271ms    remaining: 701ms\n279:    learn: 31.2064306   total: 272ms    remaining: 701ms\n280:    learn: 31.1236218   total: 273ms    remaining: 699ms\n281:    learn: 31.0698615   total: 274ms    remaining: 698ms\n282:    learn: 30.9990092   total: 275ms    remaining: 697ms\n283:    learn: 30.9401974   total: 276ms    remaining: 695ms\n284:    learn: 30.9325154   total: 277ms    remaining: 694ms\n285:    learn: 30.8544742   total: 278ms    remaining: 693ms\n286:    learn: 30.7972384   total: 279ms    remaining: 692ms\n287:    learn: 30.7880988   total: 279ms    remaining: 691ms\n288:    learn: 30.7320574   total: 280ms    remaining: 690ms\n289:    learn: 30.7239007   total: 281ms    remaining: 689ms\n290:    learn: 30.6605866   total: 282ms    remaining: 688ms\n291:    learn: 30.6308656   total: 283ms    remaining: 687ms\n292:    learn: 30.5739096   total: 284ms    remaining: 686ms\n293:    learn: 30.5626222   total: 285ms    remaining: 685ms\n294:    learn: 30.5568473   total: 286ms    remaining: 684ms\n295:    learn: 30.5015496   total: 287ms    remaining: 683ms\n296:    learn: 30.4270317   total: 288ms    remaining: 682ms\n297:    learn: 30.3754589   total: 289ms    remaining: 681ms\n298:    learn: 30.3378030   total: 290ms    remaining: 680ms\n299:    learn: 30.2313208   total: 291ms    remaining: 679ms\n300:    learn: 30.1393072   total: 292ms    remaining: 678ms\n301:    learn: 30.0527433   total: 293ms    remaining: 676ms\n302:    learn: 29.9579479   total: 294ms    remaining: 676ms\n303:    learn: 29.9058132   total: 295ms    remaining: 675ms\n304:    learn: 29.8985179   total: 296ms    remaining: 674ms\n305:    learn: 29.8111192   total: 297ms    remaining: 673ms\n306:    learn: 29.7506065   total: 298ms    remaining: 673ms\n307:    learn: 29.6532751   total: 299ms    remaining: 672ms\n308:    learn: 29.6013337   total: 300ms    remaining: 671ms\n309:    learn: 29.5949394   total: 301ms    remaining: 670ms\n310:    learn: 29.5370916   total: 302ms    remaining: 668ms\n311:    learn: 29.5077830   total: 303ms    remaining: 667ms\n312:    learn: 29.4248526   total: 304ms    remaining: 666ms\n313:    learn: 29.3805316   total: 305ms    remaining: 666ms\n314:    learn: 29.2893527   total: 306ms    remaining: 665ms\n315:    learn: 29.2432232   total: 307ms    remaining: 664ms\n316:    learn: 29.2373361   total: 308ms    remaining: 663ms\n317:    learn: 29.1224803   total: 309ms    remaining: 662ms\n318:    learn: 29.0765583   total: 309ms    remaining: 661ms\n319:    learn: 28.9848827   total: 310ms    remaining: 660ms\n320:    learn: 28.9083749   total: 311ms    remaining: 658ms\n321:    learn: 28.8076597   total: 312ms    remaining: 658ms\n322:    learn: 28.7360412   total: 313ms    remaining: 657ms\n323:    learn: 28.7158107   total: 314ms    remaining: 656ms\n324:    learn: 28.6685417   total: 315ms    remaining: 655ms\n325:    learn: 28.6477891   total: 316ms    remaining: 654ms\n326:    learn: 28.6027510   total: 317ms    remaining: 653ms\n327:    learn: 28.5511355   total: 318ms    remaining: 652ms\n328:    learn: 28.5440701   total: 319ms    remaining: 651ms\n329:    learn: 28.4708110   total: 320ms    remaining: 650ms\n330:    learn: 28.4645627   total: 321ms    remaining: 649ms\n331:    learn: 28.3704438   total: 322ms    remaining: 648ms\n332:    learn: 28.3634241   total: 323ms    remaining: 648ms\n333:    learn: 28.3186244   total: 324ms    remaining: 647ms\n334:    learn: 28.2613764   total: 325ms    remaining: 646ms\n335:    learn: 28.1538932   total: 326ms    remaining: 645ms\n336:    learn: 28.1223246   total: 327ms    remaining: 644ms\n337:    learn: 28.0715556   total: 328ms    remaining: 643ms\n338:    learn: 28.0045095   total: 329ms    remaining: 642ms\n339:    learn: 27.9469559   total: 330ms    remaining: 641ms\n340:    learn: 27.9075170   total: 331ms    remaining: 640ms\n341:    learn: 27.8393856   total: 332ms    remaining: 639ms\n342:    learn: 27.8084813   total: 333ms    remaining: 638ms\n343:    learn: 27.8006204   total: 334ms    remaining: 636ms\n344:    learn: 27.7940121   total: 335ms    remaining: 635ms\n345:    learn: 27.7897137   total: 335ms    remaining: 634ms\n346:    learn: 27.7276765   total: 336ms    remaining: 633ms\n347:    learn: 27.6434422   total: 337ms    remaining: 632ms\n348:    learn: 27.6096334   total: 338ms    remaining: 631ms\n349:    learn: 27.5187315   total: 339ms    remaining: 630ms\n350:    learn: 27.4845217   total: 340ms    remaining: 628ms\n351:    learn: 27.4200688   total: 341ms    remaining: 627ms\n352:    learn: 27.3478667   total: 342ms    remaining: 626ms\n353:    learn: 27.2974769   total: 343ms    remaining: 625ms\n354:    learn: 27.2468761   total: 344ms    remaining: 624ms\n355:    learn: 27.1848065   total: 344ms    remaining: 623ms\n356:    learn: 27.1263009   total: 345ms    remaining: 622ms\n357:    learn: 27.0822692   total: 346ms    remaining: 621ms\n358:    learn: 27.0691253   total: 347ms    remaining: 620ms\n359:    learn: 27.0269038   total: 348ms    remaining: 619ms\n360:    learn: 26.9952141   total: 349ms    remaining: 618ms\n361:    learn: 26.9890372   total: 350ms    remaining: 617ms\n362:    learn: 26.9465768   total: 351ms    remaining: 616ms\n363:    learn: 26.9044433   total: 352ms    remaining: 615ms\n364:    learn: 26.8682630   total: 353ms    remaining: 614ms\n365:    learn: 26.8618548   total: 354ms    remaining: 612ms\n366:    learn: 26.8144006   total: 354ms    remaining: 611ms\n367:    learn: 26.8093179   total: 355ms    remaining: 610ms\n368:    learn: 26.7618810   total: 356ms    remaining: 609ms\n369:    learn: 26.7571170   total: 357ms    remaining: 608ms\n370:    learn: 26.7043205   total: 358ms    remaining: 607ms\n371:    learn: 26.6378026   total: 359ms    remaining: 606ms\n372:    learn: 26.5514300   total: 360ms    remaining: 605ms\n373:    learn: 26.4460130   total: 361ms    remaining: 604ms\n374:    learn: 26.4398424   total: 362ms    remaining: 603ms\n375:    learn: 26.3852939   total: 363ms    remaining: 602ms\n376:    learn: 26.3344798   total: 364ms    remaining: 601ms\n377:    learn: 26.3272707   total: 365ms    remaining: 600ms\n378:    learn: 26.2989990   total: 365ms    remaining: 599ms\n379:    learn: 26.2476501   total: 366ms    remaining: 598ms\n380:    learn: 26.1918108   total: 367ms    remaining: 596ms\n381:    learn: 26.1154654   total: 368ms    remaining: 595ms\n382:    learn: 26.1087670   total: 369ms    remaining: 594ms\n383:    learn: 26.1040716   total: 370ms    remaining: 593ms\n384:    learn: 26.0318993   total: 371ms    remaining: 592ms\n385:    learn: 25.9751131   total: 372ms    remaining: 591ms\n386:    learn: 25.9379526   total: 373ms    remaining: 590ms\n387:    learn: 25.9287243   total: 374ms    remaining: 589ms\n388:    learn: 25.8751276   total: 375ms    remaining: 588ms\n389:    learn: 25.8300439   total: 376ms    remaining: 587ms\n390:    learn: 25.7598570   total: 376ms    remaining: 586ms\n391:    learn: 25.7332718   total: 377ms    remaining: 585ms\n392:    learn: 25.6894335   total: 378ms    remaining: 584ms\n393:    learn: 25.6840453   total: 379ms    remaining: 583ms\n394:    learn: 25.6672354   total: 380ms    remaining: 582ms\n395:    learn: 25.6613645   total: 381ms    remaining: 581ms\n396:    learn: 25.6040062   total: 382ms    remaining: 580ms\n397:    learn: 25.5713619   total: 383ms    remaining: 579ms\n398:    learn: 25.5146095   total: 384ms    remaining: 578ms\n399:    learn: 25.4836363   total: 384ms    remaining: 577ms\n400:    learn: 25.4311068   total: 385ms    remaining: 576ms\n401:    learn: 25.3692198   total: 386ms    remaining: 575ms\n402:    learn: 25.3480067   total: 387ms    remaining: 573ms\n403:    learn: 25.3052966   total: 388ms    remaining: 572ms\n404:    learn: 25.2519111   total: 389ms    remaining: 571ms\n405:    learn: 25.2460603   total: 390ms    remaining: 571ms\n406:    learn: 25.2144729   total: 391ms    remaining: 570ms\n407:    learn: 25.1347374   total: 392ms    remaining: 569ms\n408:    learn: 25.0284719   total: 394ms    remaining: 569ms\n409:    learn: 24.9752360   total: 395ms    remaining: 568ms\n410:    learn: 24.9260888   total: 396ms    remaining: 567ms\n411:    learn: 24.8766066   total: 396ms    remaining: 566ms\n412:    learn: 24.8719284   total: 397ms    remaining: 565ms\n413:    learn: 24.7976067   total: 398ms    remaining: 564ms\n414:    learn: 24.7491830   total: 399ms    remaining: 563ms\n415:    learn: 24.7333382   total: 400ms    remaining: 561ms\n416:    learn: 24.6709770   total: 401ms    remaining: 560ms\n417:    learn: 24.6271914   total: 402ms    remaining: 559ms\n418:    learn: 24.5975426   total: 403ms    remaining: 558ms\n419:    learn: 24.5646781   total: 404ms    remaining: 557ms\n420:    learn: 24.5254733   total: 405ms    remaining: 557ms\n421:    learn: 24.4870436   total: 406ms    remaining: 556ms\n422:    learn: 24.4832883   total: 407ms    remaining: 555ms\n423:    learn: 24.4214499   total: 408ms    remaining: 554ms\n424:    learn: 24.4178535   total: 409ms    remaining: 553ms\n425:    learn: 24.4120127   total: 409ms    remaining: 552ms\n426:    learn: 24.3547134   total: 410ms    remaining: 551ms\n427:    learn: 24.3406164   total: 411ms    remaining: 550ms\n428:    learn: 24.2735838   total: 412ms    remaining: 549ms\n429:    learn: 24.1751947   total: 413ms    remaining: 548ms\n430:    learn: 24.1377276   total: 414ms    remaining: 547ms\n431:    learn: 24.1304914   total: 415ms    remaining: 546ms\n432:    learn: 24.1223580   total: 416ms    remaining: 545ms\n433:    learn: 24.0860040   total: 417ms    remaining: 544ms\n434:    learn: 24.0466889   total: 418ms    remaining: 543ms\n435:    learn: 23.9919742   total: 419ms    remaining: 542ms\n436:    learn: 23.9457932   total: 420ms    remaining: 541ms\n437:    learn: 23.8815083   total: 421ms    remaining: 540ms\n438:    learn: 23.8781224   total: 422ms    remaining: 540ms\n439:    learn: 23.8259252   total: 423ms    remaining: 539ms\n440:    learn: 23.7926954   total: 424ms    remaining: 538ms\n441:    learn: 23.7407856   total: 425ms    remaining: 537ms\n442:    learn: 23.7320938   total: 426ms    remaining: 536ms\n443:    learn: 23.6915647   total: 427ms    remaining: 535ms\n444:    learn: 23.6792172   total: 428ms    remaining: 534ms\n445:    learn: 23.6210072   total: 429ms    remaining: 533ms\n446:    learn: 23.6161297   total: 430ms    remaining: 532ms\n447:    learn: 23.5496392   total: 431ms    remaining: 531ms\n448:    learn: 23.5328028   total: 432ms    remaining: 530ms\n449:    learn: 23.4639952   total: 433ms    remaining: 529ms\n450:    learn: 23.4092092   total: 433ms    remaining: 528ms\n451:    learn: 23.3452766   total: 434ms    remaining: 527ms\n452:    learn: 23.3140723   total: 436ms    remaining: 526ms\n453:    learn: 23.2249015   total: 437ms    remaining: 525ms\n454:    learn: 23.1744393   total: 438ms    remaining: 524ms\n455:    learn: 23.1699584   total: 439ms    remaining: 523ms\n456:    learn: 23.1302089   total: 440ms    remaining: 522ms\n457:    learn: 23.0294126   total: 440ms    remaining: 521ms\n458:    learn: 22.9840724   total: 441ms    remaining: 520ms\n459:    learn: 22.9358059   total: 442ms    remaining: 519ms\n460:    learn: 22.8968573   total: 443ms    remaining: 518ms\n461:    learn: 22.8930427   total: 444ms    remaining: 517ms\n462:    learn: 22.8331553   total: 445ms    remaining: 516ms\n463:    learn: 22.7810277   total: 446ms    remaining: 515ms\n464:    learn: 22.7759205   total: 447ms    remaining: 514ms\n465:    learn: 22.7720110   total: 448ms    remaining: 513ms\n466:    learn: 22.7024479   total: 449ms    remaining: 512ms\n467:    learn: 22.6995010   total: 450ms    remaining: 511ms\n468:    learn: 22.6619011   total: 451ms    remaining: 510ms\n469:    learn: 22.6081489   total: 452ms    remaining: 510ms\n470:    learn: 22.5927501   total: 453ms    remaining: 509ms\n471:    learn: 22.5346437   total: 454ms    remaining: 508ms\n472:    learn: 22.4779136   total: 455ms    remaining: 507ms\n473:    learn: 22.4276706   total: 456ms    remaining: 506ms\n474:    learn: 22.4199813   total: 457ms    remaining: 505ms\n475:    learn: 22.3876330   total: 457ms    remaining: 504ms\n476:    learn: 22.3528581   total: 458ms    remaining: 503ms\n477:    learn: 22.3490392   total: 459ms    remaining: 502ms\n478:    learn: 22.3031423   total: 460ms    remaining: 501ms\n479:    learn: 22.3007944   total: 461ms    remaining: 500ms\n480:    learn: 22.2542492   total: 462ms    remaining: 499ms\n481:    learn: 22.2209373   total: 463ms    remaining: 498ms\n482:    learn: 22.2123723   total: 464ms    remaining: 496ms\n483:    learn: 22.1482190   total: 465ms    remaining: 495ms\n484:    learn: 22.1339636   total: 466ms    remaining: 495ms\n485:    learn: 22.0631395   total: 467ms    remaining: 494ms\n486:    learn: 22.0588600   total: 468ms    remaining: 493ms\n487:    learn: 22.0347781   total: 469ms    remaining: 492ms\n488:    learn: 22.0277106   total: 470ms    remaining: 491ms\n489:    learn: 22.0139632   total: 470ms    remaining: 490ms\n490:    learn: 21.9731526   total: 471ms    remaining: 489ms\n491:    learn: 21.9436288   total: 472ms    remaining: 488ms\n492:    learn: 21.8956250   total: 473ms    remaining: 487ms\n493:    learn: 21.8193608   total: 474ms    remaining: 485ms\n494:    learn: 21.8167284   total: 475ms    remaining: 485ms\n495:    learn: 21.7766221   total: 476ms    remaining: 483ms\n496:    learn: 21.7498528   total: 477ms    remaining: 482ms\n497:    learn: 21.7303242   total: 477ms    remaining: 481ms\n498:    learn: 21.6849682   total: 478ms    remaining: 480ms\n499:    learn: 21.6427310   total: 479ms    remaining: 479ms\n500:    learn: 21.5528544   total: 480ms    remaining: 478ms\n501:    learn: 21.5054316   total: 481ms    remaining: 478ms\n502:    learn: 21.4760129   total: 482ms    remaining: 477ms\n503:    learn: 21.4490218   total: 483ms    remaining: 476ms\n504:    learn: 21.4265026   total: 484ms    remaining: 475ms\n505:    learn: 21.3902548   total: 485ms    remaining: 473ms\n506:    learn: 21.3540889   total: 486ms    remaining: 472ms\n507:    learn: 21.3198302   total: 487ms    remaining: 471ms\n508:    learn: 21.2740171   total: 488ms    remaining: 470ms\n509:    learn: 21.2345957   total: 488ms    remaining: 469ms\n510:    learn: 21.1872209   total: 489ms    remaining: 468ms\n511:    learn: 21.1695153   total: 490ms    remaining: 467ms\n512:    learn: 21.1522472   total: 491ms    remaining: 466ms\n513:    learn: 21.1497867   total: 492ms    remaining: 465ms\n514:    learn: 21.1015735   total: 493ms    remaining: 464ms\n515:    learn: 21.0385321   total: 494ms    remaining: 464ms\n516:    learn: 20.9991473   total: 495ms    remaining: 463ms\n517:    learn: 20.9703746   total: 496ms    remaining: 462ms\n518:    learn: 20.9222185   total: 497ms    remaining: 461ms\n519:    learn: 20.8999891   total: 498ms    remaining: 460ms\n520:    learn: 20.8554804   total: 499ms    remaining: 459ms\n521:    learn: 20.8128166   total: 500ms    remaining: 458ms\n522:    learn: 20.7911705   total: 501ms    remaining: 457ms\n523:    learn: 20.7460615   total: 501ms    remaining: 456ms\n524:    learn: 20.7055146   total: 502ms    remaining: 455ms\n525:    learn: 20.6579428   total: 504ms    remaining: 454ms\n526:    learn: 20.6358801   total: 504ms    remaining: 453ms\n527:    learn: 20.5876242   total: 505ms    remaining: 452ms\n528:    learn: 20.5663373   total: 506ms    remaining: 451ms\n529:    learn: 20.5187919   total: 507ms    remaining: 450ms\n530:    learn: 20.4578327   total: 508ms    remaining: 449ms\n531:    learn: 20.3828339   total: 509ms    remaining: 448ms\n532:    learn: 20.3633039   total: 510ms    remaining: 447ms\n533:    learn: 20.3358557   total: 511ms    remaining: 446ms\n534:    learn: 20.3215443   total: 512ms    remaining: 445ms\n535:    learn: 20.3123640   total: 513ms    remaining: 444ms\n536:    learn: 20.2915068   total: 514ms    remaining: 443ms\n537:    learn: 20.2365966   total: 515ms    remaining: 442ms\n538:    learn: 20.2115139   total: 516ms    remaining: 441ms\n539:    learn: 20.2054815   total: 517ms    remaining: 440ms\n540:    learn: 20.1590883   total: 518ms    remaining: 439ms\n541:    learn: 20.1547842   total: 519ms    remaining: 438ms\n542:    learn: 20.1279782   total: 520ms    remaining: 437ms\n543:    learn: 20.1237081   total: 521ms    remaining: 436ms\n544:    learn: 20.0569792   total: 522ms    remaining: 435ms\n545:    learn: 20.0329940   total: 522ms    remaining: 434ms\n546:    learn: 19.9896129   total: 523ms    remaining: 433ms\n547:    learn: 19.9398546   total: 524ms    remaining: 432ms\n548:    learn: 19.8595835   total: 525ms    remaining: 431ms\n549:    learn: 19.8409663   total: 526ms    remaining: 430ms\n550:    learn: 19.8053021   total: 527ms    remaining: 430ms\n551:    learn: 19.7454656   total: 529ms    remaining: 429ms\n552:    learn: 19.6980410   total: 530ms    remaining: 428ms\n553:    learn: 19.6626935   total: 531ms    remaining: 427ms\n554:    learn: 19.6266865   total: 531ms    remaining: 426ms\n555:    learn: 19.5898795   total: 533ms    remaining: 425ms\n556:    learn: 19.5751403   total: 533ms    remaining: 424ms\n557:    learn: 19.5731983   total: 534ms    remaining: 423ms\n558:    learn: 19.5371056   total: 535ms    remaining: 422ms\n559:    learn: 19.5048042   total: 536ms    remaining: 421ms\n560:    learn: 19.5012571   total: 537ms    remaining: 420ms\n561:    learn: 19.4832223   total: 538ms    remaining: 419ms\n562:    learn: 19.4600865   total: 539ms    remaining: 418ms\n563:    learn: 19.4384968   total: 540ms    remaining: 417ms\n564:    learn: 19.4361093   total: 541ms    remaining: 416ms\n565:    learn: 19.4110086   total: 542ms    remaining: 415ms\n566:    learn: 19.3699414   total: 543ms    remaining: 414ms\n567:    learn: 19.3500309   total: 544ms    remaining: 414ms\n568:    learn: 19.2982027   total: 545ms    remaining: 413ms\n569:    learn: 19.2285478   total: 546ms    remaining: 412ms\n570:    learn: 19.2107074   total: 547ms    remaining: 411ms\n571:    learn: 19.1894003   total: 548ms    remaining: 410ms\n572:    learn: 19.1858983   total: 549ms    remaining: 409ms\n573:    learn: 19.1385056   total: 549ms    remaining: 408ms\n574:    learn: 19.1160386   total: 550ms    remaining: 407ms\n575:    learn: 19.0988308   total: 551ms    remaining: 406ms\n576:    learn: 19.0783785   total: 552ms    remaining: 405ms\n577:    learn: 19.0231023   total: 553ms    remaining: 404ms\n578:    learn: 19.0036597   total: 554ms    remaining: 403ms\n579:    learn: 18.9799549   total: 555ms    remaining: 402ms\n580:    learn: 18.9711749   total: 556ms    remaining: 401ms\n581:    learn: 18.9443614   total: 557ms    remaining: 400ms\n582:    learn: 18.9191490   total: 558ms    remaining: 399ms\n583:    learn: 18.8751081   total: 559ms    remaining: 398ms\n584:    learn: 18.8293391   total: 560ms    remaining: 397ms\n585:    learn: 18.7965695   total: 561ms    remaining: 396ms\n586:    learn: 18.7819962   total: 561ms    remaining: 395ms\n587:    learn: 18.7304407   total: 562ms    remaining: 394ms\n588:    learn: 18.6926858   total: 563ms    remaining: 393ms\n589:    learn: 18.6510988   total: 564ms    remaining: 392ms\n590:    learn: 18.6365421   total: 565ms    remaining: 391ms\n591:    learn: 18.6016154   total: 566ms    remaining: 390ms\n592:    learn: 18.5842759   total: 567ms    remaining: 389ms\n593:    learn: 18.5401411   total: 568ms    remaining: 388ms\n594:    learn: 18.5373289   total: 569ms    remaining: 387ms\n595:    learn: 18.5086795   total: 570ms    remaining: 386ms\n596:    learn: 18.4493193   total: 571ms    remaining: 386ms\n597:    learn: 18.4063125   total: 572ms    remaining: 385ms\n598:    learn: 18.3520546   total: 573ms    remaining: 384ms\n599:    learn: 18.3180003   total: 574ms    remaining: 383ms\n600:    learn: 18.3041324   total: 575ms    remaining: 382ms\n601:    learn: 18.2722337   total: 576ms    remaining: 381ms\n602:    learn: 18.2287353   total: 577ms    remaining: 380ms\n603:    learn: 18.1832568   total: 578ms    remaining: 379ms\n604:    learn: 18.1805032   total: 579ms    remaining: 378ms\n605:    learn: 18.1520544   total: 580ms    remaining: 377ms\n606:    learn: 18.1354692   total: 581ms    remaining: 376ms\n607:    learn: 18.1264107   total: 581ms    remaining: 375ms\n608:    learn: 18.1026188   total: 583ms    remaining: 374ms\n609:    learn: 18.0549990   total: 584ms    remaining: 373ms\n610:    learn: 18.0037741   total: 585ms    remaining: 372ms\n611:    learn: 17.9604370   total: 586ms    remaining: 371ms\n612:    learn: 17.9285276   total: 587ms    remaining: 370ms\n613:    learn: 17.9213726   total: 588ms    remaining: 369ms\n614:    learn: 17.9066908   total: 589ms    remaining: 368ms\n615:    learn: 17.9030634   total: 590ms    remaining: 368ms\n616:    learn: 17.8900287   total: 591ms    remaining: 367ms\n617:    learn: 17.8829280   total: 592ms    remaining: 366ms\n618:    learn: 17.8388985   total: 593ms    remaining: 365ms\n619:    learn: 17.7928862   total: 593ms    remaining: 364ms\n620:    learn: 17.7466362   total: 594ms    remaining: 363ms\n621:    learn: 17.7104434   total: 595ms    remaining: 362ms\n622:    learn: 17.6661359   total: 596ms    remaining: 361ms\n623:    learn: 17.6501147   total: 597ms    remaining: 360ms\n624:    learn: 17.6396963   total: 598ms    remaining: 359ms\n625:    learn: 17.6102058   total: 599ms    remaining: 358ms\n626:    learn: 17.6078614   total: 600ms    remaining: 357ms\n627:    learn: 17.5630963   total: 601ms    remaining: 356ms\n628:    learn: 17.5522732   total: 602ms    remaining: 355ms\n629:    learn: 17.5429766   total: 603ms    remaining: 354ms\n630:    learn: 17.5297201   total: 604ms    remaining: 353ms\n631:    learn: 17.5253753   total: 605ms    remaining: 352ms\n632:    learn: 17.4715324   total: 606ms    remaining: 351ms\n633:    learn: 17.4328538   total: 607ms    remaining: 351ms\n634:    learn: 17.4197552   total: 608ms    remaining: 350ms\n635:    learn: 17.3686977   total: 609ms    remaining: 349ms\n636:    learn: 17.3455865   total: 610ms    remaining: 348ms\n637:    learn: 17.2964399   total: 611ms    remaining: 347ms\n638:    learn: 17.2723522   total: 612ms    remaining: 346ms\n639:    learn: 17.2310604   total: 613ms    remaining: 345ms\n640:    learn: 17.1793967   total: 614ms    remaining: 344ms\n641:    learn: 17.1565288   total: 615ms    remaining: 343ms\n642:    learn: 17.1317463   total: 616ms    remaining: 342ms\n643:    learn: 17.1118715   total: 617ms    remaining: 341ms\n644:    learn: 17.0735889   total: 618ms    remaining: 340ms\n645:    learn: 17.0529158   total: 618ms    remaining: 339ms\n646:    learn: 17.0329358   total: 619ms    remaining: 338ms\n647:    learn: 16.9940748   total: 620ms    remaining: 337ms\n648:    learn: 16.9521366   total: 621ms    remaining: 336ms\n649:    learn: 16.9500090   total: 622ms    remaining: 335ms\n650:    learn: 16.9398288   total: 623ms    remaining: 334ms\n651:    learn: 16.9269785   total: 624ms    remaining: 333ms\n652:    learn: 16.9251987   total: 625ms    remaining: 332ms\n653:    learn: 16.8865075   total: 626ms    remaining: 331ms\n654:    learn: 16.8336257   total: 627ms    remaining: 330ms\n655:    learn: 16.7745723   total: 628ms    remaining: 329ms\n656:    learn: 16.7453018   total: 629ms    remaining: 328ms\n657:    learn: 16.7132547   total: 630ms    remaining: 327ms\n658:    learn: 16.6688611   total: 631ms    remaining: 326ms\n659:    learn: 16.6336092   total: 631ms    remaining: 325ms\n660:    learn: 16.6224205   total: 632ms    remaining: 324ms\n661:    learn: 16.6047201   total: 633ms    remaining: 323ms\n662:    learn: 16.5744777   total: 634ms    remaining: 322ms\n663:    learn: 16.5205394   total: 635ms    remaining: 321ms\n664:    learn: 16.4841640   total: 636ms    remaining: 320ms\n665:    learn: 16.4718565   total: 637ms    remaining: 320ms\n666:    learn: 16.4506629   total: 638ms    remaining: 319ms\n667:    learn: 16.4434901   total: 639ms    remaining: 318ms\n668:    learn: 16.4390434   total: 640ms    remaining: 317ms\n669:    learn: 16.4033147   total: 641ms    remaining: 316ms\n670:    learn: 16.3914168   total: 642ms    remaining: 315ms\n671:    learn: 16.3883133   total: 643ms    remaining: 314ms\n672:    learn: 16.3619882   total: 644ms    remaining: 313ms\n673:    learn: 16.3187350   total: 645ms    remaining: 312ms\n674:    learn: 16.3071318   total: 646ms    remaining: 311ms\n675:    learn: 16.2799616   total: 646ms    remaining: 310ms\n676:    learn: 16.2543052   total: 647ms    remaining: 309ms\n677:    learn: 16.2247365   total: 648ms    remaining: 308ms\n678:    learn: 16.2224638   total: 649ms    remaining: 307ms\n679:    learn: 16.1731674   total: 651ms    remaining: 306ms\n680:    learn: 16.1714538   total: 652ms    remaining: 305ms\n681:    learn: 16.1613971   total: 652ms    remaining: 304ms\n682:    learn: 16.1147448   total: 653ms    remaining: 303ms\n683:    learn: 16.0696533   total: 654ms    remaining: 302ms\n684:    learn: 16.0237966   total: 655ms    remaining: 301ms\n685:    learn: 16.0150106   total: 656ms    remaining: 300ms\n686:    learn: 15.9803812   total: 657ms    remaining: 299ms\n687:    learn: 15.9782292   total: 658ms    remaining: 298ms\n688:    learn: 15.9350017   total: 659ms    remaining: 297ms\n689:    learn: 15.9165426   total: 660ms    remaining: 296ms\n690:    learn: 15.9147553   total: 660ms    remaining: 295ms\n691:    learn: 15.9021542   total: 661ms    remaining: 294ms\n692:    learn: 15.8686086   total: 662ms    remaining: 293ms\n693:    learn: 15.8668570   total: 663ms    remaining: 292ms\n694:    learn: 15.8137514   total: 664ms    remaining: 291ms\n695:    learn: 15.8034165   total: 665ms    remaining: 290ms\n696:    learn: 15.7543075   total: 666ms    remaining: 289ms\n697:    learn: 15.7227008   total: 667ms    remaining: 289ms\n698:    learn: 15.6928794   total: 668ms    remaining: 288ms\n699:    learn: 15.6511903   total: 669ms    remaining: 287ms\n700:    learn: 15.6321406   total: 670ms    remaining: 286ms\n701:    learn: 15.6043360   total: 671ms    remaining: 285ms\n702:    learn: 15.5952870   total: 672ms    remaining: 284ms\n703:    learn: 15.5905096   total: 673ms    remaining: 283ms\n704:    learn: 15.5836839   total: 673ms    remaining: 282ms\n705:    learn: 15.5652754   total: 674ms    remaining: 281ms\n706:    learn: 15.5282494   total: 675ms    remaining: 280ms\n707:    learn: 15.4869307   total: 676ms    remaining: 279ms\n708:    learn: 15.4644509   total: 677ms    remaining: 278ms\n709:    learn: 15.4380354   total: 678ms    remaining: 277ms\n710:    learn: 15.4270000   total: 679ms    remaining: 276ms\n711:    learn: 15.4189790   total: 680ms    remaining: 275ms\n712:    learn: 15.4178505   total: 680ms    remaining: 274ms\n713:    learn: 15.3923931   total: 681ms    remaining: 273ms\n714:    learn: 15.3478757   total: 682ms    remaining: 272ms\n715:    learn: 15.3082102   total: 683ms    remaining: 271ms\n716:    learn: 15.2968684   total: 685ms    remaining: 270ms\n717:    learn: 15.2819067   total: 686ms    remaining: 269ms\n718:    learn: 15.2708815   total: 687ms    remaining: 268ms\n719:    learn: 15.2426082   total: 688ms    remaining: 267ms\n720:    learn: 15.2010081   total: 689ms    remaining: 266ms\n721:    learn: 15.1757935   total: 690ms    remaining: 266ms\n722:    learn: 15.1496573   total: 691ms    remaining: 265ms\n723:    learn: 15.1069891   total: 691ms    remaining: 264ms\n724:    learn: 15.0957545   total: 692ms    remaining: 263ms\n725:    learn: 15.0678419   total: 693ms    remaining: 262ms\n726:    learn: 15.0323543   total: 694ms    remaining: 261ms\n727:    learn: 15.0099373   total: 695ms    remaining: 260ms\n728:    learn: 14.9804890   total: 696ms    remaining: 259ms\n729:    learn: 14.9526908   total: 698ms    remaining: 258ms\n730:    learn: 14.9302848   total: 699ms    remaining: 257ms\n731:    learn: 14.8966998   total: 700ms    remaining: 256ms\n732:    learn: 14.8699435   total: 701ms    remaining: 255ms\n733:    learn: 14.8304247   total: 702ms    remaining: 254ms\n734:    learn: 14.7929834   total: 703ms    remaining: 253ms\n735:    learn: 14.7696209   total: 704ms    remaining: 253ms\n736:    learn: 14.7562766   total: 705ms    remaining: 252ms\n737:    learn: 14.7213105   total: 706ms    remaining: 251ms\n738:    learn: 14.7000863   total: 707ms    remaining: 250ms\n739:    learn: 14.6493550   total: 708ms    remaining: 249ms\n740:    learn: 14.6335162   total: 709ms    remaining: 248ms\n741:    learn: 14.5998758   total: 710ms    remaining: 247ms\n742:    learn: 14.5869238   total: 711ms    remaining: 246ms\n743:    learn: 14.5759591   total: 712ms    remaining: 245ms\n744:    learn: 14.5547290   total: 714ms    remaining: 244ms\n745:    learn: 14.5283070   total: 715ms    remaining: 243ms\n746:    learn: 14.4853954   total: 716ms    remaining: 242ms\n747:    learn: 14.4757814   total: 717ms    remaining: 241ms\n748:    learn: 14.4354938   total: 718ms    remaining: 241ms\n749:    learn: 14.4036861   total: 719ms    remaining: 240ms\n750:    learn: 14.3687833   total: 720ms    remaining: 239ms\n751:    learn: 14.3437669   total: 721ms    remaining: 238ms\n752:    learn: 14.3209004   total: 722ms    remaining: 237ms\n753:    learn: 14.2981607   total: 723ms    remaining: 236ms\n754:    learn: 14.2959300   total: 724ms    remaining: 235ms\n755:    learn: 14.2680160   total: 724ms    remaining: 234ms\n756:    learn: 14.2535389   total: 725ms    remaining: 233ms\n757:    learn: 14.2501520   total: 727ms    remaining: 232ms\n758:    learn: 14.2260538   total: 728ms    remaining: 231ms\n759:    learn: 14.2234442   total: 729ms    remaining: 230ms\n760:    learn: 14.1873135   total: 730ms    remaining: 229ms\n761:    learn: 14.1765115   total: 731ms    remaining: 228ms\n762:    learn: 14.1697434   total: 731ms    remaining: 227ms\n763:    learn: 14.1365525   total: 732ms    remaining: 226ms\n764:    learn: 14.1015278   total: 733ms    remaining: 225ms\n765:    learn: 14.0622675   total: 734ms    remaining: 224ms\n766:    learn: 14.0409553   total: 735ms    remaining: 223ms\n767:    learn: 14.0337570   total: 736ms    remaining: 222ms\n768:    learn: 13.9910329   total: 737ms    remaining: 221ms\n769:    learn: 13.9510113   total: 738ms    remaining: 220ms\n770:    learn: 13.9174679   total: 739ms    remaining: 220ms\n771:    learn: 13.8864002   total: 740ms    remaining: 219ms\n772:    learn: 13.8446475   total: 741ms    remaining: 218ms\n773:    learn: 13.8397236   total: 742ms    remaining: 217ms\n774:    learn: 13.8029820   total: 743ms    remaining: 216ms\n775:    learn: 13.7857820   total: 744ms    remaining: 215ms\n776:    learn: 13.7646024   total: 745ms    remaining: 214ms\n777:    learn: 13.7486851   total: 746ms    remaining: 213ms\n778:    learn: 13.7014327   total: 747ms    remaining: 212ms\n779:    learn: 13.6934358   total: 747ms    remaining: 211ms\n780:    learn: 13.6683386   total: 748ms    remaining: 210ms\n781:    learn: 13.6566338   total: 749ms    remaining: 209ms\n782:    learn: 13.6444486   total: 750ms    remaining: 208ms\n783:    learn: 13.6250279   total: 751ms    remaining: 207ms\n784:    learn: 13.5875176   total: 752ms    remaining: 206ms\n785:    learn: 13.5783446   total: 752ms    remaining: 205ms\n786:    learn: 13.5576008   total: 753ms    remaining: 204ms\n787:    learn: 13.5477919   total: 754ms    remaining: 203ms\n788:    learn: 13.5055189   total: 755ms    remaining: 202ms\n789:    learn: 13.4644475   total: 756ms    remaining: 201ms\n790:    learn: 13.4570524   total: 757ms    remaining: 200ms\n791:    learn: 13.4244055   total: 758ms    remaining: 199ms\n792:    learn: 13.4171601   total: 759ms    remaining: 198ms\n793:    learn: 13.3977292   total: 760ms    remaining: 197ms\n794:    learn: 13.3911146   total: 761ms    remaining: 196ms\n795:    learn: 13.3816584   total: 762ms    remaining: 195ms\n796:    learn: 13.3539009   total: 763ms    remaining: 194ms\n797:    learn: 13.3166557   total: 764ms    remaining: 193ms\n798:    learn: 13.2942330   total: 765ms    remaining: 192ms\n799:    learn: 13.2692862   total: 765ms    remaining: 191ms\n800:    learn: 13.2547553   total: 766ms    remaining: 190ms\n801:    learn: 13.2259343   total: 767ms    remaining: 189ms\n802:    learn: 13.2246521   total: 768ms    remaining: 188ms\n803:    learn: 13.2119291   total: 769ms    remaining: 188ms\n804:    learn: 13.1793474   total: 771ms    remaining: 187ms\n805:    learn: 13.1652542   total: 772ms    remaining: 186ms\n806:    learn: 13.1293534   total: 773ms    remaining: 185ms\n807:    learn: 13.1211653   total: 774ms    remaining: 184ms\n808:    learn: 13.1110732   total: 775ms    remaining: 183ms\n809:    learn: 13.1056131   total: 776ms    remaining: 182ms\n810:    learn: 13.0704954   total: 776ms    remaining: 181ms\n811:    learn: 13.0482918   total: 777ms    remaining: 180ms\n812:    learn: 13.0313717   total: 778ms    remaining: 179ms\n813:    learn: 13.0295572   total: 779ms    remaining: 178ms\n814:    learn: 13.0076548   total: 780ms    remaining: 177ms\n815:    learn: 13.0056559   total: 781ms    remaining: 176ms\n816:    learn: 12.9992709   total: 782ms    remaining: 175ms\n817:    learn: 12.9824007   total: 783ms    remaining: 174ms\n818:    learn: 12.9670734   total: 784ms    remaining: 173ms\n819:    learn: 12.9372833   total: 785ms    remaining: 172ms\n820:    learn: 12.9087596   total: 786ms    remaining: 171ms\n821:    learn: 12.9075563   total: 787ms    remaining: 170ms\n822:    learn: 12.8694642   total: 788ms    remaining: 169ms\n823:    learn: 12.8683302   total: 789ms    remaining: 168ms\n824:    learn: 12.8490869   total: 790ms    remaining: 168ms\n825:    learn: 12.8290022   total: 791ms    remaining: 167ms\n826:    learn: 12.8107471   total: 792ms    remaining: 166ms\n827:    learn: 12.7834375   total: 793ms    remaining: 165ms\n828:    learn: 12.7492612   total: 794ms    remaining: 164ms\n829:    learn: 12.7143908   total: 795ms    remaining: 163ms\n830:    learn: 12.6794802   total: 796ms    remaining: 162ms\n831:    learn: 12.6575840   total: 796ms    remaining: 161ms\n832:    learn: 12.6414062   total: 797ms    remaining: 160ms\n833:    learn: 12.6323550   total: 798ms    remaining: 159ms\n834:    learn: 12.6004844   total: 799ms    remaining: 158ms\n835:    learn: 12.5992796   total: 800ms    remaining: 157ms\n836:    learn: 12.5664653   total: 801ms    remaining: 156ms\n837:    learn: 12.5389714   total: 802ms    remaining: 155ms\n838:    learn: 12.5116178   total: 803ms    remaining: 154ms\n839:    learn: 12.4901320   total: 803ms    remaining: 153ms\n840:    learn: 12.4812527   total: 805ms    remaining: 152ms\n841:    learn: 12.4562707   total: 805ms    remaining: 151ms\n842:    learn: 12.4209878   total: 806ms    remaining: 150ms\n843:    learn: 12.4015992   total: 807ms    remaining: 149ms\n844:    learn: 12.3675998   total: 808ms    remaining: 148ms\n845:    learn: 12.3521691   total: 809ms    remaining: 147ms\n846:    learn: 12.3426886   total: 810ms    remaining: 146ms\n847:    learn: 12.3188571   total: 811ms    remaining: 145ms\n848:    learn: 12.2937388   total: 812ms    remaining: 144ms\n849:    learn: 12.2724402   total: 813ms    remaining: 143ms\n850:    learn: 12.2342559   total: 814ms    remaining: 142ms\n851:    learn: 12.2159725   total: 815ms    remaining: 142ms\n852:    learn: 12.2147289   total: 816ms    remaining: 141ms\n853:    learn: 12.1935801   total: 817ms    remaining: 140ms\n854:    learn: 12.1795597   total: 818ms    remaining: 139ms\n855:    learn: 12.1489328   total: 819ms    remaining: 138ms\n856:    learn: 12.1476791   total: 820ms    remaining: 137ms\n857:    learn: 12.1143878   total: 821ms    remaining: 136ms\n858:    learn: 12.1133877   total: 821ms    remaining: 135ms\n859:    learn: 12.0927666   total: 822ms    remaining: 134ms\n860:    learn: 12.0908934   total: 823ms    remaining: 133ms\n861:    learn: 12.0667739   total: 824ms    remaining: 132ms\n862:    learn: 12.0312513   total: 825ms    remaining: 131ms\n863:    learn: 12.0298478   total: 826ms    remaining: 130ms\n864:    learn: 12.0174484   total: 827ms    remaining: 129ms\n865:    learn: 12.0093901   total: 828ms    remaining: 128ms\n866:    learn: 11.9731302   total: 829ms    remaining: 127ms\n867:    learn: 11.9617694   total: 830ms    remaining: 126ms\n868:    learn: 11.9536413   total: 831ms    remaining: 125ms\n869:    learn: 11.9234807   total: 832ms    remaining: 124ms\n870:    learn: 11.8995296   total: 833ms    remaining: 123ms\n871:    learn: 11.8985399   total: 833ms    remaining: 122ms\n872:    learn: 11.8748993   total: 834ms    remaining: 121ms\n873:    learn: 11.8473937   total: 835ms    remaining: 120ms\n874:    learn: 11.7980081   total: 836ms    remaining: 119ms\n875:    learn: 11.7968597   total: 837ms    remaining: 119ms\n876:    learn: 11.7952536   total: 838ms    remaining: 118ms\n877:    learn: 11.7765712   total: 839ms    remaining: 117ms\n878:    learn: 11.7433623   total: 840ms    remaining: 116ms\n879:    learn: 11.7220227   total: 841ms    remaining: 115ms\n880:    learn: 11.7079171   total: 842ms    remaining: 114ms\n881:    learn: 11.6892547   total: 843ms    remaining: 113ms\n882:    learn: 11.6752283   total: 844ms    remaining: 112ms\n883:    learn: 11.6465248   total: 845ms    remaining: 111ms\n884:    learn: 11.6199604   total: 846ms    remaining: 110ms\n885:    learn: 11.5840290   total: 847ms    remaining: 109ms\n886:    learn: 11.5665343   total: 848ms    remaining: 108ms\n887:    learn: 11.5653334   total: 849ms    remaining: 107ms\n888:    learn: 11.5409858   total: 850ms    remaining: 106ms\n889:    learn: 11.5126963   total: 850ms    remaining: 105ms\n890:    learn: 11.4937102   total: 851ms    remaining: 104ms\n891:    learn: 11.4652907   total: 852ms    remaining: 103ms\n892:    learn: 11.4406454   total: 853ms    remaining: 102ms\n893:    learn: 11.4157600   total: 854ms    remaining: 101ms\n894:    learn: 11.4145225   total: 855ms    remaining: 100ms\n895:    learn: 11.3944764   total: 856ms    remaining: 99.4ms\n896:    learn: 11.3718753   total: 857ms    remaining: 98.4ms\n897:    learn: 11.3698629   total: 858ms    remaining: 97.5ms\n898:    learn: 11.3652583   total: 859ms    remaining: 96.5ms\n899:    learn: 11.3403921   total: 860ms    remaining: 95.6ms\n900:    learn: 11.3188723   total: 861ms    remaining: 94.6ms\n901:    learn: 11.2825028   total: 862ms    remaining: 93.6ms\n902:    learn: 11.2756316   total: 863ms    remaining: 92.7ms\n903:    learn: 11.2746247   total: 864ms    remaining: 91.7ms\n904:    learn: 11.2506409   total: 864ms    remaining: 90.7ms\n905:    learn: 11.2459222   total: 865ms    remaining: 89.8ms\n906:    learn: 11.2231937   total: 866ms    remaining: 88.8ms\n907:    learn: 11.2191230   total: 867ms    remaining: 87.8ms\n908:    learn: 11.1878231   total: 868ms    remaining: 86.9ms\n909:    learn: 11.1635473   total: 869ms    remaining: 86ms\n910:    learn: 11.1593446   total: 870ms    remaining: 85ms\n911:    learn: 11.1312444   total: 871ms    remaining: 84.1ms\n912:    learn: 11.1178570   total: 872ms    remaining: 83.1ms\n913:    learn: 11.1042827   total: 873ms    remaining: 82.1ms\n914:    learn: 11.0835715   total: 874ms    remaining: 81.2ms\n915:    learn: 11.0643196   total: 875ms    remaining: 80.2ms\n916:    learn: 11.0602889   total: 876ms    remaining: 79.3ms\n917:    learn: 11.0272219   total: 877ms    remaining: 78.3ms\n918:    learn: 10.9986897   total: 878ms    remaining: 77.3ms\n919:    learn: 10.9792479   total: 879ms    remaining: 76.4ms\n920:    learn: 10.9535501   total: 879ms    remaining: 75.4ms\n921:    learn: 10.9275044   total: 880ms    remaining: 74.5ms\n922:    learn: 10.9219224   total: 881ms    remaining: 73.5ms\n923:    learn: 10.9035961   total: 882ms    remaining: 72.6ms\n924:    learn: 10.8891537   total: 884ms    remaining: 71.6ms\n925:    learn: 10.8702443   total: 885ms    remaining: 70.7ms\n926:    learn: 10.8421716   total: 886ms    remaining: 69.7ms\n927:    learn: 10.8203110   total: 886ms    remaining: 68.8ms\n928:    learn: 10.7915801   total: 888ms    remaining: 67.8ms\n929:    learn: 10.7652304   total: 888ms    remaining: 66.9ms\n930:    learn: 10.7342804   total: 889ms    remaining: 65.9ms\n931:    learn: 10.7211866   total: 890ms    remaining: 65ms\n932:    learn: 10.7047031   total: 891ms    remaining: 64ms\n933:    learn: 10.6961734   total: 892ms    remaining: 63ms\n934:    learn: 10.6841913   total: 893ms    remaining: 62.1ms\n935:    learn: 10.6763968   total: 894ms    remaining: 61.1ms\n936:    learn: 10.6645409   total: 895ms    remaining: 60.2ms\n937:    learn: 10.6631696   total: 896ms    remaining: 59.2ms\n938:    learn: 10.6586104   total: 896ms    remaining: 58.2ms\n939:    learn: 10.6395476   total: 897ms    remaining: 57.3ms\n940:    learn: 10.6354434   total: 898ms    remaining: 56.3ms\n941:    learn: 10.6184324   total: 899ms    remaining: 55.4ms\n942:    learn: 10.6055869   total: 900ms    remaining: 54.4ms\n943:    learn: 10.5965561   total: 901ms    remaining: 53.5ms\n944:    learn: 10.5955793   total: 902ms    remaining: 52.5ms\n945:    learn: 10.5947611   total: 903ms    remaining: 51.6ms\n946:    learn: 10.5693144   total: 904ms    remaining: 50.6ms\n947:    learn: 10.5637694   total: 905ms    remaining: 49.6ms\n948:    learn: 10.5374266   total: 906ms    remaining: 48.7ms\n949:    learn: 10.5099668   total: 907ms    remaining: 47.7ms\n950:    learn: 10.5086617   total: 908ms    remaining: 46.8ms\n951:    learn: 10.5067682   total: 909ms    remaining: 45.8ms\n952:    learn: 10.4925699   total: 909ms    remaining: 44.9ms\n953:    learn: 10.4716689   total: 911ms    remaining: 43.9ms\n954:    learn: 10.4379081   total: 912ms    remaining: 43ms\n955:    learn: 10.4006751   total: 913ms    remaining: 42ms\n956:    learn: 10.3836660   total: 913ms    remaining: 41ms\n957:    learn: 10.3575722   total: 915ms    remaining: 40.1ms\n958:    learn: 10.3478138   total: 916ms    remaining: 39.1ms\n959:    learn: 10.3187226   total: 916ms    remaining: 38.2ms\n960:    learn: 10.3022324   total: 918ms    remaining: 37.2ms\n961:    learn: 10.2997720   total: 919ms    remaining: 36.3ms\n962:    learn: 10.2989286   total: 920ms    remaining: 35.3ms\n963:    learn: 10.2895343   total: 921ms    remaining: 34.4ms\n964:    learn: 10.2815641   total: 922ms    remaining: 33.4ms\n965:    learn: 10.2625952   total: 923ms    remaining: 32.5ms\n966:    learn: 10.2506695   total: 924ms    remaining: 31.5ms\n967:    learn: 10.2397609   total: 925ms    remaining: 30.6ms\n968:    learn: 10.2154145   total: 927ms    remaining: 29.6ms\n969:    learn: 10.2143644   total: 928ms    remaining: 28.7ms\n970:    learn: 10.1981073   total: 929ms    remaining: 27.7ms\n971:    learn: 10.1920221   total: 930ms    remaining: 26.8ms\n972:    learn: 10.1806991   total: 931ms    remaining: 25.8ms\n973:    learn: 10.1597212   total: 933ms    remaining: 24.9ms\n974:    learn: 10.1358828   total: 934ms    remaining: 23.9ms\n975:    learn: 10.1222784   total: 935ms    remaining: 23ms\n976:    learn: 10.1128777   total: 936ms    remaining: 22ms\n977:    learn: 10.1065225   total: 937ms    remaining: 21.1ms\n978:    learn: 10.0954545   total: 938ms    remaining: 20.1ms\n979:    learn: 10.0725280   total: 939ms    remaining: 19.2ms\n980:    learn: 10.0443783   total: 940ms    remaining: 18.2ms\n981:    learn: 10.0421942   total: 941ms    remaining: 17.2ms\n982:    learn: 10.0135134   total: 941ms    remaining: 16.3ms\n983:    learn: 10.0009906   total: 942ms    remaining: 15.3ms\n984:    learn: 9.9894585    total: 943ms    remaining: 14.4ms\n985:    learn: 9.9872832    total: 944ms    remaining: 13.4ms\n986:    learn: 9.9654572    total: 945ms    remaining: 12.4ms\n987:    learn: 9.9461931    total: 946ms    remaining: 11.5ms\n988:    learn: 9.9451290    total: 947ms    remaining: 10.5ms\n989:    learn: 9.9431210    total: 948ms    remaining: 9.57ms\n990:    learn: 9.9299945    total: 949ms    remaining: 8.62ms\n991:    learn: 9.9131150    total: 950ms    remaining: 7.66ms\n992:    learn: 9.9123473    total: 950ms    remaining: 6.7ms\n993:    learn: 9.8978089    total: 951ms    remaining: 5.74ms\n994:    learn: 9.8841806    total: 952ms    remaining: 4.79ms\n995:    learn: 9.8638563    total: 953ms    remaining: 3.83ms\n996:    learn: 9.8491248    total: 954ms    remaining: 2.87ms\n997:    learn: 9.8427010    total: 955ms    remaining: 1.91ms\n998:    learn: 9.8417925    total: 956ms    remaining: 957us\n999:    learn: 9.8380086    total: 957ms    remaining: 0us\nMean absolute error: 43.06\nMean absolute percentage error: 39.39%\nRoot Mean squared error: 55.14\nCoefficient of determination: 0.45\n\n\n\n\n\n\n\n\n\n\nplot_residuals(diabetes_y_test, diabetes_y_pred)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_tree_based_models_regression.html#plotting-residuals",
    "href": "4e_tree_based_models_regression.html#plotting-residuals",
    "title": "9  Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression",
    "section": "10.1 Plotting Residuals",
    "text": "10.1 Plotting Residuals\nPlotting the residuals (errors) can help us to understand whether the model is consistently over or under estimating.",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree-Based Models for Regression - Predicting a Numeric Indicator of Diabetes Disease Progression</span>"
    ]
  },
  {
    "objectID": "4e_regression_tree_exercise_SOLUTION.html",
    "href": "4e_regression_tree_exercise_SOLUTION.html",
    "title": "10  Exercise Solution: Regression with Trees (LOS Dataset)",
    "section": "",
    "text": "10.1 Core\nWe’re going to work with a dataset to try to predict patient length of stay.\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import the relevant models from Sklearn, XGBoost, CatBoost and LightGBM\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom ydata_profiling import ProfileReport\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n# import any other libraries you need\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, \\\n                            r2_score, root_mean_squared_error\nOpen the data dictionary in the los_dataset folder and take a look at what data is available.\nNext, load in the dataframe containing the LOS data.\nlos_df = pd.read_csv(\"../datasets/los_dataset/LengthOfStay.csv\", index_col=\"eid\")\nView the dataframe.\nlos_df.head()\n\n\n\n\n\n\n\n\n\nvdate\nrcount\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\n...\nsodium\nglucose\nbloodureanitro\ncreatinine\nbmi\npulse\nrespiration\nsecondarydiagnosisnonicd9\nfacid\nlengthofstay\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n8/29/2012\n0\nF\n0\n0\n0\n0\n0\n0\n0\n...\n140.361132\n192.476918\n12.0\n1.390722\n30.432418\n96\n6.5\n4\nB\n3\n\n\n2\n5/26/2012\n5+\nF\n0\n0\n0\n0\n0\n0\n0\n...\n136.731692\n94.078507\n8.0\n0.943164\n28.460516\n61\n6.5\n1\nA\n7\n\n\n3\n9/22/2012\n1\nF\n0\n0\n0\n0\n0\n0\n0\n...\n133.058514\n130.530524\n12.0\n1.065750\n28.843812\n64\n6.5\n2\nB\n3\n\n\n4\n8/9/2012\n0\nF\n0\n0\n0\n0\n0\n0\n0\n...\n138.994023\n163.377028\n12.0\n0.906862\n27.959007\n76\n6.5\n1\nA\n1\n\n\n5\n12/20/2012\n0\nF\n0\n0\n0\n1\n0\n1\n0\n...\n138.634836\n94.886654\n11.5\n1.242854\n30.258927\n67\n5.6\n2\nE\n4\n\n\n\n\n5 rows × 26 columns\nProfileReport(los_df)\nConsider what columns to remove.\nHINT: Is there a column in the dataset that doesn’t really make much sense to predict from? If you’re not sure, use the full dataset for now and come back to this later.\nNOTE: For now, we’re going to assume that all of the included measures will be available to us at the point we need to make a prediction - they’re not things that will be calculated later in the patient’s stay.\nlos_df = los_df.drop(columns=\"vdate\")\nlos_df.head(1)\n\n\n\n\n\n\n\n\n\nrcount\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\n...\nsodium\nglucose\nbloodureanitro\ncreatinine\nbmi\npulse\nrespiration\nsecondarydiagnosisnonicd9\nfacid\nlengthofstay\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\nF\n0\n0\n0\n0\n0\n0\n0\n0\n...\n140.361132\n192.476918\n12.0\n1.390722\n30.432418\n96\n6.5\n4\nB\n3\n\n\n\n\n1 rows × 25 columns\nConvert categories with only two options into a boolean value (e.g. for a gender column in which gender has only been provided as M or F, you could encode M as 0 and F as 1).\nlos_df.gender.unique()\n\narray(['F', 'M'], dtype=object)\nlos_df['gender'].replace('M', 0, inplace=True)\nlos_df['gender'].replace('F', 1, inplace=True)\n\nlos_df.gender.unique()\n\narray([1, 0], dtype=int64)\nConvert columns with multiple options per category into multiple columns using one-hot encoding.\nlos_df.facid.unique()\n\n# Bonus - astype('int') will convert the true/false values to 0/1\n# not necessary - it will work regardless\none_hot = pd.get_dummies(los_df['facid']).astype('int')\nlos_df = los_df.drop('facid', axis=1)\nlos_df = los_df.join(one_hot)\nlos_df.head()\n\n\n\n\n\n\n\n\n\nrcount\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\n...\nbmi\npulse\nrespiration\nsecondarydiagnosisnonicd9\nlengthofstay\nA\nB\nC\nD\nE\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n30.432418\n96\n6.5\n4\n3\n0\n1\n0\n0\n0\n\n\n2\n5+\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n28.460516\n61\n6.5\n1\n7\n1\n0\n0\n0\n0\n\n\n3\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n28.843812\n64\n6.5\n2\n3\n0\n1\n0\n0\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n27.959007\n76\n6.5\n1\n1\n1\n0\n0\n0\n0\n\n\n5\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n...\n30.258927\n67\n5.6\n2\n4\n0\n0\n0\n0\n1\n\n\n\n\n5 rows × 29 columns\nlos_df.rcount.value_counts()\n\n# Bonus - astype('int') will convert the true/false values to 0/1\n# not necessary - it will work regardless\none_hot = pd.get_dummies(los_df['rcount'], prefix=\"rcount\").astype('int')\nlos_df = los_df.drop('rcount', axis=1)\nlos_df = los_df.join(one_hot)\nlos_df.head()\n\n\n\n\n\n\n\n\n\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\nfibrosisandother\n...\nB\nC\nD\nE\nrcount_0\nrcount_1\nrcount_2\nrcount_3\nrcount_4\nrcount_5+\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n5\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 34 columns\nTrain a decision tree model to predict length of stay based on the variables in this dataset.\nX = los_df.drop(columns='lengthofstay')\ny = los_df['lengthofstay']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size = 0.25,\n    random_state=42\n    )\n\nregr_dt = DecisionTreeRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_dt.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_train = regr_dt.predict(X_train)\ny_pred_test = regr_dt.predict(X_test)\ny_pred_test\n\narray([3., 1., 3., ..., 8., 2., 5.])\nAssess the performance of this model.\nprint(\"TRAINING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_train, y_pred_train):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_train, y_pred_train):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_train, y_pred_train))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_train, y_pred_train))\n\nTRAINING DATA\nMean absolute error: 0.00\nMean absolute percentage error: 0.00%\nRoot Mean squared error: 0.00\nCoefficient of determination: 1.00\nprint(\"TESTING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred_test):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred_test):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_test, y_pred_test))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_test))\n\nTRAINING DATA\nMean absolute error: 0.50\nMean absolute percentage error: 12.94%\nRoot Mean squared error: 0.93\nCoefficient of determination: 0.84\ndef plot_residuals(actual, predicted):\n    residuals = actual - predicted\n\n    plt.figure(figsize=(10, 5))\n    plt.hist(residuals, bins=20)\n    plt.axvline(x = 0, color = 'r')\n    plt.xlabel('Residual')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Residuals')\n    plt.show()\n\nplot_residuals(y_test, y_pred_test)\nNote that here the alpha parameter has been set to 0.05 - this means that the points will be transparent. Higher numbers of points in a given space will appear more opaque.\ndef plot_actual_vs_predicted(actual, predicted):\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.scatter(actual, predicted, color=\"black\", alpha=0.05)\n    ax.axline((1, 1), slope=1)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('True vs Predicted Values')\n    plt.show()\n\nplot_actual_vs_predicted(y_test, y_pred_test)\nTrain a boosting model to predict length of stay based on the variables in this dataset.\nX = los_df.drop(columns='lengthofstay')\ny = los_df['lengthofstay']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size = 0.25,\n    random_state=42\n    )\n\nregr_xgb = XGBRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_xgb.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_train = regr_xgb.predict(X_train)\ny_pred_test = regr_xgb.predict(X_test)\ny_pred_test\n\narray([3.6313417, 0.8304186, 2.4800673, ..., 5.5354204, 1.5881324,\n       5.249632 ], dtype=float32)\nAssess the performance of this model and compare it with your decision tree model.\nprint(\"TRAINING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_train, y_pred_train):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_train, y_pred_train):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_train, y_pred_train))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_train, y_pred_train))\n\nTRAINING DATA\nMean absolute error: 0.29\nMean absolute percentage error: 10.66%\nRoot Mean squared error: 0.37\nCoefficient of determination: 0.98\nprint(\"TESTING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred_test):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred_test):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_test, y_pred_test))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_test))\n\nTESTING DATA\nMean absolute error: 0.33\nMean absolute percentage error: 11.60%\nRoot Mean squared error: 0.44\nCoefficient of determination: 0.96\nplot_residuals(y_test, y_pred_test)\nplot_actual_vs_predicted(y_test, y_pred_test)",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exercise Solution: Regression with Trees (LOS Dataset)</span>"
    ]
  },
  {
    "objectID": "4e_regression_tree_exercise_SOLUTION.html#extension",
    "href": "4e_regression_tree_exercise_SOLUTION.html#extension",
    "title": "10  Exercise Solution: Regression with Trees (LOS Dataset)",
    "section": "10.2 Extension",
    "text": "10.2 Extension\nContinue to refine your model.\nYou could - tune hyperparameters - try additional model types (e.g. different boosting trees, linear regression) - try dropping some features - calculate additional metrics - add some visualisations of the features in the dataset - store the results from your different model iterations and display this as a table of metrics - turn this table of metrics into a graph - try using k-fold cross validation\n\n# NO SOLUTION GIVEN",
    "crumbs": [
      "4E - Tree-based models for regression",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exercise Solution: Regression with Trees (LOS Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html",
    "href": "4f_HSMA_neural_nets.html",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "",
    "text": "11.1 The neural network unit - a neuron or perceptron\nThe building block of a neural network is a neuron, which is essentially the same as the ‘perceptron’ described by Frank Rosenblatt in 1958.\nThe neuron, or perceptron, takes inputs X and weights W (each individual input has a weight; a bias weight is also introduced by creating a dummy input with value 1). The neuron sums the input multiplied by the weight and passes the output to an activation function. The simplest activation function is a step function, whereby if the output is &gt;0 the output of the activation function is 1, otherwise the output is 0.",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#neural-networks",
    "href": "4f_HSMA_neural_nets.html#neural-networks",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.2 Neural networks",
    "text": "11.2 Neural networks\nHaving understood a neuron - which calculates the weighted sum of its inputs and passes it through an activation function, neural networks are easy(ish)!\nThey are ‘just’ a network of such neurons, where the output of one becomes one of the inputs to the neurons in the next layer.\nThis allows any complexity of function to be mimicked by a neural network (so long as the network includes a non-linear activation function, like ReLU - see below).\nNote the output layer may be composed of a single neuron, to predict a single value or single probability, or may be multiple neurons, to predict multiple values or multiple probabilities.",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#activation-functions",
    "href": "4f_HSMA_neural_nets.html#activation-functions",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.3 Activation functions",
    "text": "11.3 Activation functions\nEach neuron calculates the weighted sum of its inputs and passes that sum to an activation function. The two simplest functions are:\n\nLinear: The weighted output is passed forward with no change.\nStep: The output of the activation function is 0 or 1 depending on whether a threshold is reached.\n\nOther common activation functions are:\n\nSigmoid: Scales output 0-1 using a logistic function. Note that our simple single perceptron becomes a logistic regression model if we use a sigmoid activation function. The sigmoid function is often used to produce a probability output at the final layer.\ntanh: Scales output -1 to 1. Commonly used in older neural network models. Not commonly used now.\nReLU (rectifying linear unit): Simply converts all negative values to zero, and leaves positive values unchanged. This very simple method is very common in deep neural networks, and is sufficient to allow networks to model non-linear functions.\nLeaky ReLU and Exponential Linear Unit (ELU): Common modifications to ReLU that do not have such a hard constraint on negative inputs, and can be useful if we run into the Dying ReLU problem (in which - typically due to high learning rates - our weights are commonly set to negative values, leading to them effectively being switched off (set to 0) under ReLU). Try them out as replacements to ReLU.\nMaxout: A generalised activation function that can model a complex non-linear activation function.\nSoftMax: SoftMax is the final layer to use if you wish to normalise probability outputs from a network which has multiple class outputs (e.g. you want the total of your probabilities for “this is dog”, “this is a cat”, “this is a fish” etc to add up to 1).",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#loss-functions",
    "href": "4f_HSMA_neural_nets.html#loss-functions",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.4 Loss functions",
    "text": "11.4 Loss functions\nLoss functions are critical to neural networks as they provide the measure by which the neural network is in error, allowing modification of the network to reduce error.\nThe most common loss functions are:\n\nMean Squared Error Loss: Common loss function for regression (predicting values rather than class).\nCross Entropy Loss: Common loss function for classification. Binary Cross Entropy Loss is used when the output is a binary classifier (like survive/die in the Titanic model).",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#how-do-neural-networks-learn-backpropagation-and-optimisation",
    "href": "4f_HSMA_neural_nets.html#how-do-neural-networks-learn-backpropagation-and-optimisation",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.5 How do neural networks learn? Backpropagation and optimisation",
    "text": "11.5 How do neural networks learn? Backpropagation and optimisation\nBackpropagation is the process by which the final loss is distributed back through the network, allowing each weight to be updated in proportion to its contribution to the final error.\nFor more on backpropagation see: https://youtu.be/Ilg3gGewQ5U\nFor deeper maths on backpropagation see: https://youtu.be/tIeHLnjs5U8\nOptimisation is the step-wise process by which weights are updated. The basic underlying method, gradient descent, is that weights are adjusted in the direction that improves fit, and that weights are adjust more when the gradient (how much the output changes with each unit change to the weight) is higher.\nCommon optimisers used are:\n\nStochastic gradient descent: Updates gradients based on single samples. Can be inefficient, so can be modified to use gradients based on a small batch (e.g. 8-64) of samples. Momentum may also be added to avoid becoming trapped in local minima.\nRMSprop: A ‘classic’ benchmark optimiser. Adjusts steps based on a weighted average of all weight gradients.\nAdam: The most common optimiser used today. Has complex adaptive momentum for speeding up learning.\n\nFor more on optimisers see: https://youtu.be/mdKjMPmcWjY",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#training-a-neural-network---the-practicalities",
    "href": "4f_HSMA_neural_nets.html#training-a-neural-network---the-practicalities",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.6 Training a neural network - the practicalities",
    "text": "11.6 Training a neural network - the practicalities\nThe training process of a neural network consists of three general phases which are repeated across all the data. All of the data is passed through the network multiple times (the number of iterations, which may be as few as 3-5 or may be 1000+) until all of the data has been fed forward and backpropogated - this then represents an “Epoch”. The three phases of an iteration are :\n\nPass training X data to the network and predict y\nCalculate the ‘loss’ (error) between the predicted and observed (actual) values of y\nBackpropagate the loss and update the weights (the job of the optimiser).\n\nThe learning is repeated until maximum accuracy is achieved (but keep an eye on accuracy of test data as well as training data as the network may develop significant over-fitting to training data unless steps are taken to offset the potential for over-fitting, such as use of ‘drop-out’ layers described below).",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#architectures",
    "href": "4f_HSMA_neural_nets.html#architectures",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.7 Architectures",
    "text": "11.7 Architectures\nThe most common fully connected architecture design is to have the same number of neurons in each layer, and adjust that number and the number of layers. This makes exploring the size of the neural net relatively easy (if sometimes slow).\nAs a rough guide - the size of the neural net should be increased until it over-fits data (increasing accuracy of training data with reducing accuracy of test data), and then use a form of regularisation to reduce the over-fitting (we will go through this process below).\nSome common architecture designs, which may be mixed in a single larger network, are:\n\nFully connected: The output of each neuron goes to all neurons in the next layer.\nConvolutional: Common in image analysis. Small ‘mini-nets’ that look for patterns across the data - like a ‘sliding window’, but that can look at the whole picture at the same time. May also be used, for example, in time series to look for fingerprints of events anywhere in the time series.\nRecurrent: Introduce the concept of some (limited) form of memory into the network - at any one time a number of input steps are affecting the network output. Useful, for example, in sound or video analysis.\nTransformers: Sequence-to-sequence architecture. Convert sequences to sequences (e.g. translation). Big in Natural Language Processing - we’ll cover them in the NLP module.\nEmbedding: Converts a categorical value to a vector of numbers, e.g. word-2-vec converts words to vectors such that similar meaning words are positioned close together.\nEncoding: Reduce many input features to fewer. This ‘compresses’ the data. De-coding layers may convert back to the original data.\nGenerative: Rather than regression, or classification, generative networks output some form of synthetic data (such as fake images; see https://www.thispersondoesnotexist.com/).\n\nFor the kind of classification problem we’re looking at here, a Fully Connected Neural Network is the most commonly used architecture now, and typically you keep all layers the same size (the same number of Neurons) apart from your output layer. This makes it easy to test different sizes of network.",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#additional-resources",
    "href": "4f_HSMA_neural_nets.html#additional-resources",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.8 Additional resources",
    "text": "11.8 Additional resources\nAlso see the excellent introductory video (20 minutes) from 3brown1blue: https://youtu.be/aircAruvnKk",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#lets-go",
    "href": "4f_HSMA_neural_nets.html#lets-go",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.9 Let’s go !!!!!!!!!!!!!",
    "text": "11.9 Let’s go !!!!!!!!!!!!!\nIn this first cell, we’re going to be a bit naughty, and turn off warnings (such as “you’re using an out-of-date version of this” etc). This will make the notebook cleaner and easier to interpret as you learn this, but in real-world work you shouldn’t really do this unless you know what you’re doing. But we’ll do it here because we do (I think).\nDon’t forget to select the tf_hsma environment when you run the first cell. If you’re prompted that you need to install the ipykernel, click that you want to do it.\n\n# Turn warnings off to keep notebook tidy\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#load-modules",
    "href": "4f_HSMA_neural_nets.html#load-modules",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.10 Load modules",
    "text": "11.10 Load modules\nFirst we need to import the packages we’re going to use. The first three (MatPlotLib, NumPy and Pandas) are the stuff we use in pretty much everything in data science. From SciKitLearn, we import functions to automatically split our data into training and test data (as we did for the Logistic Regression example) and to min-max normalise our data (remember we said that normalising our data is typical with Neural Networks (“Neural Networks are Normal”), and standardising our data - what we did last time - is typical with Logistic Regression). Remember, when we normalise we’ll scale all our feature values so they fall between 0 and 1.\nThen, we import a load of things we’ll need from TensorFlow (and particularly Keras). TensorFlow is the Neural Network architecture developed by Google, but the interface (API) for TensorFlow is not easy to use. So instead, we use Keras, which sits on top of TensorFlow, and allows us to interact with TensorFlow in a much more straightforward way. Don’t worry about what each of things that we import are at this stage - we’ll see them in use as we move through the notebook.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# sklearn for pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# TensorFlow sequential model\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#download-data-if-not-previously-downloaded",
    "href": "4f_HSMA_neural_nets.html#download-data-if-not-previously-downloaded",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.11 Download data if not previously downloaded",
    "text": "11.11 Download data if not previously downloaded\nThis cell downloads the Titanic data that we’re going to use. You don’t need to do this if you’ve already downloaded the data, but if you’re unsure, run the cell anyway (it takes seconds!).\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#define-function-to-scale-data",
    "href": "4f_HSMA_neural_nets.html#define-function-to-scale-data",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.12 Define function to scale data",
    "text": "11.12 Define function to scale data\nIn neural networks it is common to normalise (scale input data 0-1) rather than use standardise (subtracting mean and dividing by standard deviation) each feature. As with the Logistic Regression example, we’ll set up a function here that we can call whenever we want to do this (the only difference being that in the Logistic Regression example we standardised our data, rather than normalising it).\n\ndef scale_data(X_train, X_test):\n    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n\n    # Initialise a new scaling object for normalising input data\n    sc = MinMaxScaler()\n\n    # Apply the scaler to the training and test sets\n    train_sc = sc.fit_transform(X_train)\n    test_sc = sc.fit_transform(X_test)\n\n    return train_sc, test_sc",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#load-data",
    "href": "4f_HSMA_neural_nets.html#load-data",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "11.13 Load data",
    "text": "11.13 Load data\nWe’re going to load up and do a bit of initial prep on our data, much as we did before for the Logistic Regression. We’re going to load our data (which is stored in a .csv file) into a Pandas DataFrame. We’ll convert all the data into floating point numbers so everything is consistent. We’ll drop the Passenger ID column, as that isn’t part of the original data, and we don’t want the machine to learn anything from this.\nThen we define our input (X) and output (y) data. Remember we’re trying to predict y from X. X is all of our columns (features) except for the “Survived” column (which is our label - the thing we’re trying to predict). The axis=1 argument tells Pandas we’re referring to columns when we tell it to drop stuff.\nWe also set up NumPy versions of our X and y data - this is a necessary step if we were going to do k-fold splits (remember we talked about those in the last session - it’s where we split up our data into training and test sets in multiple different ways to try to avoid biasing the data) as it requires the data to be in NumPy arrays, not Pandas DataFrames. We’re not actually going to use k-fold splits in this workbook, but we’ll still go through the step of getting the data into the right format for when we do. Because, in real world applications, you should use k-fold splits.\n\ndata = pd.read_csv('data/processed_data.csv')\n# Make all data 'float' type\ndata = data.astype(float)\ndata.drop('PassengerId', inplace=True, axis=1)\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'\n# Convert to NumPy as required for k-fold splits\nX_np = X.values\ny_np = y.values",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#show-summary-of-the-model-structure",
    "href": "4f_HSMA_neural_nets.html#show-summary-of-the-model-structure",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "12.1 Show summary of the model structure",
    "text": "12.1 Show summary of the model structure\nHere we will create an arbitrary model (that we won’t use) with 10 input features, just to show the function we wrote above being used and so you can see how you can use the summary() function of a model to see an overview of the structure of it.\nWe can see what the layers are in order. Remember we have five main layers in total (input, 3 x hidden, output) but you won’t see the input layer here. When you run the below cell, you should see three hidden layers (each with a dropout layer immediately after) with 128 neurons in each layer followed by a final output layer with just one neuron. You’ll also see that there are over 34,500 parameters (weights) that it needs to optimise, just in a very simple network like this on a very small dataset with 10 features. Now you can see why they’re so complicated (and magical!).\n\nmodel = make_net(10)\nmodel.summary()\n\nWARNING:tensorflow:From c:\\Users\\Sammi\\Anaconda3\\envs\\ml_sammi\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 128)            │         1,408 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 34,561 (135.00 KB)\n\n\n\n Trainable params: 34,561 (135.00 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#split-and-scale-data",
    "href": "4f_HSMA_neural_nets.html#split-and-scale-data",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "12.2 Split and Scale data",
    "text": "12.2 Split and Scale data\nNow, as we did before with the Logistic Regression, we split our data into training and test sets. We’ve got 25% carved off for our test set. But what is this random_state=42 thing? So, pulling back the curtain now, but the random numbers we tend to generate in our computers are not strictly random. They are pseudo-random - they use complex algorithms to generate numbers that appear random (and which are good enough for the vast majority of things you will ever do). Because they are pseudo-random, this means that we can fix the random number generator to use a pre-defined seed - a number that feeds into the algorithm which will ensure we always get the same random numbers being generated. This can be useful if we’re 1) teaching, and you want everyone to get the same thing, or 2) validating our outputs whilst we build our model. Since we’re doing both of those things here, we use a fixed seed.\nBut why the number 42? Those of you who have read, watched and / or listened to The Hitchiker’s Guide to the Galaxy will know why. Those that haven’t, go off and read, watch or listen to it and then you’ll get the “joke” (Computer Scientists love doing stuff like this..)\nOnce we’ve established our training and testing data, we scale the data by normalising it, using the function we wrote earlier (which uses min-max normalisation).\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_np, y_np, test_size = 0.25, random_state=42)\n\n# Scale X data\nX_train_sc, X_test_sc = scale_data(X_train, X_test)\n\nLet’s just have a look at the scaled data for the first two records (passengers) in our input data. We should see that all of the feature values have scaled between 0 and 1.\n\nX_train_sc[0:2]\n\narray([[0.        , 0.34656949, 0.        , 0.        , 0.05953204,\n        1.        , 0.        , 0.        , 0.828125  , 0.        ,\n        1.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        , 1.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        ],\n       [1.        , 0.30887158, 0.        , 0.        , 0.01376068,\n        0.        , 0.        , 1.        , 0.        , 1.        ,\n        1.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 1.        ]])\n\n\nWe can compare this with the unscaled data for the same two passengers to see the original values.\n\nX_train[0:2]\n\narray([[  1.  ,  28.  ,   0.  ,   0.  ,  30.5 ,   1.  ,   0.  ,   0.  ,\n        106.  ,   0.  ,   1.  ,   0.  ,   0.  ,   1.  ,   0.  ,   0.  ,\n          0.  ,   1.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n       [  3.  ,  25.  ,   0.  ,   0.  ,   7.05,   0.  ,   0.  ,   1.  ,\n          0.  ,   1.  ,   1.  ,   0.  ,   0.  ,   1.  ,   0.  ,   0.  ,\n          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   1.  ]])",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#function-to-calculate-accuracy",
    "href": "4f_HSMA_neural_nets.html#function-to-calculate-accuracy",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "12.3 Function to calculate accuracy",
    "text": "12.3 Function to calculate accuracy\nWe’re now going to write a little function that will report the accuracy of the model on the training set and the test set. This will help us assess how well our model is performing. We pass into the function the model, the (normalised) input data for both the training and test sets, and the output data for both the training and test sets.\nThe function uses the predict function of the model to grab out the probability predictions based on the input data for the training set. We specify that a classification of 1 (in the case of Titanic, this means “survived”) should be made if the probability predicted is greater than 0.5. Then we “flatten” the data to get it in the right shape (because it comes out as a complex shape - a tensor. Don’t worry about this. Just imagine a blob of data, and we squish it so we can read it). Then we use y_pred_train == y_train to return boolean True values for each time where the prediction (survived or died) matched the real answer, and take the average of those matches (that effectively gives us accuracy - what proportion of times did prediction match real answer). (Python interprets Trues and Falses as 1s and 0s, in case you’re wondering how that works!).\nThen we do the same as above but for the test set.\nFinally we print the accuracy on both the training and test sets.\n\ndef calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"\n\n    ### Get accuracy of fit to training data\n    probability = model.predict(X_train_sc)\n    y_pred_train = probability &gt;= 0.5\n    y_pred_train = y_pred_train.flatten()\n    accuracy_train = np.mean(y_pred_train == y_train)\n\n    ### Get accuracy of fit to test data\n    probability = model.predict(X_test_sc)\n    y_pred_test = probability &gt;= 0.5\n    y_pred_test = y_pred_test.flatten()\n    accuracy_test = np.mean(y_pred_test == y_test)\n\n    # Show acuracy\n    print (f'Training accuracy {accuracy_train:0.3f}')\n    print (f'Test accuracy {accuracy_test:0.3f}')\n\nWe’ll also write a little function to plot the accuracy on the training set and the test set over time. Keras keeps a “history” (which is a dictionary) of the learning which allows us to do this easily. It’s quite useful to plot the performance over time, as it allows us to look for indications as to when the model is becoming overfitted etc.\nIn our function, we’ll grab out the values from the passed in history dictionary, and then plot them using standard matplotlib plotting methods.\n\ndef plot_training(history_dict):\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n    epochs = range(1, len(acc_values) + 1)\n\n    fig, ax = plt.subplots()\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Accuracy')\n\n    ax.plot(epochs, acc_values, color='blue', label='Training acc')\n    ax.plot(epochs, val_acc_values, color='red', label='Test accuracy')\n    ax.set_title('Training and validation accuracy')\n\n    ax.legend()\n\n    fig.show()",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#run-the-model",
    "href": "4f_HSMA_neural_nets.html#run-the-model",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "12.4 Run the model",
    "text": "12.4 Run the model\nWe’ve now defined everything that will allow us to build the model. So we’ll now define the model we want and train it!\nTo work out how many features we need (which we then need to pass into the make_net function we defined earlier), we can simply look at the number of columns in our X (input) data (where we’ve removed the ‘label’ (output) column). We can grab this from the standardised training data, by looking at index 1 of the shape tuple (index 0 would be rows (passengers in the Titanic data), and index 1 would be columns). We can see this if we run the code X_train_sc.shape. Try it yourself (just insert a code cell below this markdown cell)! You should see there are 668 rows, and 24 columns. Therefore, we’ve got 668 passengers and 24 features.\nNext we call our make_net function, passing in the number of features we calculated above. This will create our Neural Network. As we’ve passed in nothing else, we’ll have defaults for the rest of the network - 3 hidden layers, 128 neurons per layer, a learning rate of 0.003 and no dropout (although, we will still have dropput layers, they just won’t do anything).\nThen, we fit (train) the model. To do that, we call the fit method of the model, and pass it :\n\nthe standardised training data\nthe output (label) data\nthe number of epochs (training generations - full passes of all of the data through the network). Initially, we want enough epochs that we see overfitting start to happen (the training accuracy starts to plateau) because then we know we’ve trained “enough” (albeit a bit too much) and can then look to reduce it back a bit\nthe batch size (how much data we shunt through the network at once. Yann LeCun (French Computer Scientist) advises “Friends shouldn’t let friends use batch sizes of more than 32”. But we will here… :))\nthe data we want to use as our “validation data” (which we use to fine tune the parameters of the model). Keras will check performance on this validation data. Here we just use our test set, but you should really have a separate “validation set” that you’d use whilst tuning the model.\nwhether we want to see all the things it’s doing as it’s learning. If we set verbose to 0, all of this will be hidden (keeping things tidier), but as we’re experimenting with our model, it’s a good idea to set verbose to 1 so we can monitor what it’s doing.\n\nYou’ll also see that we not only call model.fit but we store the output of that function in a variable called history. This allows us to access all the useful information that keras was keeping track of whilst the model was training. We’ll use that later.\nNote - when you run the cell below, the model will be built and then start training. How long this takes will depend on your computer specs, including whether you have a CUDA-enabled GPU (if you’re running locally) or your priority in the queue for cloud computing (if you’re running this on CoLab).\nDan has a very fast computer with a high performance CUDA-enabled GPU, and the below (with 250 epochs) takes about 6 seconds on the GPU and about 11 seconds on the CPU. It might take a little while longer on yours - don’t worry, as long as you can see it moving through the epochs.\nFor each epoch, you’ll see various information, including the epoch number, the loss (error) that’s been calculated in that epoch (for both the training and validation data), and the accuracy (for both the training and validation data). You should see loss gradually reduce, and accuracy gradually increase. But you’ll likely see that training accuracy tends to keep getting better (before it reaches a plateau) and validation accuracy gets better but then starts to drop a bit. That’s a sign of overfitting (our model’s become increasingly brilliant for the training data, but starting to get increasingly rubbish at being more generally useful).\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1)\n\nEpoch 1/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 1s 15ms/step - accuracy: 0.5894 - loss: 0.6207 - val_accuracy: 0.7489 - val_loss: 0.5343\nEpoch 2/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.7645 - loss: 0.5116 - val_accuracy: 0.7803 - val_loss: 0.4798\nEpoch 3/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7654 - loss: 0.4894 - val_accuracy: 0.8117 - val_loss: 0.4555\nEpoch 4/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8346 - loss: 0.4073 - val_accuracy: 0.8027 - val_loss: 0.4448\nEpoch 5/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8469 - loss: 0.4044 - val_accuracy: 0.7758 - val_loss: 0.5106\nEpoch 6/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8175 - loss: 0.4148 - val_accuracy: 0.8117 - val_loss: 0.4318\nEpoch 7/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8279 - loss: 0.3935 - val_accuracy: 0.8027 - val_loss: 0.4375\nEpoch 8/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8366 - loss: 0.4046 - val_accuracy: 0.8027 - val_loss: 0.4640\nEpoch 9/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8236 - loss: 0.4184 - val_accuracy: 0.8072 - val_loss: 0.4712\nEpoch 10/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8344 - loss: 0.3743 - val_accuracy: 0.8072 - val_loss: 0.4901\nEpoch 11/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8338 - loss: 0.3994 - val_accuracy: 0.8072 - val_loss: 0.4832\nEpoch 12/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8273 - loss: 0.3678 - val_accuracy: 0.8206 - val_loss: 0.4622\nEpoch 13/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8438 - loss: 0.3705 - val_accuracy: 0.7937 - val_loss: 0.5043\nEpoch 14/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8478 - loss: 0.3726 - val_accuracy: 0.7982 - val_loss: 0.5243\nEpoch 15/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8387 - loss: 0.3699 - val_accuracy: 0.8251 - val_loss: 0.5106\nEpoch 16/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8551 - loss: 0.3550 - val_accuracy: 0.7578 - val_loss: 0.4942\nEpoch 17/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8415 - loss: 0.3626 - val_accuracy: 0.7848 - val_loss: 0.5928\nEpoch 18/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8400 - loss: 0.3567 - val_accuracy: 0.8161 - val_loss: 0.4823\nEpoch 19/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8661 - loss: 0.3454 - val_accuracy: 0.8117 - val_loss: 0.5576\nEpoch 20/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8629 - loss: 0.3278 - val_accuracy: 0.8206 - val_loss: 0.5223\nEpoch 21/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8526 - loss: 0.3550 - val_accuracy: 0.8117 - val_loss: 0.5888\nEpoch 22/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8647 - loss: 0.3301 - val_accuracy: 0.8206 - val_loss: 0.5603\nEpoch 23/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8622 - loss: 0.3260 - val_accuracy: 0.8072 - val_loss: 0.6828\nEpoch 24/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8717 - loss: 0.3087 - val_accuracy: 0.8251 - val_loss: 0.5943\nEpoch 25/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8460 - loss: 0.3446 - val_accuracy: 0.8027 - val_loss: 0.6073\nEpoch 26/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8601 - loss: 0.3467 - val_accuracy: 0.8161 - val_loss: 0.6106\nEpoch 27/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8680 - loss: 0.3236 - val_accuracy: 0.8072 - val_loss: 0.5850\nEpoch 28/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8689 - loss: 0.3169 - val_accuracy: 0.7803 - val_loss: 0.7162\nEpoch 29/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8634 - loss: 0.3264 - val_accuracy: 0.8161 - val_loss: 0.6224\nEpoch 30/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8903 - loss: 0.2943 - val_accuracy: 0.8117 - val_loss: 0.6422\nEpoch 31/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9017 - loss: 0.2637 - val_accuracy: 0.7892 - val_loss: 0.7711\nEpoch 32/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8693 - loss: 0.3187 - val_accuracy: 0.7848 - val_loss: 0.6392\nEpoch 33/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8680 - loss: 0.3148 - val_accuracy: 0.8072 - val_loss: 0.6474\nEpoch 34/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8742 - loss: 0.3211 - val_accuracy: 0.8117 - val_loss: 0.7169\nEpoch 35/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8721 - loss: 0.3133 - val_accuracy: 0.8251 - val_loss: 0.7245\nEpoch 36/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8893 - loss: 0.3021 - val_accuracy: 0.7982 - val_loss: 0.7685\nEpoch 37/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9001 - loss: 0.2708 - val_accuracy: 0.8027 - val_loss: 0.8393\nEpoch 38/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8821 - loss: 0.2675 - val_accuracy: 0.7982 - val_loss: 0.7583\nEpoch 39/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8499 - loss: 0.3350 - val_accuracy: 0.7982 - val_loss: 0.9216\nEpoch 40/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8934 - loss: 0.2763 - val_accuracy: 0.7803 - val_loss: 0.7980\nEpoch 41/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8644 - loss: 0.3112 - val_accuracy: 0.8027 - val_loss: 0.8078\nEpoch 42/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8864 - loss: 0.2734 - val_accuracy: 0.7982 - val_loss: 0.9689\nEpoch 43/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8827 - loss: 0.3064 - val_accuracy: 0.8027 - val_loss: 0.8839\nEpoch 44/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9008 - loss: 0.2696 - val_accuracy: 0.8027 - val_loss: 0.8412\nEpoch 45/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8761 - loss: 0.3005 - val_accuracy: 0.7892 - val_loss: 1.1601\nEpoch 46/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8918 - loss: 0.3082 - val_accuracy: 0.8072 - val_loss: 0.7309\nEpoch 47/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8803 - loss: 0.2903 - val_accuracy: 0.7758 - val_loss: 0.7280\nEpoch 48/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8889 - loss: 0.2922 - val_accuracy: 0.7982 - val_loss: 0.7266\nEpoch 49/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8941 - loss: 0.2913 - val_accuracy: 0.7848 - val_loss: 0.8113\nEpoch 50/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8766 - loss: 0.2874 - val_accuracy: 0.8117 - val_loss: 0.7758\nEpoch 51/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8967 - loss: 0.2838 - val_accuracy: 0.7803 - val_loss: 0.8528\nEpoch 52/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8900 - loss: 0.2808 - val_accuracy: 0.7937 - val_loss: 0.8781\nEpoch 53/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8819 - loss: 0.2898 - val_accuracy: 0.8027 - val_loss: 0.9223\nEpoch 54/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8991 - loss: 0.2740 - val_accuracy: 0.8027 - val_loss: 0.9023\nEpoch 55/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8974 - loss: 0.2591 - val_accuracy: 0.7848 - val_loss: 1.0537\nEpoch 56/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8879 - loss: 0.2733 - val_accuracy: 0.8027 - val_loss: 0.9573\nEpoch 57/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9012 - loss: 0.2500 - val_accuracy: 0.7982 - val_loss: 0.9256\nEpoch 58/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8798 - loss: 0.2910 - val_accuracy: 0.7982 - val_loss: 0.9879\nEpoch 59/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9055 - loss: 0.2531 - val_accuracy: 0.7848 - val_loss: 1.0956\nEpoch 60/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8856 - loss: 0.2910 - val_accuracy: 0.8027 - val_loss: 0.9856\nEpoch 61/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8928 - loss: 0.2812 - val_accuracy: 0.8117 - val_loss: 0.9860\nEpoch 62/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8784 - loss: 0.2847 - val_accuracy: 0.7892 - val_loss: 1.0311\nEpoch 63/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8993 - loss: 0.2482 - val_accuracy: 0.8161 - val_loss: 1.0434\nEpoch 64/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8928 - loss: 0.2728 - val_accuracy: 0.7892 - val_loss: 0.9807\nEpoch 65/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9025 - loss: 0.2681 - val_accuracy: 0.8027 - val_loss: 1.0414\nEpoch 66/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8994 - loss: 0.2493 - val_accuracy: 0.8117 - val_loss: 1.0447\nEpoch 67/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8953 - loss: 0.2612 - val_accuracy: 0.8027 - val_loss: 1.0741\nEpoch 68/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9051 - loss: 0.2460 - val_accuracy: 0.8161 - val_loss: 1.1610\nEpoch 69/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8952 - loss: 0.2547 - val_accuracy: 0.8027 - val_loss: 1.1435\nEpoch 70/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8987 - loss: 0.2439 - val_accuracy: 0.8206 - val_loss: 1.1246\nEpoch 71/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9031 - loss: 0.2437 - val_accuracy: 0.8072 - val_loss: 1.1499\nEpoch 72/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8842 - loss: 0.2726 - val_accuracy: 0.8117 - val_loss: 1.1043\nEpoch 73/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8934 - loss: 0.2641 - val_accuracy: 0.8027 - val_loss: 1.1882\nEpoch 74/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8886 - loss: 0.2595 - val_accuracy: 0.7937 - val_loss: 1.1686\nEpoch 75/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8918 - loss: 0.2583 - val_accuracy: 0.8027 - val_loss: 1.1956\nEpoch 76/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9068 - loss: 0.2362 - val_accuracy: 0.8117 - val_loss: 1.1993\nEpoch 77/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8918 - loss: 0.2342 - val_accuracy: 0.8251 - val_loss: 1.2751\nEpoch 78/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9055 - loss: 0.2424 - val_accuracy: 0.7937 - val_loss: 1.3184\nEpoch 79/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8898 - loss: 0.2682 - val_accuracy: 0.8161 - val_loss: 1.2269\nEpoch 80/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8871 - loss: 0.2632 - val_accuracy: 0.8117 - val_loss: 1.2805\nEpoch 81/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8793 - loss: 0.2542 - val_accuracy: 0.8206 - val_loss: 1.2283\nEpoch 82/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9033 - loss: 0.2421 - val_accuracy: 0.7713 - val_loss: 1.5658\nEpoch 83/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8944 - loss: 0.2464 - val_accuracy: 0.8117 - val_loss: 1.3082\nEpoch 84/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8859 - loss: 0.2661 - val_accuracy: 0.8117 - val_loss: 1.3584\nEpoch 85/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9031 - loss: 0.2413 - val_accuracy: 0.8117 - val_loss: 1.1927\nEpoch 86/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9046 - loss: 0.2347 - val_accuracy: 0.7848 - val_loss: 1.6775\nEpoch 87/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8836 - loss: 0.2844 - val_accuracy: 0.8027 - val_loss: 1.4149\nEpoch 88/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8927 - loss: 0.2857 - val_accuracy: 0.8206 - val_loss: 0.9820\nEpoch 89/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8876 - loss: 0.2678 - val_accuracy: 0.7937 - val_loss: 1.1708\nEpoch 90/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8873 - loss: 0.3120 - val_accuracy: 0.8117 - val_loss: 1.0113\nEpoch 91/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9057 - loss: 0.2408 - val_accuracy: 0.7937 - val_loss: 1.0703\nEpoch 92/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9058 - loss: 0.2399 - val_accuracy: 0.8027 - val_loss: 1.0774\nEpoch 93/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9008 - loss: 0.2366 - val_accuracy: 0.8027 - val_loss: 1.0887\nEpoch 94/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8977 - loss: 0.2540 - val_accuracy: 0.7848 - val_loss: 1.2006\nEpoch 95/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8891 - loss: 0.2580 - val_accuracy: 0.7937 - val_loss: 1.1843\nEpoch 96/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9007 - loss: 0.2438 - val_accuracy: 0.8161 - val_loss: 1.0627\nEpoch 97/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8920 - loss: 0.2779 - val_accuracy: 0.8072 - val_loss: 1.1149\nEpoch 98/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9096 - loss: 0.2227 - val_accuracy: 0.8027 - val_loss: 1.1924\nEpoch 99/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9080 - loss: 0.2331 - val_accuracy: 0.7937 - val_loss: 1.1596\nEpoch 100/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9054 - loss: 0.2379 - val_accuracy: 0.8027 - val_loss: 1.2739\nEpoch 101/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9096 - loss: 0.2241 - val_accuracy: 0.8117 - val_loss: 1.2690\nEpoch 102/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9056 - loss: 0.2272 - val_accuracy: 0.8161 - val_loss: 1.2559\nEpoch 103/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8976 - loss: 0.2425 - val_accuracy: 0.8161 - val_loss: 1.2372\nEpoch 104/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9137 - loss: 0.2138 - val_accuracy: 0.8027 - val_loss: 1.3317\nEpoch 105/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8869 - loss: 0.2747 - val_accuracy: 0.8072 - val_loss: 1.3751\nEpoch 106/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9027 - loss: 0.2292 - val_accuracy: 0.7982 - val_loss: 1.3915\nEpoch 107/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9002 - loss: 0.2418 - val_accuracy: 0.7982 - val_loss: 1.3141\nEpoch 108/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9095 - loss: 0.2260 - val_accuracy: 0.8117 - val_loss: 1.3647\nEpoch 109/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9018 - loss: 0.2362 - val_accuracy: 0.8027 - val_loss: 1.3624\nEpoch 110/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9090 - loss: 0.2283 - val_accuracy: 0.8027 - val_loss: 1.3470\nEpoch 111/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9253 - loss: 0.1949 - val_accuracy: 0.8072 - val_loss: 1.4330\nEpoch 112/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8980 - loss: 0.2377 - val_accuracy: 0.8027 - val_loss: 1.4227\nEpoch 113/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9182 - loss: 0.2017 - val_accuracy: 0.7982 - val_loss: 1.4266\nEpoch 114/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9243 - loss: 0.1946 - val_accuracy: 0.8072 - val_loss: 1.4049\nEpoch 115/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9201 - loss: 0.2086 - val_accuracy: 0.8117 - val_loss: 1.5301\nEpoch 116/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9081 - loss: 0.2193 - val_accuracy: 0.8072 - val_loss: 1.4580\nEpoch 117/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9265 - loss: 0.1897 - val_accuracy: 0.8117 - val_loss: 1.6142\nEpoch 118/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9002 - loss: 0.2341 - val_accuracy: 0.8117 - val_loss: 1.4237\nEpoch 119/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9007 - loss: 0.2468 - val_accuracy: 0.8072 - val_loss: 1.4527\nEpoch 120/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9050 - loss: 0.2552 - val_accuracy: 0.7982 - val_loss: 1.4548\nEpoch 121/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9180 - loss: 0.2204 - val_accuracy: 0.8072 - val_loss: 1.5674\nEpoch 122/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8948 - loss: 0.2526 - val_accuracy: 0.8206 - val_loss: 1.4663\nEpoch 123/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9069 - loss: 0.2285 - val_accuracy: 0.7982 - val_loss: 1.6611\nEpoch 124/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9029 - loss: 0.2316 - val_accuracy: 0.8027 - val_loss: 1.4079\nEpoch 125/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9042 - loss: 0.2263 - val_accuracy: 0.8027 - val_loss: 1.5330\nEpoch 126/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9162 - loss: 0.2163 - val_accuracy: 0.8296 - val_loss: 1.5034\nEpoch 127/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9054 - loss: 0.2326 - val_accuracy: 0.8161 - val_loss: 1.4424\nEpoch 128/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9115 - loss: 0.2303 - val_accuracy: 0.7848 - val_loss: 1.5685\nEpoch 129/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9084 - loss: 0.2286 - val_accuracy: 0.8161 - val_loss: 1.5663\nEpoch 130/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9012 - loss: 0.2440 - val_accuracy: 0.8251 - val_loss: 1.6786\nEpoch 131/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9033 - loss: 0.2283 - val_accuracy: 0.8027 - val_loss: 1.6477\nEpoch 132/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9100 - loss: 0.2287 - val_accuracy: 0.8027 - val_loss: 1.5885\nEpoch 133/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9024 - loss: 0.2255 - val_accuracy: 0.8072 - val_loss: 1.5587\nEpoch 134/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9128 - loss: 0.2038 - val_accuracy: 0.7803 - val_loss: 1.7733\nEpoch 135/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9135 - loss: 0.2249 - val_accuracy: 0.8206 - val_loss: 1.4879\nEpoch 136/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8921 - loss: 0.2508 - val_accuracy: 0.8161 - val_loss: 1.5537\nEpoch 137/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9112 - loss: 0.2282 - val_accuracy: 0.7982 - val_loss: 1.7163\nEpoch 138/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9134 - loss: 0.1975 - val_accuracy: 0.8072 - val_loss: 1.6882\nEpoch 139/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9144 - loss: 0.2140 - val_accuracy: 0.8027 - val_loss: 1.7327\nEpoch 140/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9187 - loss: 0.1955 - val_accuracy: 0.8161 - val_loss: 1.6430\nEpoch 141/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9207 - loss: 0.2046 - val_accuracy: 0.8117 - val_loss: 1.6671\nEpoch 142/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9009 - loss: 0.2251 - val_accuracy: 0.7937 - val_loss: 1.7145\nEpoch 143/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9103 - loss: 0.2087 - val_accuracy: 0.8117 - val_loss: 1.7463\nEpoch 144/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9042 - loss: 0.2122 - val_accuracy: 0.8161 - val_loss: 1.6424\nEpoch 145/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9137 - loss: 0.2045 - val_accuracy: 0.8027 - val_loss: 1.8534\nEpoch 146/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9114 - loss: 0.2300 - val_accuracy: 0.8296 - val_loss: 1.6790\nEpoch 147/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9051 - loss: 0.2112 - val_accuracy: 0.8072 - val_loss: 1.7015\nEpoch 148/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8965 - loss: 0.2250 - val_accuracy: 0.8206 - val_loss: 1.6936\nEpoch 149/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9083 - loss: 0.2216 - val_accuracy: 0.8206 - val_loss: 1.7359\nEpoch 150/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9207 - loss: 0.2102 - val_accuracy: 0.8251 - val_loss: 1.7592\nEpoch 151/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9028 - loss: 0.2250 - val_accuracy: 0.7982 - val_loss: 1.8319\nEpoch 152/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9255 - loss: 0.1886 - val_accuracy: 0.8117 - val_loss: 1.8389\nEpoch 153/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9227 - loss: 0.1878 - val_accuracy: 0.8117 - val_loss: 1.8696\nEpoch 154/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9283 - loss: 0.1935 - val_accuracy: 0.8117 - val_loss: 1.8943\nEpoch 155/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9197 - loss: 0.2009 - val_accuracy: 0.8117 - val_loss: 1.8556\nEpoch 156/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9152 - loss: 0.1983 - val_accuracy: 0.8341 - val_loss: 1.8383\nEpoch 157/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9130 - loss: 0.2079 - val_accuracy: 0.7892 - val_loss: 1.9779\nEpoch 158/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9167 - loss: 0.1999 - val_accuracy: 0.8161 - val_loss: 1.9083\nEpoch 159/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9226 - loss: 0.1932 - val_accuracy: 0.8072 - val_loss: 1.9361\nEpoch 160/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9228 - loss: 0.1930 - val_accuracy: 0.8161 - val_loss: 1.8877\nEpoch 161/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9087 - loss: 0.2185 - val_accuracy: 0.8206 - val_loss: 1.8714\nEpoch 162/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9219 - loss: 0.2077 - val_accuracy: 0.7982 - val_loss: 1.8634\nEpoch 163/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9236 - loss: 0.1848 - val_accuracy: 0.8206 - val_loss: 1.9356\nEpoch 164/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9014 - loss: 0.2278 - val_accuracy: 0.8251 - val_loss: 1.9316\nEpoch 165/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9297 - loss: 0.1821 - val_accuracy: 0.8117 - val_loss: 2.0098\nEpoch 166/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9080 - loss: 0.2141 - val_accuracy: 0.8296 - val_loss: 1.9840\nEpoch 167/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9241 - loss: 0.2003 - val_accuracy: 0.8117 - val_loss: 1.9231\nEpoch 168/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9093 - loss: 0.2033 - val_accuracy: 0.8161 - val_loss: 1.9485\nEpoch 169/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9049 - loss: 0.2183 - val_accuracy: 0.8251 - val_loss: 1.9557\nEpoch 170/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9057 - loss: 0.2173 - val_accuracy: 0.8117 - val_loss: 1.9372\nEpoch 171/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9197 - loss: 0.1989 - val_accuracy: 0.8206 - val_loss: 1.9695\nEpoch 172/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9055 - loss: 0.2403 - val_accuracy: 0.8117 - val_loss: 1.9416\nEpoch 173/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9126 - loss: 0.2024 - val_accuracy: 0.8161 - val_loss: 2.0270\nEpoch 174/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9190 - loss: 0.2114 - val_accuracy: 0.8117 - val_loss: 2.0630\nEpoch 175/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9069 - loss: 0.2111 - val_accuracy: 0.8206 - val_loss: 2.1022\nEpoch 176/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9156 - loss: 0.2135 - val_accuracy: 0.7982 - val_loss: 2.1598\nEpoch 177/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9094 - loss: 0.2158 - val_accuracy: 0.8206 - val_loss: 2.1567\nEpoch 178/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9122 - loss: 0.2049 - val_accuracy: 0.8072 - val_loss: 2.1527\nEpoch 179/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8964 - loss: 0.2311 - val_accuracy: 0.8072 - val_loss: 2.1787\nEpoch 180/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9075 - loss: 0.2177 - val_accuracy: 0.8027 - val_loss: 2.2352\nEpoch 181/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9073 - loss: 0.2131 - val_accuracy: 0.8117 - val_loss: 1.9330\nEpoch 182/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8977 - loss: 0.2284 - val_accuracy: 0.8117 - val_loss: 1.9407\nEpoch 183/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9147 - loss: 0.2004 - val_accuracy: 0.8296 - val_loss: 1.9101\nEpoch 184/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9151 - loss: 0.2007 - val_accuracy: 0.8117 - val_loss: 2.0112\nEpoch 185/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9225 - loss: 0.1961 - val_accuracy: 0.8206 - val_loss: 2.0660\nEpoch 186/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9253 - loss: 0.1922 - val_accuracy: 0.8117 - val_loss: 2.1485\nEpoch 187/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9210 - loss: 0.2142 - val_accuracy: 0.8206 - val_loss: 2.1587\nEpoch 188/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9134 - loss: 0.2054 - val_accuracy: 0.8161 - val_loss: 1.9631\nEpoch 189/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9137 - loss: 0.2075 - val_accuracy: 0.7982 - val_loss: 2.1524\nEpoch 190/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9022 - loss: 0.2224 - val_accuracy: 0.8027 - val_loss: 2.2020\nEpoch 191/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9206 - loss: 0.2107 - val_accuracy: 0.8117 - val_loss: 2.2168\nEpoch 192/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9074 - loss: 0.2091 - val_accuracy: 0.8117 - val_loss: 2.2054\nEpoch 193/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9178 - loss: 0.2138 - val_accuracy: 0.8251 - val_loss: 2.1201\nEpoch 194/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9078 - loss: 0.2185 - val_accuracy: 0.8206 - val_loss: 2.1451\nEpoch 195/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9156 - loss: 0.1962 - val_accuracy: 0.8206 - val_loss: 2.1810\nEpoch 196/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9142 - loss: 0.1864 - val_accuracy: 0.8072 - val_loss: 2.1932\nEpoch 197/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9102 - loss: 0.2077 - val_accuracy: 0.8206 - val_loss: 2.1306\nEpoch 198/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9122 - loss: 0.2230 - val_accuracy: 0.8027 - val_loss: 2.2611\nEpoch 199/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9082 - loss: 0.2241 - val_accuracy: 0.8027 - val_loss: 2.1842\nEpoch 200/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8993 - loss: 0.2270 - val_accuracy: 0.7982 - val_loss: 2.2389\nEpoch 201/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9177 - loss: 0.1908 - val_accuracy: 0.8161 - val_loss: 2.2516\nEpoch 202/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9275 - loss: 0.1922 - val_accuracy: 0.7982 - val_loss: 2.3478\nEpoch 203/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9113 - loss: 0.2057 - val_accuracy: 0.8251 - val_loss: 2.3609\nEpoch 204/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9170 - loss: 0.1957 - val_accuracy: 0.7982 - val_loss: 2.2785\nEpoch 205/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9049 - loss: 0.2291 - val_accuracy: 0.8251 - val_loss: 2.1180\nEpoch 206/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9051 - loss: 0.2010 - val_accuracy: 0.8161 - val_loss: 2.1736\nEpoch 207/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9332 - loss: 0.1805 - val_accuracy: 0.8206 - val_loss: 2.0803\nEpoch 208/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9142 - loss: 0.2080 - val_accuracy: 0.8072 - val_loss: 2.1005\nEpoch 209/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9203 - loss: 0.2045 - val_accuracy: 0.7892 - val_loss: 2.3324\nEpoch 210/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9069 - loss: 0.2234 - val_accuracy: 0.7848 - val_loss: 2.7736\nEpoch 211/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9192 - loss: 0.2368 - val_accuracy: 0.8027 - val_loss: 1.5454\nEpoch 212/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8978 - loss: 0.2573 - val_accuracy: 0.7803 - val_loss: 1.1517\nEpoch 213/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9071 - loss: 0.2551 - val_accuracy: 0.7758 - val_loss: 1.0374\nEpoch 214/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9016 - loss: 0.2485 - val_accuracy: 0.8072 - val_loss: 0.9112\nEpoch 215/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9007 - loss: 0.2578 - val_accuracy: 0.7982 - val_loss: 1.1066\nEpoch 216/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9042 - loss: 0.2257 - val_accuracy: 0.8072 - val_loss: 1.1030\nEpoch 217/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9149 - loss: 0.2167 - val_accuracy: 0.7848 - val_loss: 1.2617\nEpoch 218/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9233 - loss: 0.2147 - val_accuracy: 0.8251 - val_loss: 1.1500\nEpoch 219/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9081 - loss: 0.2371 - val_accuracy: 0.7937 - val_loss: 1.2920\nEpoch 220/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9214 - loss: 0.2137 - val_accuracy: 0.7982 - val_loss: 1.2232\nEpoch 221/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9008 - loss: 0.2419 - val_accuracy: 0.8161 - val_loss: 1.1188\nEpoch 222/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9204 - loss: 0.2116 - val_accuracy: 0.8072 - val_loss: 1.2259\nEpoch 223/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8947 - loss: 0.2215 - val_accuracy: 0.7982 - val_loss: 1.3546\nEpoch 224/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9186 - loss: 0.1976 - val_accuracy: 0.8117 - val_loss: 1.3808\nEpoch 225/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9148 - loss: 0.2067 - val_accuracy: 0.8206 - val_loss: 1.3904\nEpoch 226/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9233 - loss: 0.1872 - val_accuracy: 0.8161 - val_loss: 1.4109\nEpoch 227/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8977 - loss: 0.2221 - val_accuracy: 0.8072 - val_loss: 1.4569\nEpoch 228/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9299 - loss: 0.1682 - val_accuracy: 0.7937 - val_loss: 1.4599\nEpoch 229/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9222 - loss: 0.1951 - val_accuracy: 0.8161 - val_loss: 1.4171\nEpoch 230/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9279 - loss: 0.1891 - val_accuracy: 0.8206 - val_loss: 1.4438\nEpoch 231/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9115 - loss: 0.1979 - val_accuracy: 0.8117 - val_loss: 1.4886\nEpoch 232/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9193 - loss: 0.1959 - val_accuracy: 0.7982 - val_loss: 1.5116\nEpoch 233/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9065 - loss: 0.2083 - val_accuracy: 0.8072 - val_loss: 1.5837\nEpoch 234/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9154 - loss: 0.1979 - val_accuracy: 0.7937 - val_loss: 1.6042\nEpoch 235/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9243 - loss: 0.1860 - val_accuracy: 0.7982 - val_loss: 1.6524\nEpoch 236/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9287 - loss: 0.1729 - val_accuracy: 0.7982 - val_loss: 1.6007\nEpoch 237/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9362 - loss: 0.1780 - val_accuracy: 0.8072 - val_loss: 1.5806\nEpoch 238/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9251 - loss: 0.1722 - val_accuracy: 0.7892 - val_loss: 1.6355\nEpoch 239/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9195 - loss: 0.1850 - val_accuracy: 0.8161 - val_loss: 1.6360\nEpoch 240/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9123 - loss: 0.1994 - val_accuracy: 0.7937 - val_loss: 1.6234\nEpoch 241/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9266 - loss: 0.1822 - val_accuracy: 0.8072 - val_loss: 1.6250\nEpoch 242/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9271 - loss: 0.1808 - val_accuracy: 0.7937 - val_loss: 1.6264\nEpoch 243/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9291 - loss: 0.1882 - val_accuracy: 0.7982 - val_loss: 1.6230\nEpoch 244/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9061 - loss: 0.2033 - val_accuracy: 0.8161 - val_loss: 1.6164\nEpoch 245/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9224 - loss: 0.1934 - val_accuracy: 0.8027 - val_loss: 1.6767\nEpoch 246/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9153 - loss: 0.1981 - val_accuracy: 0.7937 - val_loss: 1.7148\nEpoch 247/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9193 - loss: 0.2082 - val_accuracy: 0.8161 - val_loss: 1.6781\nEpoch 248/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9271 - loss: 0.1777 - val_accuracy: 0.8117 - val_loss: 1.7112\nEpoch 249/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9212 - loss: 0.1772 - val_accuracy: 0.8161 - val_loss: 1.7084\nEpoch 250/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9130 - loss: 0.2118 - val_accuracy: 0.8072 - val_loss: 1.6142\n\n\nLet’s calculate and print the final accuracy scores for both the training and test (validation) data. Remember, we’ll call the function we wrote to do this earlier. You should see training accuracy is much better than test accuracy. We’ve overfitted. Don’t worry - we’ll try and improve that in a moment.\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nTraining accuracy 0.918\nTest accuracy 0.807\n\n\n\n12.4.1 Get training history\nhistory is a dictionary containing data collected during training. Remember - we stored it when we called the model.fit() method. Let’s take a look at the keys in this dictionary (these are the metrics monitored during training).\n\nhistory_dict = history.history\nhistory_dict.keys()\n\ndict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n\n\nWe see from the above that we have four keys in our history dictionary - loss, accuracy, validation loss and validation accuracy.\n\n\n12.4.2 Plot training history\nNow let’s plot our history data using the plotting function we wrote earlier.\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see from the plot above that the training accuracy gets better and better before reaching a plateau, but for the test data the accuracy initially improves, but then reduces a bit and plateaus at poorer performance. As we thought, we’ve overfitted. So let’s look at how we can now try to reduce the overfitting.",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#improving-fit-by-avoiding-or-reducing-over-fitting",
    "href": "4f_HSMA_neural_nets.html#improving-fit-by-avoiding-or-reducing-over-fitting",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "12.5 Improving fit by avoiding or reducing-over fitting",
    "text": "12.5 Improving fit by avoiding or reducing-over fitting\nIn the lecture, we discussed a number of strategies we can take to try to reduce overfitting. Let’s look at each in turn.\n\n12.5.1 1) Reduce complexity of model\nA simple initial strategy is to reduce the complexity of the model, so that the “dividing line” it learns becomes less complex (and less likely to be an overfit).\nHere, we create a new model where we reduce the number of hidden layers to 1 (from the default we used of 3), and we reduce the number of neurons on each hidden layer to 32 (from the default we used of 128).\nThen we fit (train) this new model, exactly as we did before. We’ll set verbose to 0 though, so we don’t see everything as it trains (if you’d rather see it, just change verbose to 1 below).\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                hidden_layers=1,\n                hidden_layer_neurones=32)\n\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)\n\nLet’s calculate, print and plot accuracy as we did before.\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nTraining accuracy 0.880\nTest accuracy 0.825\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see that the simplification of the model above has improved things a bit (though it may not, there’s randomness at play here, and your network may have learned differently) - training accuracy has reduced, but test accuracy (our measure of how generally useful our model will be beyond the training set) has improved - a little bit. But there’s still a bit of a gap between them - we’re still overfitting.\n\n\n12.5.2 2) Reduce training time\nFor the moment, let’s do one change at a time, so we’ll go back to our original model before trying our next strategy.\nAnother approach we can use is simply to stop training for so long. We can see from our earlier plots that things improve in the test set initially but then reduces. So, by not training for so long, we can stop training before it significantly overfits.\nHere, we’ll run the model exactly as we did the first time, except we’ll only run it for 25 epochs, rather than 250 - just 10% of the original training time.\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=25,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)\n\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 834us/step\nTraining accuracy 0.868\nTest accuracy 0.798\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see that reducing the training time has also led to an improvement in test accuracy, much as simplifying the model did, although you might not. You might find that this measure is slightly more effective than the simplifying measure. You should also see from the plot that the test set accuracy tends to plateau, and it doesn’t get to the bit where it starts dropping significantly.\n\n\n12.5.3 3) Add dropout\nUsing dropout, in each training epoch a random selection of weights are “switched off” (the selection changes from epoch to epoch). It does this by using the Dropout layers after each hidden layer (remember when we added those earlier?), and randomly switching some of the incoming weights to 0. When predicting (after fitting) all weights are used. Dropout ensures that, during training, the model can’t rely too much on any set of weights (because they’ll occasionally be turned off), and looks to explore them more globally.\nThis is probably the most common method for reducing overfitting. Dropout values of 0.2 to 0.5 are common.\nHere, we’ll use a dropout value of 0.5. So 50% of the weights coming out of each hidden layer will be set to 0 in each epoch.\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                dropout=0.5)\n\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)\n\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nTraining accuracy 0.883\nTest accuracy 0.803\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nAgain, we should see that Dropout has improved performance on the test set over the base case (although it might not).\n\n\n12.5.4 4) Combination of the above and with automatic early stopping\nRather than just doing one of these things above, we tend to combine these measures. We’ll also use a Keras callback called EarlyStopping to automate the measure where we try to stop the training sooner. A callback is simply a function that Keras can use to perform various actions continually throughout the training.\nEarlyStopping will automatically stop the training when it appears the validation accuracy isn’t getting any better. It allows us to specify a patience level, which is the number of epochs we are prepared to wait (to give it a chance to improve) before EarlyStopping cuts things off. We can also optionally specify the minimum level we want our metric(s) (e.g. accuracy) to improve between epochs to count as an “improvement” - this allows us to say that we don’t consider a very small improvement as significant enough. You’ll see examples of this later in the course, but here we’ll just specify patience, and we’ll allow any improvement to count as improvement.\nHere, we specify a patience of 25 epochs - this means that we are prepared to wait 25 epochs to see if we can get a better accuracy score on the validation set. By setting restore_best_weights=True we tell it that, once it stops (if it didn’t manage to improve things in 25 epochs), then it should roll back the network to how it was when it reached its peak performance.\nSo, here we set up our EarlyStopping callback. Then we define a simpler network with 1 hidden layer and 64 neurons per layer, have a 50% dropout rate, and run for 250 epochs but add in the EarlyStopping callback so that Keras will stop the training when things stop improving in the validation set, and revert back to the best version it’s seen.\nIn the below, you’ll see we’ve also added another callback called ModelCheckpoint. This callback just automatically saves the model at its best point so we can easily retrieve it. In combination with EarlyStopping, this means we have a model that won’t keep going beyond when it should, and it’ll save the best version for later use.\nNote that as well as creating and defining the callbacks, you also need to ensure you add them into the list of inputs you pass in when you call model.fit.\n\n# Define save checkpoint callback (only save if new best validation results)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\n# Stop when no validation improvement for 25 epochs\n# Restore weights to best validation accuracy\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=25, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(\n    number_features,\n    hidden_layers=1,\n    hidden_layer_neurones=64,\n    dropout=0.5)\n\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\n\n# Show accuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nTraining accuracy 0.832\nTest accuracy 0.812\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see from the above, where we’ve combined the three anti-overfitting measures, that we get quite a decent improvement in test accuracy and a closing of the gap between training and test accuracy. This indicates that our model is far less overfitted than it was originally.",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_HSMA_neural_nets.html#saving-and-reloading-the-model",
    "href": "4f_HSMA_neural_nets.html#saving-and-reloading-the-model",
    "title": "11  Neural Networks with Tensorflow/Keras (Titanic Dataset)",
    "section": "12.6 Saving and reloading the model",
    "text": "12.6 Saving and reloading the model\nFinally, we’ll look at how we can save our models so we can come back to them another time, and we don’t have to retrain them each time. For a small model like this, it’s not hugely inconvenient, but if we had a large model (that could take hours or even days to run) we don’t want to have to retrain it every time we want to use it!\nHere, we can use the save() function of the model to easily save a model. We just pass in a filename - we use the new .keras file extension. The model will be saved in the present working directory for the code.\nYou can also see in the cell below how to load a model back in, and then use it again. You can verify this if you run the two cells below, which will save the model, then load it back up, and recalculate its accuracy - you should see that the reported training and test accuracies are the same as you had above (because that’s the model we saved and then loaded back up).\n\n# Save model\nmodel.save('titanic_tf_model.keras')\n\n# Load and use saved model - we need to first set up a model\nrestored_model = keras.models.load_model('titanic_tf_model.keras')\n\n# Predict classes as normal\npredicted_proba = restored_model.predict(X_test_sc)\n\n# Show examples of predicted probability\nprint(predicted_proba[0:5].flatten())\n\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \n[0.09658381 0.15201195 0.15485787 0.85435104 0.68767977]\n\n\n\ncalculate_accuracy(restored_model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nTraining accuracy 0.832\nTest accuracy 0.812",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Neural Networks with Tensorflow/Keras (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4f_ex_1_solution.html",
    "href": "4f_ex_1_solution.html",
    "title": "12  Exercise Solution: Neural Networks (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "The data loaded in this exercise is for seven acute stroke units, and whether a patient receives clost-busting treatment for stroke. There are lots of features, and a description of the features can be found in the file stroke_data_feature_descriptions.csv.\nTrain a Neural Network model to try to predict whether or not a stroke patient receives clot-busting treatment. Use the prompts below to write each section of code.\nHow accurate can you get your model on the test set?\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# sklearn for pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# TensorFlow sequential model\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Download data\n# (not required if running locally and have previously downloaded data)\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '2004_titanic/master/jupyter_notebooks/data/hsma_stroke.csv'\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data to data subfolder\n    data.to_csv(data_directory + 'hsma_stroke.csv', index=False)\n\nLook at an overview of the data using the describe() method of Pandas.\n\ndata.describe()\n\n\n\n\n\n\n\n\n\nClotbuster given\nHosp_1\nHosp_2\nHosp_3\nHosp_4\nHosp_5\nHosp_6\nHosp_7\nMale\nAge\n...\nS2NihssArrivalFacialPalsy\nS2NihssArrivalMotorArmLeft\nS2NihssArrivalMotorArmRight\nS2NihssArrivalMotorLegLeft\nS2NihssArrivalMotorLegRight\nS2NihssArrivalLimbAtaxia\nS2NihssArrivalSensory\nS2NihssArrivalBestLanguage\nS2NihssArrivalDysarthria\nS2NihssArrivalExtinctionInattention\n\n\n\n\ncount\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n...\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n1862.000000\n\n\nmean\n0.403330\n0.159506\n0.142320\n0.154672\n0.165414\n0.055854\n0.113319\n0.208915\n0.515575\n74.553706\n...\n1.114930\n1.002148\n0.963480\n0.963480\n0.910849\n0.216971\n0.610097\n0.944146\n0.739527\n0.566595\n\n\nstd\n0.490698\n0.366246\n0.349472\n0.361689\n0.371653\n0.229701\n0.317068\n0.406643\n0.499892\n12.280576\n...\n0.930527\n1.479211\n1.441594\n1.406501\n1.380606\n0.522643\n0.771932\n1.121379\n0.731083\n0.794000\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n40.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n67.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n76.000000\n...\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n83.000000\n...\n2.000000\n2.000000\n2.000000\n2.000000\n2.000000\n0.000000\n1.000000\n2.000000\n1.000000\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n100.000000\n...\n3.000000\n4.000000\n4.000000\n4.000000\n4.000000\n2.000000\n2.000000\n3.000000\n2.000000\n2.000000\n\n\n\n\n8 rows × 51 columns\n\n\n\n\nConvert all of the data in the dataframe to type float, pull out the X (feature) and y (label) data, and put X and y into Numpy arrays.\n\ndata = data.astype(float)\n\nX = data.drop('Clotbuster given', axis=1)\ny = data['Clotbuster given']\n\nX_np = X.values\ny_np = y.values\n\nDefine a function that will MinMax Normalise training and test feature data passed into it.\n\ndef scale_data(X_train, X_test):\n    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n\n    # Initialise a new scaling object for normalising input data\n    sc = MinMaxScaler()\n\n    # Apply the scaler to the training and test sets\n    train_sc = sc.fit_transform(X_train)\n    test_sc = sc.fit_transform(X_test)\n\n    return train_sc, test_sc\n\nDefine a function that will build a sequential neural network, given a number of features, a number of hidden layers (with a default of 5), a number of neurons per hidden layer (with a default of 64), a dropout rate (with a default of 0), and a learning rate (with a default of 0.003). The function should also create a single neuron output layer with a Sigmoid activation function, use an Adam optimiser, and a Binary Crossentropy loss function, with accuracy as the performance metric.\n\ndef make_net(number_features,\n             hidden_layers=5,\n             hidden_layer_neurones=64,\n             dropout=0.0,\n             learning_rate=0.003):\n\n    \"\"\"Make TensorFlow neural net\"\"\"\n\n    # Clear Tensorflow\n    K.clear_session()\n\n    # Set up neural net\n    net = Sequential()\n\n    # Add hidden hidden_layers using a loop\n    for i in range(hidden_layers):\n        # Add fully connected layer with ReLu activation\n        net.add(Dense(\n            hidden_layer_neurones,\n            input_dim=number_features,\n            activation='relu'))\n        # Add droput layer\n        net.add(Dropout(dropout))\n\n    # Add final sigmoid activation output\n    net.add(Dense(1, activation='sigmoid'))\n\n    # Compiling model\n    opt = Adam(learning_rate=learning_rate)\n\n    net.compile(loss='binary_crossentropy',\n                optimizer=opt,\n                metrics=['accuracy'])\n\n    return net\n\nSplit your data into training and test sets. Decide on an appropriate test data size. Then scale the feature data using MinMax Normalisation.\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_np, y_np, test_size = 0.25, random_state=42)\n\n# Scale X data\nX_train_sc, X_test_sc = scale_data(X_train, X_test)\n\nWrite a function to calculate accuracy of the model on both training and test sets.\n\ndef calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"\n\n    ### Get accuracy of fit to training data\n    probability = model.predict(X_train_sc)\n    y_pred_train = probability &gt;= 0.5\n    y_pred_train = y_pred_train.flatten()\n    accuracy_train = np.mean(y_pred_train == y_train)\n\n    ### Get accuracy of fit to test data\n    probability = model.predict(X_test_sc)\n    y_pred_test = probability &gt;= 0.5\n    y_pred_test = y_pred_test.flatten()\n    accuracy_test = np.mean(y_pred_test == y_test)\n\n    # Show acuracy\n    print (f'Training accuracy {accuracy_train:0.3f}')\n    print (f'Test accuracy {accuracy_test:0.3f}')\n\nWrite a function to plot training and test set accuracy over time during model fitting.\n\ndef plot_training(history_dict):\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n    epochs = range(1, len(acc_values) + 1)\n\n    fig, ax = plt.subplots()\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Accuracy')\n\n    ax.plot(epochs, acc_values, color='blue', label='Training acc')\n    ax.plot(epochs, val_acc_values, color='red', label='Test accuracy')\n    ax.set_title('Training and validation accuracy')\n\n    ax.legend()\n\n    fig.show()\n\nCreate a neural network with a number of hidden layers, neurons, dropout rate and learning rate of your choosing. Run the model for a number of epochs and with a batch size of your choosing (be careful about using large batch sizes unless you’ve got a CUDA-enabled GPU and TensorFlow is set up to use it).\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=100,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1)\n\nWARNING:tensorflow:From c:\\Users\\dan\\anaconda3\\envs\\tf_hsma\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:73: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n\nEpoch 1/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.6053 - loss: 0.6310 - val_accuracy: 0.7232 - val_loss: 0.5111\nEpoch 2/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7871 - loss: 0.4625 - val_accuracy: 0.7682 - val_loss: 0.4507\nEpoch 3/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 970us/step - accuracy: 0.8193 - loss: 0.4136 - val_accuracy: 0.8176 - val_loss: 0.4210\nEpoch 4/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 960us/step - accuracy: 0.8137 - loss: 0.3941 - val_accuracy: 0.8112 - val_loss: 0.3919\nEpoch 5/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8437 - loss: 0.3576 - val_accuracy: 0.8133 - val_loss: 0.4186\nEpoch 6/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8605 - loss: 0.3330 - val_accuracy: 0.7983 - val_loss: 0.4143\nEpoch 7/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 916us/step - accuracy: 0.8259 - loss: 0.3623 - val_accuracy: 0.8326 - val_loss: 0.3939\nEpoch 8/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8528 - loss: 0.3238 - val_accuracy: 0.8326 - val_loss: 0.4427\nEpoch 9/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 997us/step - accuracy: 0.8667 - loss: 0.2907 - val_accuracy: 0.7983 - val_loss: 0.4531\nEpoch 10/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8759 - loss: 0.2771 - val_accuracy: 0.8262 - val_loss: 0.5343\nEpoch 11/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 966us/step - accuracy: 0.8924 - loss: 0.2407 - val_accuracy: 0.8047 - val_loss: 0.5151\nEpoch 12/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 961us/step - accuracy: 0.8852 - loss: 0.2259 - val_accuracy: 0.8026 - val_loss: 0.5504\nEpoch 13/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 881us/step - accuracy: 0.9026 - loss: 0.2148 - val_accuracy: 0.8133 - val_loss: 0.6227\nEpoch 14/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 963us/step - accuracy: 0.9062 - loss: 0.1986 - val_accuracy: 0.8004 - val_loss: 0.8707\nEpoch 15/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 940us/step - accuracy: 0.9229 - loss: 0.1931 - val_accuracy: 0.8112 - val_loss: 0.7989\nEpoch 16/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 888us/step - accuracy: 0.9072 - loss: 0.1717 - val_accuracy: 0.7833 - val_loss: 0.6860\nEpoch 17/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 930us/step - accuracy: 0.9183 - loss: 0.1623 - val_accuracy: 0.7983 - val_loss: 0.8908\nEpoch 18/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 992us/step - accuracy: 0.9408 - loss: 0.1574 - val_accuracy: 0.7747 - val_loss: 0.6969\nEpoch 19/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 968us/step - accuracy: 0.9451 - loss: 0.1396 - val_accuracy: 0.7854 - val_loss: 0.8706\nEpoch 20/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 924us/step - accuracy: 0.9475 - loss: 0.1148 - val_accuracy: 0.7554 - val_loss: 1.1075\nEpoch 21/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9511 - loss: 0.1186 - val_accuracy: 0.7961 - val_loss: 0.6700\nEpoch 22/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9314 - loss: 0.1808 - val_accuracy: 0.7918 - val_loss: 0.7345\nEpoch 23/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9701 - loss: 0.0757 - val_accuracy: 0.7747 - val_loss: 1.1924\nEpoch 24/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9593 - loss: 0.0831 - val_accuracy: 0.7854 - val_loss: 1.0050\nEpoch 25/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9634 - loss: 0.1004 - val_accuracy: 0.7554 - val_loss: 1.0066\nEpoch 26/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9648 - loss: 0.0856 - val_accuracy: 0.7597 - val_loss: 1.0974\nEpoch 27/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 957us/step - accuracy: 0.9661 - loss: 0.0847 - val_accuracy: 0.7575 - val_loss: 1.1660\nEpoch 28/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9689 - loss: 0.0635 - val_accuracy: 0.7897 - val_loss: 1.2466\nEpoch 29/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 997us/step - accuracy: 0.9821 - loss: 0.0489 - val_accuracy: 0.7704 - val_loss: 1.1211\nEpoch 30/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9667 - loss: 0.0809 - val_accuracy: 0.8219 - val_loss: 1.1547\nEpoch 31/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 986us/step - accuracy: 0.9958 - loss: 0.0216 - val_accuracy: 0.7811 - val_loss: 1.6401\nEpoch 32/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 879us/step - accuracy: 0.9728 - loss: 0.0752 - val_accuracy: 0.7833 - val_loss: 1.2428\nEpoch 33/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9781 - loss: 0.0615 - val_accuracy: 0.7983 - val_loss: 1.0353\nEpoch 34/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9903 - loss: 0.0351 - val_accuracy: 0.7940 - val_loss: 1.3027\nEpoch 35/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9939 - loss: 0.0128 - val_accuracy: 0.7961 - val_loss: 1.6230\nEpoch 36/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9948 - loss: 0.0153 - val_accuracy: 0.7940 - val_loss: 1.5783\nEpoch 37/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9911 - loss: 0.0221 - val_accuracy: 0.7854 - val_loss: 1.6514\nEpoch 38/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 958us/step - accuracy: 0.9853 - loss: 0.0537 - val_accuracy: 0.7961 - val_loss: 1.1731\nEpoch 39/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9764 - loss: 0.0606 - val_accuracy: 0.7768 - val_loss: 1.3468\nEpoch 40/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9961 - loss: 0.0203 - val_accuracy: 0.7747 - val_loss: 1.4595\nEpoch 41/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9928 - loss: 0.0239 - val_accuracy: 0.7897 - val_loss: 1.5007\nEpoch 42/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9935 - loss: 0.0199 - val_accuracy: 0.7554 - val_loss: 1.5526\nEpoch 43/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9809 - loss: 0.0577 - val_accuracy: 0.7532 - val_loss: 1.4397\nEpoch 44/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9807 - loss: 0.0596 - val_accuracy: 0.7790 - val_loss: 1.0982\nEpoch 45/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9876 - loss: 0.0351 - val_accuracy: 0.7790 - val_loss: 1.3248\nEpoch 46/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 938us/step - accuracy: 0.9907 - loss: 0.0292 - val_accuracy: 0.7554 - val_loss: 1.5941\nEpoch 47/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 959us/step - accuracy: 0.9746 - loss: 0.0762 - val_accuracy: 0.7854 - val_loss: 1.1063\nEpoch 48/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9972 - loss: 0.0206 - val_accuracy: 0.7961 - val_loss: 1.4467\nEpoch 49/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9976 - loss: 0.0126 - val_accuracy: 0.8004 - val_loss: 1.6696\nEpoch 50/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9960 - loss: 0.0106 - val_accuracy: 0.7897 - val_loss: 1.7471\nEpoch 51/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9889 - loss: 0.0322 - val_accuracy: 0.7833 - val_loss: 1.3172\nEpoch 52/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 931us/step - accuracy: 0.9873 - loss: 0.0337 - val_accuracy: 0.7897 - val_loss: 1.3244\nEpoch 53/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 935us/step - accuracy: 0.9857 - loss: 0.0392 - val_accuracy: 0.8004 - val_loss: 1.2599\nEpoch 54/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9873 - loss: 0.0312 - val_accuracy: 0.7833 - val_loss: 1.4854\nEpoch 55/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9992 - loss: 0.0045 - val_accuracy: 0.7811 - val_loss: 1.6885\nEpoch 56/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 931us/step - accuracy: 0.9953 - loss: 0.0109 - val_accuracy: 0.7811 - val_loss: 1.5901\nEpoch 57/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 998us/step - accuracy: 0.9818 - loss: 0.0656 - val_accuracy: 0.7940 - val_loss: 1.1972\nEpoch 58/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 975us/step - accuracy: 0.9936 - loss: 0.0192 - val_accuracy: 0.7983 - val_loss: 1.4499\nEpoch 59/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 931us/step - accuracy: 0.9889 - loss: 0.0374 - val_accuracy: 0.7833 - val_loss: 1.2353\nEpoch 60/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9904 - loss: 0.0327 - val_accuracy: 0.7790 - val_loss: 1.4266\nEpoch 61/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 918us/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.7747 - val_loss: 1.9654\nEpoch 62/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 861us/step - accuracy: 1.0000 - loss: 3.3569e-04 - val_accuracy: 0.7747 - val_loss: 2.3092\nEpoch 63/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 948us/step - accuracy: 1.0000 - loss: 1.6892e-04 - val_accuracy: 0.7704 - val_loss: 2.5314\nEpoch 64/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 7.2569e-05 - val_accuracy: 0.7768 - val_loss: 2.6577\nEpoch 65/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 7.2654e-05 - val_accuracy: 0.7725 - val_loss: 2.7705\nEpoch 66/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 3.9437e-05 - val_accuracy: 0.7725 - val_loss: 2.8538\nEpoch 67/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 4.1582e-05 - val_accuracy: 0.7704 - val_loss: 2.9289\nEpoch 68/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.8892e-05 - val_accuracy: 0.7725 - val_loss: 2.9817\nEpoch 69/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.8131e-05 - val_accuracy: 0.7704 - val_loss: 3.0400\nEpoch 70/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 938us/step - accuracy: 1.0000 - loss: 1.9904e-05 - val_accuracy: 0.7704 - val_loss: 3.0946\nEpoch 71/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.9827e-05 - val_accuracy: 0.7747 - val_loss: 3.1410\nEpoch 72/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.7470e-05 - val_accuracy: 0.7725 - val_loss: 3.1845\nEpoch 73/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.3605e-05 - val_accuracy: 0.7725 - val_loss: 3.2222\nEpoch 74/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.0218e-05 - val_accuracy: 0.7725 - val_loss: 3.2573\nEpoch 75/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.2152e-05 - val_accuracy: 0.7704 - val_loss: 3.2939\nEpoch 76/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.1728e-05 - val_accuracy: 0.7704 - val_loss: 3.3254\nEpoch 77/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.3472e-05 - val_accuracy: 0.7704 - val_loss: 3.3560\nEpoch 78/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 8.8660e-06 - val_accuracy: 0.7704 - val_loss: 3.3849\nEpoch 79/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 963us/step - accuracy: 1.0000 - loss: 5.7242e-06 - val_accuracy: 0.7704 - val_loss: 3.4115\nEpoch 80/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 877us/step - accuracy: 1.0000 - loss: 6.4141e-06 - val_accuracy: 0.7704 - val_loss: 3.4389\nEpoch 81/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 6.1472e-06 - val_accuracy: 0.7682 - val_loss: 3.4642\nEpoch 82/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 3.6581e-06 - val_accuracy: 0.7682 - val_loss: 3.4863\nEpoch 83/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 4.8500e-06 - val_accuracy: 0.7704 - val_loss: 3.5141\nEpoch 84/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 921us/step - accuracy: 1.0000 - loss: 6.0898e-06 - val_accuracy: 0.7704 - val_loss: 3.5375\nEpoch 85/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 4.9514e-06 - val_accuracy: 0.7704 - val_loss: 3.5587\nEpoch 86/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 991us/step - accuracy: 1.0000 - loss: 7.3356e-06 - val_accuracy: 0.7704 - val_loss: 3.5808\nEpoch 87/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 899us/step - accuracy: 1.0000 - loss: 4.5561e-06 - val_accuracy: 0.7704 - val_loss: 3.6012\nEpoch 88/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 4.4042e-06 - val_accuracy: 0.7704 - val_loss: 3.6212\nEpoch 89/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 960us/step - accuracy: 1.0000 - loss: 4.5633e-06 - val_accuracy: 0.7704 - val_loss: 3.6406\nEpoch 90/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 926us/step - accuracy: 1.0000 - loss: 2.7448e-06 - val_accuracy: 0.7704 - val_loss: 3.6579\nEpoch 91/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 1.0000 - loss: 3.4195e-06 - val_accuracy: 0.7704 - val_loss: 3.6789\nEpoch 92/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 3.9085e-06 - val_accuracy: 0.7704 - val_loss: 3.6972\nEpoch 93/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 3.2672e-06 - val_accuracy: 0.7704 - val_loss: 3.7148\nEpoch 94/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 3.3254e-06 - val_accuracy: 0.7704 - val_loss: 3.7319\nEpoch 95/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 930us/step - accuracy: 1.0000 - loss: 2.4199e-06 - val_accuracy: 0.7704 - val_loss: 3.7486\nEpoch 96/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 2.1130e-06 - val_accuracy: 0.7704 - val_loss: 3.7652\nEpoch 97/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 2.6757e-06 - val_accuracy: 0.7704 - val_loss: 3.7820\nEpoch 98/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.7106e-06 - val_accuracy: 0.7704 - val_loss: 3.7969\nEpoch 99/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 2.0971e-06 - val_accuracy: 0.7704 - val_loss: 3.8135\nEpoch 100/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 1.0000 - loss: 1.5652e-06 - val_accuracy: 0.7725 - val_loss: 3.8275\n\n\nc:\\Users\\dan\\anaconda3\\envs\\tf_hsma\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nCalculate training and test set accuracy of your model.\n\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\n15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 549us/step\nTraining accuracy 1.000\nTest accuracy 0.773\n\n\nPlot training and test set accuracy over time during fitting.\n\nhistory_dict = history.history\nhistory_dict.keys()\n\nplot_training(history.history)\n\nC:\\Users\\dan\\AppData\\Local\\Temp\\ipykernel_31272\\832266572.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\nSave this baseline version of your model, so you can access it again later if you need it.\n\n# Save model\nmodel.save('baseline_stroke_dan.keras')\n\nNow try different things in your model to improve test accuracy. You might consider : - Reducing overfitting if overfitting is a problem. - Changing the number of hidden layers - Changing the number of hidden neurons - Changing batch size - Changing dropout rate - Changing the learning rate - Changing the train / test split - Trying stratified k-fold validation - Dropping features\nor more!\nTip : keep your analysis above as your base case. Then below, just use the functions you’ve built to rebuild and retrain models with different parameters (or run altered versions of other cells below). Don’t forget, you need to build and train again before you get new outputs.\nAdd comments to your code to explain what you’ve changed, and change things a bit at a time (don’t change everything all at once!)\n\n\"\"\"Add EarlyStopping with 10 epoch patience, and restore best weights\"\"\"\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=10, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=100,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\nEpoch 1/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.6107 - loss: 0.6270 - val_accuracy: 0.8155 - val_loss: 0.4071\nEpoch 2/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7961 - loss: 0.4391 - val_accuracy: 0.8133 - val_loss: 0.3966\nEpoch 3/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8156 - loss: 0.4099 - val_accuracy: 0.8219 - val_loss: 0.3931\nEpoch 4/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8115 - loss: 0.4034 - val_accuracy: 0.7790 - val_loss: 0.5224\nEpoch 5/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7804 - loss: 0.4493 - val_accuracy: 0.8240 - val_loss: 0.3890\nEpoch 6/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8438 - loss: 0.3663 - val_accuracy: 0.8197 - val_loss: 0.3900\nEpoch 7/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8591 - loss: 0.3349 - val_accuracy: 0.8112 - val_loss: 0.4120\nEpoch 8/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8278 - loss: 0.3703 - val_accuracy: 0.8112 - val_loss: 0.4029\nEpoch 9/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8594 - loss: 0.3229 - val_accuracy: 0.7876 - val_loss: 0.4435\nEpoch 10/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8776 - loss: 0.3051 - val_accuracy: 0.8026 - val_loss: 0.4627\nEpoch 11/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 991us/step - accuracy: 0.8698 - loss: 0.2934 - val_accuracy: 0.7554 - val_loss: 0.6602\nEpoch 12/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 967us/step - accuracy: 0.8655 - loss: 0.2906 - val_accuracy: 0.8197 - val_loss: 0.4412\nEpoch 13/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 991us/step - accuracy: 0.8786 - loss: 0.2782 - val_accuracy: 0.8112 - val_loss: 0.4730\nEpoch 14/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8936 - loss: 0.2298 - val_accuracy: 0.8069 - val_loss: 0.5321\nEpoch 15/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 997us/step - accuracy: 0.9035 - loss: 0.2295 - val_accuracy: 0.8176 - val_loss: 0.5017\n\n\n\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 992us/step\n15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 529us/step\nTraining accuracy 0.835\nTest accuracy 0.824\n\n\n\nplot_training(history.history)\n\nC:\\Users\\dan\\AppData\\Local\\Temp\\ipykernel_31272\\832266572.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\n\"\"\"Keep EarlyStopping with 10 epoch patience, and restore best weights.\nReduce number of hidden layers to 3 (from 5)\n\"\"\"\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=10, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features, hidden_layers=3)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=100,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\nEpoch 1/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.6508 - loss: 0.6134 - val_accuracy: 0.8004 - val_loss: 0.4267\nEpoch 2/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7875 - loss: 0.4342 - val_accuracy: 0.8197 - val_loss: 0.3939\nEpoch 3/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8147 - loss: 0.4055 - val_accuracy: 0.8112 - val_loss: 0.3909\nEpoch 4/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8235 - loss: 0.3869 - val_accuracy: 0.8369 - val_loss: 0.3855\nEpoch 5/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8459 - loss: 0.3681 - val_accuracy: 0.8391 - val_loss: 0.3859\nEpoch 6/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8339 - loss: 0.3546 - val_accuracy: 0.8369 - val_loss: 0.3840\nEpoch 7/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8555 - loss: 0.3348 - val_accuracy: 0.8197 - val_loss: 0.4127\nEpoch 8/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 956us/step - accuracy: 0.8794 - loss: 0.3000 - val_accuracy: 0.8047 - val_loss: 0.4339\nEpoch 9/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 938us/step - accuracy: 0.8790 - loss: 0.2948 - val_accuracy: 0.8197 - val_loss: 0.4363\nEpoch 10/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8838 - loss: 0.2697 - val_accuracy: 0.8391 - val_loss: 0.4655\nEpoch 11/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 992us/step - accuracy: 0.8887 - loss: 0.2523 - val_accuracy: 0.8240 - val_loss: 0.5127\nEpoch 12/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 929us/step - accuracy: 0.8853 - loss: 0.2642 - val_accuracy: 0.8262 - val_loss: 0.4821\nEpoch 13/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 862us/step - accuracy: 0.9054 - loss: 0.2427 - val_accuracy: 0.8283 - val_loss: 0.5086\nEpoch 14/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9248 - loss: 0.1904 - val_accuracy: 0.8262 - val_loss: 0.5621\nEpoch 15/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.9099 - loss: 0.2048 - val_accuracy: 0.8155 - val_loss: 0.6487\nEpoch 16/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 916us/step - accuracy: 0.9298 - loss: 0.1772 - val_accuracy: 0.7833 - val_loss: 0.7619\n\n\n\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 868us/step\n15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 650us/step\nTraining accuracy 0.866\nTest accuracy 0.837\n\n\n\nplot_training(history.history)\n\nC:\\Users\\dan\\AppData\\Local\\Temp\\ipykernel_31272\\832266572.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\n\"\"\"Keep EarlyStopping and 3 hidden layers.\nAdd dropout with rate of 0.5\n\"\"\"\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=10, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features, hidden_layers=3, dropout=0.5)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=100,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\nEpoch 1/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.5520 - loss: 0.6732 - val_accuracy: 0.7339 - val_loss: 0.5768\nEpoch 2/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.6873 - loss: 0.5617 - val_accuracy: 0.8283 - val_loss: 0.4466\nEpoch 3/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7454 - loss: 0.5279 - val_accuracy: 0.8176 - val_loss: 0.4190\nEpoch 4/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7650 - loss: 0.5017 - val_accuracy: 0.8348 - val_loss: 0.4051\nEpoch 5/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7930 - loss: 0.4507 - val_accuracy: 0.8262 - val_loss: 0.4038\nEpoch 6/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7885 - loss: 0.4564 - val_accuracy: 0.8155 - val_loss: 0.3887\nEpoch 7/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7976 - loss: 0.4555 - val_accuracy: 0.8219 - val_loss: 0.3939\nEpoch 8/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8125 - loss: 0.4141 - val_accuracy: 0.8262 - val_loss: 0.3924\nEpoch 9/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 958us/step - accuracy: 0.8250 - loss: 0.4037 - val_accuracy: 0.8262 - val_loss: 0.3944\nEpoch 10/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 945us/step - accuracy: 0.7960 - loss: 0.4407 - val_accuracy: 0.8155 - val_loss: 0.3962\nEpoch 11/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8085 - loss: 0.4200 - val_accuracy: 0.8069 - val_loss: 0.3916\nEpoch 12/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 995us/step - accuracy: 0.8281 - loss: 0.3999 - val_accuracy: 0.8219 - val_loss: 0.3949\nEpoch 13/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8235 - loss: 0.3940 - val_accuracy: 0.8219 - val_loss: 0.3828\nEpoch 14/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8165 - loss: 0.3901 - val_accuracy: 0.8197 - val_loss: 0.3931\nEpoch 15/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8400 - loss: 0.3738 - val_accuracy: 0.8262 - val_loss: 0.3871\nEpoch 16/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8179 - loss: 0.3934 - val_accuracy: 0.8047 - val_loss: 0.4043\nEpoch 17/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8340 - loss: 0.4029 - val_accuracy: 0.8004 - val_loss: 0.4027\nEpoch 18/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 914us/step - accuracy: 0.8297 - loss: 0.3927 - val_accuracy: 0.8240 - val_loss: 0.3883\nEpoch 19/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8325 - loss: 0.3957 - val_accuracy: 0.8004 - val_loss: 0.4047\nEpoch 20/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8406 - loss: 0.3817 - val_accuracy: 0.8069 - val_loss: 0.4060\nEpoch 21/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8344 - loss: 0.3684 - val_accuracy: 0.7961 - val_loss: 0.4106\nEpoch 22/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8493 - loss: 0.3780 - val_accuracy: 0.7918 - val_loss: 0.4022\nEpoch 23/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8283 - loss: 0.3681 - val_accuracy: 0.8112 - val_loss: 0.4149\n\n\n\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 822us/step\n15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step\nTraining accuracy 0.837\nTest accuracy 0.822\n\n\n\n\"\"\"Keep EarlyStopping, 3 hidden layers and 0.5 dropout.\nChange to 32 hidden layer neurons (from 64)\n\"\"\"\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=10, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features, hidden_layers=3, dropout=0.5,\n                 hidden_layer_neurones=32)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=100,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\nEpoch 1/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.5521 - loss: 0.7286 - val_accuracy: 0.6030 - val_loss: 0.6366\nEpoch 2/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5799 - loss: 0.6559 - val_accuracy: 0.6931 - val_loss: 0.5837\nEpoch 3/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.6458 - loss: 0.6387 - val_accuracy: 0.7489 - val_loss: 0.5316\nEpoch 4/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.6782 - loss: 0.5821 - val_accuracy: 0.8090 - val_loss: 0.4784\nEpoch 5/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7050 - loss: 0.5606 - val_accuracy: 0.7983 - val_loss: 0.4451\nEpoch 6/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7499 - loss: 0.5106 - val_accuracy: 0.8197 - val_loss: 0.4301\nEpoch 7/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7868 - loss: 0.4857 - val_accuracy: 0.8112 - val_loss: 0.4158\nEpoch 8/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7849 - loss: 0.4531 - val_accuracy: 0.8412 - val_loss: 0.3920\nEpoch 9/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7752 - loss: 0.4861 - val_accuracy: 0.8348 - val_loss: 0.3976\nEpoch 10/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7822 - loss: 0.4594 - val_accuracy: 0.8348 - val_loss: 0.3914\nEpoch 11/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 971us/step - accuracy: 0.7885 - loss: 0.4631 - val_accuracy: 0.8326 - val_loss: 0.3998\nEpoch 12/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7685 - loss: 0.5016 - val_accuracy: 0.8348 - val_loss: 0.3889\nEpoch 13/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8031 - loss: 0.4424 - val_accuracy: 0.8369 - val_loss: 0.3759\nEpoch 14/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8167 - loss: 0.4183 - val_accuracy: 0.8476 - val_loss: 0.3722\nEpoch 15/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8213 - loss: 0.4152 - val_accuracy: 0.8240 - val_loss: 0.3798\nEpoch 16/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8163 - loss: 0.4082 - val_accuracy: 0.8305 - val_loss: 0.3910\nEpoch 17/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 942us/step - accuracy: 0.8055 - loss: 0.4163 - val_accuracy: 0.8283 - val_loss: 0.3822\nEpoch 18/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8019 - loss: 0.4332 - val_accuracy: 0.8369 - val_loss: 0.3796\nEpoch 19/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 934us/step - accuracy: 0.8201 - loss: 0.4099 - val_accuracy: 0.8412 - val_loss: 0.3780\nEpoch 20/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 850us/step - accuracy: 0.8199 - loss: 0.3930 - val_accuracy: 0.8219 - val_loss: 0.3830\nEpoch 21/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8101 - loss: 0.4252 - val_accuracy: 0.8326 - val_loss: 0.3770\nEpoch 22/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8142 - loss: 0.3924 - val_accuracy: 0.8283 - val_loss: 0.3833\nEpoch 23/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7955 - loss: 0.4265 - val_accuracy: 0.8326 - val_loss: 0.3724\nEpoch 24/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8231 - loss: 0.3923 - val_accuracy: 0.8219 - val_loss: 0.3774\n\n\n\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 862us/step\n15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 720us/step\nTraining accuracy 0.832\nTest accuracy 0.848\n\n\nIn the following cells, I walk through an example of dropping one of the features (whether the patient has diabetes) from the data as an example of how you would do this. In practice, you should do this following some analysis via Explainable AI methods, such as the use of Shapley values, which you will learn about in session 4G.\n\n# We cut from X, which already has the label (\"Clotbuster given\") dropped\n# We can use the same y as before\nX_cut = X.drop('Diabetes', axis=1)\n\nX_np_cut = X_cut.values\n\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_np_cut, y_np, test_size = 0.25, random_state=42)\n\n# Scale X data\nX_train_sc, X_test_sc = scale_data(X_train, X_test)\n\n\n\"\"\"Keep EarlyStopping, 3 hidden layers, 0.5 dropout, and 32 hidden layer\nneurons.\nDropped 'diabetes' feature from data.\n\"\"\"\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=10, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features, hidden_layers=3, dropout=0.5,\n                 hidden_layer_neurones=32)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=100,\n                    batch_size=32,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\nEpoch 1/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.5255 - loss: 0.7121 - val_accuracy: 0.6009 - val_loss: 0.6663\nEpoch 2/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.6119 - loss: 0.6685 - val_accuracy: 0.6137 - val_loss: 0.6449\nEpoch 3/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.6093 - loss: 0.6471 - val_accuracy: 0.6931 - val_loss: 0.5566\nEpoch 4/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.6685 - loss: 0.5886 - val_accuracy: 0.7854 - val_loss: 0.4961\nEpoch 5/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.6935 - loss: 0.5392 - val_accuracy: 0.8026 - val_loss: 0.4620\nEpoch 6/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7225 - loss: 0.5247 - val_accuracy: 0.7918 - val_loss: 0.4429\nEpoch 7/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.7649 - loss: 0.4968 - val_accuracy: 0.8112 - val_loss: 0.4242\nEpoch 8/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7832 - loss: 0.5054 - val_accuracy: 0.8112 - val_loss: 0.4209\nEpoch 9/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7763 - loss: 0.4727 - val_accuracy: 0.8197 - val_loss: 0.4113\nEpoch 10/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8052 - loss: 0.4480 - val_accuracy: 0.8112 - val_loss: 0.4108\nEpoch 11/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7930 - loss: 0.4660 - val_accuracy: 0.8133 - val_loss: 0.3960\nEpoch 12/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7972 - loss: 0.4456 - val_accuracy: 0.8197 - val_loss: 0.3940\nEpoch 13/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7865 - loss: 0.4443 - val_accuracy: 0.8176 - val_loss: 0.4032\nEpoch 14/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7971 - loss: 0.4593 - val_accuracy: 0.8112 - val_loss: 0.3945\nEpoch 15/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7964 - loss: 0.4329 - val_accuracy: 0.8090 - val_loss: 0.3985\nEpoch 16/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7855 - loss: 0.4345 - val_accuracy: 0.8090 - val_loss: 0.3975\nEpoch 17/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.7984 - loss: 0.4616 - val_accuracy: 0.8240 - val_loss: 0.3913\nEpoch 18/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8087 - loss: 0.4263 - val_accuracy: 0.8155 - val_loss: 0.3910\nEpoch 19/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8138 - loss: 0.4199 - val_accuracy: 0.8240 - val_loss: 0.3952\nEpoch 20/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8190 - loss: 0.4022 - val_accuracy: 0.8197 - val_loss: 0.3943\nEpoch 21/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8070 - loss: 0.4218 - val_accuracy: 0.8348 - val_loss: 0.3825\nEpoch 22/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8208 - loss: 0.4084 - val_accuracy: 0.8262 - val_loss: 0.3912\nEpoch 23/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8273 - loss: 0.3988 - val_accuracy: 0.8155 - val_loss: 0.3992\nEpoch 24/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8295 - loss: 0.4057 - val_accuracy: 0.8155 - val_loss: 0.3878\nEpoch 25/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8199 - loss: 0.4170 - val_accuracy: 0.8305 - val_loss: 0.3885\nEpoch 26/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8262 - loss: 0.4101 - val_accuracy: 0.8133 - val_loss: 0.3868\nEpoch 27/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8368 - loss: 0.3920 - val_accuracy: 0.8197 - val_loss: 0.3940\nEpoch 28/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 947us/step - accuracy: 0.8230 - loss: 0.3890 - val_accuracy: 0.8283 - val_loss: 0.3871\nEpoch 29/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8281 - loss: 0.3892 - val_accuracy: 0.8197 - val_loss: 0.3923\nEpoch 30/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8222 - loss: 0.4086 - val_accuracy: 0.8090 - val_loss: 0.4054\nEpoch 31/100\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8278 - loss: 0.3998 - val_accuracy: 0.8197 - val_loss: 0.3927\n\n\nc:\\Users\\dan\\anaconda3\\envs\\tf_hsma\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \n15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step\nTraining accuracy 0.847\nTest accuracy 0.835\n\n\nIn the above example, dropping the ‘diabetes’ feature led to a more inaccurate model on the test set.",
    "crumbs": [
      "4F - Neural Networks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exercise Solution: Neural Networks (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "",
    "text": "13.1 Logistic regression\nIn this example we will use logistic regression (see https://en.wikipedia.org/wiki/Logistic_regression).\nFor an introductory video on logistic regression see: https://www.youtube.com/watch?v=yIYKR4sgzI8\nLogistic regression takes a range of features (which we will normalise/standardise to put on the same scale) and returns a probability that a certain classification (survival in this case) is true.\nWe will go through the following steps:",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#logistic-regression",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#logistic-regression",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "",
    "text": "Download and save pre-processed data\nSplit data into features (X) and label (y)\nSplit data into training and test sets (we will test on data that has not been used to fit the model)\nStandardise data\nFit a logistic regression model (from sklearn)\nPredict survival of the test set, and assess accuracy\nReview model coefficients (weights) to see importance of features\nShow probability of survival for passengers",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#load-modules",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#load-modules",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.2 Load modules",
    "text": "13.2 Load modules\nA standard Anaconda install of Python (https://www.anaconda.com/distribution/) contains all the necessary modules. Use your base environment if in doubt, or the ml_sammi environment if you have it installed.\n\nimport numpy as np\nimport pandas as pd\n# Import machine learning methods\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import PartialDependenceDisplay, partial_dependence\n\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt\nimport statistics",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#load-data",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#load-data",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.3 Load data",
    "text": "13.3 Load data\nThe section below downloads pre-processed data, and saves it to a subfolder (from where this code is run). If data has already been downloaded that cell may be skipped.\nCode that was used to pre-process the data ready for machine learning may be found at: https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)\n\n\ndata = pd.read_csv('data/processed_data.csv')\n# Make all data 'float' type\ndata = data.astype(float)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#examine-loaded-data",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#examine-loaded-data",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.4 Examine loaded data",
    "text": "13.4 Examine loaded data\nThe data is in the form of a Pandas DataFrame, so we have column headers providing information of what is contained in each column.\nWe will use the DataFrame .head() method to show the first few rows of the imported DataFrame. By default this shows the first 5 rows. Here we will look at the first 10.\n\ndata.head(10)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n0\n1.0\n0.0\n3.0\n22.0\n1.0\n0.0\n7.2500\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n2.0\n1.0\n1.0\n38.0\n1.0\n0.0\n71.2833\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n3.0\n1.0\n3.0\n26.0\n0.0\n0.0\n7.9250\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n4.0\n1.0\n1.0\n35.0\n1.0\n0.0\n53.1000\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n5.0\n0.0\n3.0\n35.0\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n5\n6.0\n0.0\n3.0\n28.0\n0.0\n0.0\n8.4583\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n6\n7.0\n0.0\n1.0\n54.0\n0.0\n0.0\n51.8625\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n7\n8.0\n0.0\n3.0\n2.0\n3.0\n1.0\n21.0750\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n8\n9.0\n1.0\n3.0\n27.0\n0.0\n2.0\n11.1333\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n9\n10.0\n1.0\n2.0\n14.0\n1.0\n0.0\n30.0708\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n10 rows × 26 columns\n\n\n\n\nNote that in the above, “Imputed” column names indicate whether that feature was blank in the original data and was filled in (“imputed”) in the preprocessing (so “AgeImputed” value of 1.0 means the age was missing, 0.0 means it wasn’t). You can’t have missing cells in Machine Learning, so you have to decide what to do when you have missing data. This is part of the pre-processing step you’ll need to do with real data, and there’s more information about this in the pre-processing notebook here : https://michaelallen1966.github.io/titanic/01_preprocessing.html\nWe can also show a summary of the data with the .describe() method.\n\ndata.describe()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n...\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.361582\n0.523008\n0.381594\n32.204208\n0.198653\n0.002245\n0.771044\n...\n0.002245\n0.016835\n0.052750\n0.066218\n0.037037\n0.035915\n0.014590\n0.004489\n0.001122\n0.771044\n\n\nstd\n257.353842\n0.486592\n0.836071\n13.019697\n1.102743\n0.806057\n49.693429\n0.399210\n0.047351\n0.420397\n...\n0.047351\n0.128725\n0.223659\n0.248802\n0.188959\n0.186182\n0.119973\n0.066890\n0.033501\n0.420397\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n22.000000\n0.000000\n0.000000\n7.910400\n0.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n0.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n75%\n668.500000\n1.000000\n3.000000\n35.000000\n1.000000\n0.000000\n31.000000\n0.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 26 columns\n\n\n\n\nThe first column is a passenger index number. We will remove this, as this is not part of the original Titanic passenger data, and will not help us train our model.\n\n# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n# We drop passenger ID as it is not original data\n# inplace=True means change the dataframe itself - don't create a copy with this column dropped\n\ndata.drop('PassengerId', inplace=True, axis=1)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#looking-at-a-summary-of-passengers-who-survived-or-did-not-survive",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#looking-at-a-summary-of-passengers-who-survived-or-did-not-survive",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.5 Looking at a summary of passengers who survived or did not survive",
    "text": "13.5 Looking at a summary of passengers who survived or did not survive\nBefore running machine learning models, it is always good to have a look at your data. Here we will separate passengers who survived from those who died, and we will have a look at differences in features.\nWe will use a mask to select and filter passengers. The mask applies Boolean values (True and False) to entries depending on a given condition. Below, we use this to create a mask that has True values for any rows where the Survived value is 1.0 (ie where the patient survived), and then store only those rows in a separate dataframe called “survived”. Then we do the same thing but for those who died.\n\nmask = data['Survived'] == 1 # Mask for passengers who survive\nsurvived = data[mask] # filter using mask\n\nmask = data['Survived'] == 0 # Mask for passengers who died\ndied = data[mask] # filter using mask\n\nNow let’s look at average (mean) values for each feature for those who survived and those who died. We can make comparing them easier by putting these values in a new DataFrame so we can look at them side by side. What do you notice? What features do you think might have influenced survival?\n\nsummary = pd.DataFrame() # New empty DataFrame\nsummary['survived'] = survived.mean()\nsummary['died'] = died.mean()\n\n\nsummary\n\n\n\n\n\n\n\n\n\nsurvived\ndied\n\n\n\n\nSurvived\n1.000000\n0.000000\n\n\nPclass\n1.950292\n2.531876\n\n\nAge\n28.291433\n30.028233\n\n\nSibSp\n0.473684\n0.553734\n\n\nParch\n0.464912\n0.329690\n\n\nFare\n48.395408\n22.117887\n\n\nAgeImputed\n0.152047\n0.227687\n\n\nEmbarkedImputed\n0.005848\n0.000000\n\n\nCabinLetterImputed\n0.602339\n0.876138\n\n\nCabinNumber\n18.961988\n6.074681\n\n\nCabinNumberImputed\n0.611111\n0.885246\n\n\nmale\n0.318713\n0.852459\n\n\nEmbarked_C\n0.271930\n0.136612\n\n\nEmbarked_Q\n0.087719\n0.085610\n\n\nEmbarked_S\n0.634503\n0.777778\n\n\nEmbarked_missing\n0.005848\n0.000000\n\n\nCabinLetter_A\n0.020468\n0.014572\n\n\nCabinLetter_B\n0.102339\n0.021858\n\n\nCabinLetter_C\n0.102339\n0.043716\n\n\nCabinLetter_D\n0.073099\n0.014572\n\n\nCabinLetter_E\n0.070175\n0.014572\n\n\nCabinLetter_F\n0.023392\n0.009107\n\n\nCabinLetter_G\n0.005848\n0.003643\n\n\nCabinLetter_T\n0.000000\n0.001821\n\n\nCabinLetter_missing\n0.602339\n0.876138",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#divide-into-x-features-and-y-labels",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#divide-into-x-features-and-y-labels",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.6 Divide into X (features) and y (labels)",
    "text": "13.6 Divide into X (features) and y (labels)\nWe will separate out our features (the data we use to make a prediction) from our label (what we are trying to predict - survival here). By convention our features are called X (usually upper case to denote multiple features), and the label (survived or not) y.\n\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#divide-into-training-and-tets-sets",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#divide-into-training-and-tets-sets",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.7 Divide into training and tets sets",
    "text": "13.7 Divide into training and tets sets\nWhen we test a machine learning model we should always test it on data that has not been used to train the model. We will use sklearn’s train_test_split method to randomly split the data: 75% for training, and 25% for testing.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n\nLet’s have a look at the standard deviation and the mean of the feature values in the training data (Standard Deviation for each feature shown first, then mean for each feature)\n\nX_train.std(), X_train.mean()\n\n(Pclass                  0.823707\n Age                    13.021815\n SibSp                   1.185279\n Parch                   0.795588\n Fare                   51.604012\n AgeImputed              0.398491\n EmbarkedImputed         0.054677\n CabinLetterImputed      0.416606\n CabinNumber            25.685466\n CabinNumberImputed      0.410484\n male                    0.475006\n Embarked_C              0.380380\n Embarked_Q              0.281791\n Embarked_S              0.441648\n Embarked_missing        0.054677\n CabinLetter_A           0.121524\n CabinLetter_B           0.219955\n CabinLetter_C           0.253410\n CabinLetter_D           0.166360\n CabinLetter_E           0.186250\n CabinLetter_F           0.132920\n CabinLetter_G           0.066915\n CabinLetter_T           0.038691\n CabinLetter_missing     0.416606\n dtype: float64,\n Pclass                  2.333832\n Age                    29.140479\n SibSp                   0.553892\n Parch                   0.372754\n Fare                   32.179397\n AgeImputed              0.197605\n EmbarkedImputed         0.002994\n CabinLetterImputed      0.776946\n CabinNumber            10.402695\n CabinNumberImputed      0.785928\n male                    0.657186\n Embarked_C              0.175150\n Embarked_Q              0.086826\n Embarked_S              0.735030\n Embarked_missing        0.002994\n CabinLetter_A           0.014970\n CabinLetter_B           0.050898\n CabinLetter_C           0.068862\n CabinLetter_D           0.028443\n CabinLetter_E           0.035928\n CabinLetter_F           0.017964\n CabinLetter_G           0.004491\n CabinLetter_T           0.001497\n CabinLetter_missing     0.776946\n dtype: float64)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#standardise-data",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#standardise-data",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.8 Standardise data",
    "text": "13.8 Standardise data\nWe can see above that there are quite different scales across different features in this data. For example, passenger class is on a very different scale numerically than the fare paid.\nWe want all of our features to be on roughly the same scale. This generally leads to a better model, and also allows us to more easily compare the importance of different features.\nWe will use standardisation to scale our feature values, where we use the mean and standard deviation of the training set of data to normalise the data. We subtract the mean of the training set values, and divide by the standard deviation of the training data. Note that the mean and standard deviation of the training data are used to standardise the test set data as well.\nHere we will use sklearn’s StandardScaler method. This method also copes with problems we might otherwise have (such as if one feature has zero standard deviation in the training set). We write a little function so whenever we need to standardise some data, we can just call this function, pass in the training and test feature (X) data, and it’ll return the same data but standardised.\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.fit_transform(X_train)\n    test_std=sc.fit_transform(X_test)\n\n    return train_std, test_std\n\nNow let’s call this function and use it to standardise our training and test data.\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#visualise-features",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#visualise-features",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.9 Visualise Features",
    "text": "13.9 Visualise Features\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n298\n1.0\n28.00\n0.0\n0.0\n30.5000\n1.0\n0.0\n0.0\n106.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n884\n3.0\n25.00\n0.0\n0.0\n7.0500\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n247\n2.0\n24.00\n0.0\n2.0\n14.5000\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n478\n3.0\n22.00\n0.0\n0.0\n7.5208\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n305\n1.0\n0.92\n1.0\n2.0\n151.5500\n0.0\n0.0\n0.0\n22.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\nax = X_train['Age'].hist(bins=30)\nax = plt.title(\"Histogram of Age in Training Set\")\n\n\n\n\n\n\n\n\n\nax = X_train['Age'].hist(bins=30)\nax = plt.title(\"Histogram of Age in Training Set\")\n\nmean_age = np.mean(X_train['Age'])\nprint(f\"Mean age: {mean_age}\")\nstdev_age = statistics.stdev(X_train['Age'])\nprint(f\"Standard Deviation of age: {stdev_age}\")\n\nplt.vlines(x=mean_age, colors=\"red\", ymin=0, ymax=180)\nplt.vlines(x=mean_age+stdev_age, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age+stdev_age*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age+stdev_age*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age-stdev_age, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age-stdev_age*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age-stdev_age*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\n\nMean age: 29.14047904191617\nStandard Deviation of age: 13.02181499397036\n\n\n\n\n\n\n\n\n\n\nax = plt.hist(X_train_std[:, 1],bins=30)\nmean_age = np.mean(X_train_std[:, 1])\nstdev_age = statistics.stdev(X_train_std[:, 1])\nplt.vlines(x=mean_age, colors=\"red\", ymin=0, ymax=180)\nplt.vlines(x=mean_age+stdev_age, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age+stdev_age*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age+stdev_age*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age-stdev_age, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age-stdev_age*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_age-stdev_age*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.grid()\nax = plt.title(\"Histogram of Age in Training Set - Standardised Data\")\n\n\n\n\n\n\n\n\n\nX_train['Fare'].hist(bins=30)\nax = plt.title(\"Histogram of Fare in Training Set\")\n\n\n\n\n\n\n\n\n\nX_train['Fare'].hist(bins=30)\nax = plt.title(\"Histogram of Fare in Training Set\")\nmean_fare = np.mean(X_train['Fare'])\nprint(f\"Mean fare: {mean_fare}\")\nstdev_fare = statistics.stdev(X_train['Fare'])\nprint(f\"Standard Deviation of fare: {stdev_fare}\")\nplt.vlines(x=mean_fare, colors=\"red\", ymin=0, ymax=180)\nplt.vlines(x=mean_fare+stdev_fare, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare+stdev_fare*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare+stdev_fare*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare-stdev_fare, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare-stdev_fare*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare-stdev_fare*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\n\nMean fare: 32.17939670658683\nStandard Deviation of fare: 51.60401208218659\n\n\n\n\n\n\n\n\n\n\nax = plt.hist(X_train_std[:,4], bins=30)\nax = plt.title(\"Histogram of Fare in Training Set\")\nmean_fare = statistics.stdev(X_train_std[:,4])\nprint(f\"Mean fare: {mean_fare}\")\nstdev_fare = statistics.stdev(X_train_std[:,4])\nprint(f\"Standard Deviation of fare: {stdev_age}\")\nplt.vlines(x=mean_fare, colors=\"red\", ymin=0, ymax=180)\nplt.vlines(x=mean_fare+stdev_fare, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare+stdev_fare*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare+stdev_fare*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare-stdev_fare, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare-stdev_fare*2, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.vlines(x=mean_fare-stdev_fare*3, colors=\"red\", ymin=0, ymax=180, linestyles=\"--\")\nplt.grid()\n\nMean fare: 1.0007493444288698\nStandard Deviation of fare: 1.0007493444288698",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#fit-logistic-regression-model",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#fit-logistic-regression-model",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.10 Fit logistic regression model",
    "text": "13.10 Fit logistic regression model\nNow we will fit a logistic regression model, using sklearn’s LogisticRegression method. Our machine learning model fitting (training) is only two lines of code! By using the name model for our logistic regression model we will make our model more interchangeable later on if we wanted to try a different kind of model, for example.\n\nmodel = LogisticRegression()\nmodel.fit(X_train_std,y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#predict-values",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#predict-values",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.11 Predict values",
    "text": "13.11 Predict values\nNow we can use the trained model to predict survival. We will test the accuracy of both the training and test data sets.\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train_std)\ny_pred_test = model.predict(X_test_std)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#calculate-accuracy",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#calculate-accuracy",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.12 Calculate accuracy",
    "text": "13.12 Calculate accuracy\nIn this example we will measure accuracy simply as the proportion of passengers where we make the correct prediction. In later examples we will look at other measures of accuracy which explore false positives and false negatives in more detail.\n\n# The shorthand below says to check each predicted y value against the actual\n# y value in the training data.  This gives a list of True and False values\n# for each prediction, where True indicates the predicted value matches the\n# actual value.  Then we take the mean of these Boolean values, which gives\n# us a proportion (where if all values were True, the proportion would be 1.0)\n# If you want to see why that works, just uncomment the following line of code\n# to see what y_pred_train == y_train is doing.\n# print (y_pred_train == y_train)\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train}')\nprint (f'Accuracy of predicting test data = {accuracy_test}')\n\nAccuracy of predicting training data = 0.8083832335329342\nAccuracy of predicting test data = 0.8116591928251121\n\n\nNot bad - about 80% accuracy. You will probably see that accuracy of predicting the training set is usually higher than the test set. Because we are only testing one random sample, you may occasionally see otherwise. In later examples we will look at the best way to repeat multiple tests, and look at what to do if the accuracy of the training set is significantly higher than the test set (a problem called ‘overfitting’).",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#examining-the-model-coefficients-weights",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#examining-the-model-coefficients-weights",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.13 Examining the model coefficients (weights)",
    "text": "13.13 Examining the model coefficients (weights)\nNot all features are equally important. And some may be of little or no use at all, unnecessarily increasing the complexity of the model. In later examples we will look at selecting features which add value to the model (or removing features that don’t).\nHere we will look at the importance of features – how they affect our estimation of survival. These are known as the model coefficients (if you come from a traditional statistics background), or model weights (if you come from a machine learning background).\nBecause we have standardised our input data the magnitude of the weights may be compared as an indicator of their influence in the model. Weights with higher negative numbers mean that that feature correlates with reduced chance of survival. Weights with higher positive numbers mean that that feature correlates with increased chance of survival. Those weights with values closer to zero (either positive or negative) have less influence in the model.\nWe access the model weights my examining the model coef_ attribute. The model may predict more than one outcome label, in which case we have weights for each label. Because we are predicting a single label here (survive or not), the weights are found in the first element ([0]) of the coef_ attribute.\n\nco_eff = model.coef_[0]\nco_eff\n\narray([-0.59933416, -0.44334034, -0.32650096, -0.12092065,  0.20049511,\n       -0.20985474,  0.09770207,  0.07977789,  0.12022711, -0.38167408,\n       -1.28561786,  0.10666948,  0.05522223, -0.13920157,  0.09770207,\n       -0.0581842 , -0.04534243, -0.24337741,  0.0598707 ,  0.17427913,\n        0.13557114, -0.13129024, -0.15952828,  0.07977789])\n\n\nSo we have an array of model weights.\nNot very readable for us mere humans is it?!\nWe will transfer the weights array to a Pandas DataFrame. The array order is in the same order of the list of features of X, so we will put that those into the DataFrame as well. And we will sort by influence in the model. Because both large negative and positive values are more influential in the model we will take the absolute value of the weight (ie remove any negative sign), and then sort by that absolute value. That will give us a more readable table of most influential features in the model.\n\nco_eff_df = pd.DataFrame() # create empty DataFrame\nco_eff_df['feature'] = list(X) # Get feature names from X\nco_eff_df['co_eff'] = co_eff\nco_eff_df['abs_co_eff'] = np.abs(co_eff)\nco_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=True)\n\nLet’s look at the DataFrame. What do you conclude?\n\nco_eff_df\n\n\n\n\n\n\n\n\n\nfeature\nco_eff\nabs_co_eff\n\n\n\n\n10\nmale\n-1.285618\n1.285618\n\n\n0\nPclass\n-0.599334\n0.599334\n\n\n1\nAge\n-0.443340\n0.443340\n\n\n9\nCabinNumberImputed\n-0.381674\n0.381674\n\n\n2\nSibSp\n-0.326501\n0.326501\n\n\n17\nCabinLetter_C\n-0.243377\n0.243377\n\n\n5\nAgeImputed\n-0.209855\n0.209855\n\n\n4\nFare\n0.200495\n0.200495\n\n\n19\nCabinLetter_E\n0.174279\n0.174279\n\n\n22\nCabinLetter_T\n-0.159528\n0.159528\n\n\n13\nEmbarked_S\n-0.139202\n0.139202\n\n\n20\nCabinLetter_F\n0.135571\n0.135571\n\n\n21\nCabinLetter_G\n-0.131290\n0.131290\n\n\n3\nParch\n-0.120921\n0.120921\n\n\n8\nCabinNumber\n0.120227\n0.120227\n\n\n11\nEmbarked_C\n0.106669\n0.106669\n\n\n6\nEmbarkedImputed\n0.097702\n0.097702\n\n\n14\nEmbarked_missing\n0.097702\n0.097702\n\n\n23\nCabinLetter_missing\n0.079778\n0.079778\n\n\n7\nCabinLetterImputed\n0.079778\n0.079778\n\n\n18\nCabinLetter_D\n0.059871\n0.059871\n\n\n15\nCabinLetter_A\n-0.058184\n0.058184\n\n\n12\nEmbarked_Q\n0.055222\n0.055222\n\n\n16\nCabinLetter_B\n-0.045342\n0.045342\n\n\n\n\n\n\n\n\nNow - the actual order of features, and the weights for each, will be different each time you fit the model, as it’s random and we haven’t put in place any controls. So don’t worry if yours differs a bit. But, you’ll likely find that amongst the top weighted features are :\n\nmale (being male reduces probability of survival)\nPclass (lower class passengers, who have a higher class number, reduces probability of survival)\nage (being older reduces probability of survival)\nCabinNumberImputed (cabin number is missing, which may mean they didn’t have a cabin - likely lower class)\n\nLet’s add the odds to the dataframe.\n\nco_eff_df['odds'] = np.exp(co_eff_df['co_eff'])\nco_eff_df\n\n\n\n\n\n\n\n\n\nfeature\nco_eff\nabs_co_eff\nodds\n\n\n\n\n10\nmale\n-1.285618\n1.285618\n0.276480\n\n\n0\nPclass\n-0.599334\n0.599334\n0.549177\n\n\n1\nAge\n-0.443340\n0.443340\n0.641889\n\n\n9\nCabinNumberImputed\n-0.381674\n0.381674\n0.682718\n\n\n2\nSibSp\n-0.326501\n0.326501\n0.721444\n\n\n17\nCabinLetter_C\n-0.243377\n0.243377\n0.783976\n\n\n5\nAgeImputed\n-0.209855\n0.209855\n0.810702\n\n\n4\nFare\n0.200495\n0.200495\n1.222008\n\n\n19\nCabinLetter_E\n0.174279\n0.174279\n1.190388\n\n\n22\nCabinLetter_T\n-0.159528\n0.159528\n0.852546\n\n\n13\nEmbarked_S\n-0.139202\n0.139202\n0.870053\n\n\n20\nCabinLetter_F\n0.135571\n0.135571\n1.145191\n\n\n21\nCabinLetter_G\n-0.131290\n0.131290\n0.876963\n\n\n3\nParch\n-0.120921\n0.120921\n0.886104\n\n\n8\nCabinNumber\n0.120227\n0.120227\n1.127753\n\n\n11\nEmbarked_C\n0.106669\n0.106669\n1.112566\n\n\n6\nEmbarkedImputed\n0.097702\n0.097702\n1.102634\n\n\n14\nEmbarked_missing\n0.097702\n0.097702\n1.102634\n\n\n23\nCabinLetter_missing\n0.079778\n0.079778\n1.083046\n\n\n7\nCabinLetterImputed\n0.079778\n0.079778\n1.083046\n\n\n18\nCabinLetter_D\n0.059871\n0.059871\n1.061699\n\n\n15\nCabinLetter_A\n-0.058184\n0.058184\n0.943476\n\n\n12\nEmbarked_Q\n0.055222\n0.055222\n1.056775\n\n\n16\nCabinLetter_B\n-0.045342\n0.045342\n0.955670\n\n\n\n\n\n\n\n\nFinally, let’s convert this to a change in probability.\n\nintercept = model.intercept_[0]\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nbaseline_prob = sigmoid(intercept)\n\ndef prob_change(coef, intercept):\n    baseline = sigmoid(intercept)\n    new_prob = sigmoid(intercept + coef)\n    return new_prob - baseline\n\nco_eff_df['probability_changes'] = [prob_change(coef, intercept) for coef in co_eff_df['co_eff']]\n\n\nco_eff_df\n\n\n\n\n\n\n\n\n\nfeature\nco_eff\nabs_co_eff\nodds\nprobability_changes\n\n\n\n\n10\nmale\n-1.285618\n1.285618\n0.276480\n-0.217047\n\n\n0\nPclass\n-0.599334\n0.599334\n0.549177\n-0.120259\n\n\n1\nAge\n-0.443340\n0.443340\n0.641889\n-0.092060\n\n\n9\nCabinNumberImputed\n-0.381674\n0.381674\n0.682718\n-0.080281\n\n\n2\nSibSp\n-0.326501\n0.326501\n0.721444\n-0.069446\n\n\n17\nCabinLetter_C\n-0.243377\n0.243377\n0.783976\n-0.052607\n\n\n5\nAgeImputed\n-0.209855\n0.209855\n0.810702\n-0.045646\n\n\n4\nFare\n0.200495\n0.200495\n1.222008\n0.046509\n\n\n19\nCabinLetter_E\n0.174279\n0.174279\n1.190388\n0.040291\n\n\n22\nCabinLetter_T\n-0.159528\n0.159528\n0.852546\n-0.035018\n\n\n13\nEmbarked_S\n-0.139202\n0.139202\n0.870053\n-0.030667\n\n\n20\nCabinLetter_F\n0.135571\n0.135571\n1.145191\n0.031181\n\n\n21\nCabinLetter_G\n-0.131290\n0.131290\n0.876963\n-0.028964\n\n\n3\nParch\n-0.120921\n0.120921\n0.886104\n-0.026724\n\n\n8\nCabinNumber\n0.120227\n0.120227\n1.127753\n0.027593\n\n\n11\nEmbarked_C\n0.106669\n0.106669\n1.112566\n0.024435\n\n\n6\nEmbarkedImputed\n0.097702\n0.097702\n1.102634\n0.022352\n\n\n14\nEmbarked_missing\n0.097702\n0.097702\n1.102634\n0.022352\n\n\n23\nCabinLetter_missing\n0.079778\n0.079778\n1.083046\n0.018205\n\n\n7\nCabinLetterImputed\n0.079778\n0.079778\n1.083046\n0.018205\n\n\n18\nCabinLetter_D\n0.059871\n0.059871\n1.061699\n0.013622\n\n\n15\nCabinLetter_A\n-0.058184\n0.058184\n0.943476\n-0.012996\n\n\n12\nEmbarked_Q\n0.055222\n0.055222\n1.056775\n0.012556\n\n\n16\nCabinLetter_B\n-0.045342\n0.045342\n0.955670\n-0.010149",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#show-predicted-probabilities",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#show-predicted-probabilities",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.14 Show predicted probabilities",
    "text": "13.14 Show predicted probabilities\nThe predicted probabilities are for the two alternative classes 0 (does not survive) or 1 (survive).\nOrdinarily we do not see these probabilities - the predict method used above applies a cut-off of 0.5 to classify passengers into survived or not, but we can see the individual probabilities for each passenger if desired.\nIn a later example we will see how we can use such probabilities to adjust the sensitivity of our model to detecting survivors or non-survivors.\nEach passenger has two values. These are the probability of not surviving (first value) or surviving (second value). Because we only have two possible classes we only need to look at one. Multiple values are important when there are more than one class being predicted.\n\n# Show first ten predicted classes\nclasses = model.predict(X_test_std)\nclasses[0:10]\n\narray([0., 0., 0., 1., 1., 1., 1., 0., 1., 1.])\n\n\n\n# Show first ten predicted probabilities\n# (note how the values relate to the classes predicted above)\nprobabilities = model.predict_proba(X_test_std)\nprobabilities[0:10]\n\narray([[0.93647344, 0.06352656],\n       [0.83235068, 0.16764932],\n       [0.87320715, 0.12679285],\n       [0.1350535 , 0.8649465 ],\n       [0.25288959, 0.74711041],\n       [0.09785614, 0.90214386],\n       [0.3968734 , 0.6031266 ],\n       [0.9284447 , 0.0715553 ],\n       [0.20571976, 0.79428024],\n       [0.05239181, 0.94760819]])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#prediction-uncertainty",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#prediction-uncertainty",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.15 Prediction Uncertainty",
    "text": "13.15 Prediction Uncertainty\nWe can use bootstrapping - resampling with replacement - to retrain the model on multiple slightly different training sets and see the impact on the predictions.\nThis gives us some idea of how ‘certain’ predictions are and how robust the model is to changes in the mix of training data.\n\nsplits = 30\ntrain_set = []\n\nfor i in range(splits):\n    train_set.append(X_train.join(y_train).sample(frac=1, replace=True))\n\n\n# Set up lists for models and probability predictions\nmodels = []\nresults  = []\naccuracies = []\n\nfor i in range(splits):\n\n    # Get X and y\n    X_train = train_set[i].drop('Survived', axis=1)\n    y_train = train_set[i]['Survived']\n\n    X_train_std, X_test_std = standardise_data(X_train, X_test)\n\n    # Define and train model; use different random seed for each model\n    model = LogisticRegression(random_state=42+i)\n    model.fit(X_train_std, y_train)\n    models.append(model)\n\n    # Get predicted probabilities and class\n    y_probs = model.predict_proba(X_test_std)[:,1]\n    y_pred = y_probs &gt; 0.5\n    results.append([y_probs])\n\n    # Show accuracy\n    accuracy = np.mean(y_pred == y_test)\n    accuracies.append(accuracy)\n\nresults = np.array(results)\nresults = results.T.reshape(-1, splits)\n\n\n13.15.0.1 Accuracy Across Models\n\nprint (f'Mean accuracy: {np.mean(accuracies):0.3f}')\n\nMean accuracy: 0.808\n\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.boxplot(accuracies, whis=999)\nax.set_ylabel('Model accuracy')\nax.axes.xaxis.set_ticklabels([]) # Remove xtick labels\nplt.show()\n\n\n\n\n\n\n\n\n\n\n13.15.1 Classification consensus\n\nclassification = results &gt;= 0.5\nconsensus = classification.sum(axis=1) &gt;= splits/2\nconsensus_accuracy = np.mean(consensus == y_test)\nprint (f'Consensus accuracy: {consensus_accuracy:0.3f}')\n\nConsensus accuracy: 0.812\n\n\n\n\n13.15.2 Variation in predictions\n\n13.15.2.1 Standard Deviation\n\nresults = results[np.mean(results,axis=1).argsort()]\nmean = np.mean(results,axis=1)\nstdev = np.std(results,axis=1)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.errorbar(range(len(mean)), mean, yerr=stdev, label='Standard Deviation', zorder=1)\nax.plot(mean, 'o', c='r', markersize=2, label = 'Mean probability', zorder=2)\nax.axes.xaxis.set_ticklabels([])\nax.set_xlabel('Passenger')\nax.set_ylabel('Probability of survival')\nax.set_xticks([])\nax.grid()\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.15.3 Standard Error\n\n# Sort by mean survival\nresults = results[np.mean(results,axis=1).argsort()]\n\nmean = np.mean(results,axis=1)\nstdev = np.std(results,axis=1)\nse = stdev / np.sqrt(splits)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.errorbar(range(len(mean)), mean, yerr=se, label='Standard Error', zorder=1)\nax.plot(mean, 'o', c='r', markersize=2, label = 'Mean probability', zorder=2)\nax.axes.xaxis.set_ticklabels([])\nax.set_xlabel('Passenger')\nax.set_ylabel('Probability of survival')\nax.set_xticks([])\nax.grid()\nax.legend()\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#repeat-with-xgboost",
    "href": "4g_logistic_regression_titanic_feature_importance_prediction_uncertainty.html#repeat-with-xgboost",
    "title": "13  Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)",
    "section": "13.16 Repeat with XGBoost",
    "text": "13.16 Repeat with XGBoost\n\n# Set up lists for models and probability predictions\nmodels = []\nresults  = []\naccuracies = []\n\nfor i in range(splits):\n\n    # Get X and y\n    X_train = train_set[i].drop('Survived', axis=1)\n    y_train = train_set[i]['Survived']\n\n    # Define and train model; use different random seed for each model\n    model = XGBClassifier(random_state=42+i)\n    model.fit(X_train, y_train)\n    models.append(model)\n\n    # Get predicted probabilities and class\n    y_probs = model.predict_proba(X_test)[:,1]\n    y_pred = y_probs &gt; 0.5\n    results.append([y_probs])\n\n    # Show accuracy\n    accuracy = np.mean(y_pred == y_test)\n    accuracies.append(accuracy)\n\nresults = np.array(results)\nresults = results.T.reshape(-1, splits)\n\n\nprint (f'Mean accuracy: {np.mean(accuracies):0.3f}')\n\nMean accuracy: 0.790\n\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.boxplot(accuracies, whis=999)\nax.set_ylabel('Model accuracy')\nax.axes.xaxis.set_ticklabels([]) # Remove xtick labels\nplt.show()\n\n\n\n\n\n\n\n\n\nclassification = results &gt;= 0.5\nconsensus = classification.sum(axis=1) &gt;= splits/2\nconsensus_accuracy = np.mean(consensus == y_test)\nprint (f'Consensus accuracy: {consensus_accuracy:0.3f}')\n\nConsensus accuracy: 0.798\n\n\n\n13.16.0.1 Variation in Predictions: Standard Deviation\n\nresults = results[np.mean(results,axis=1).argsort()]\nmean = np.mean(results,axis=1)\nstdev = np.std(results,axis=1)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.errorbar(range(len(mean)), mean, yerr=stdev, label='Standard Deviation', zorder=1)\nax.plot(mean, 'o', c='r', markersize=2, label = 'Mean probability', zorder=2)\nax.axes.xaxis.set_ticklabels([])\nax.set_xlabel('Passenger')\nax.set_ylabel('Probability of survival')\nax.set_xticks([])\nax.grid()\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n13.16.1 Standard Error\n\nmean = np.mean(results,axis=1)\nstdev = np.std(results,axis=1)\nse = stdev / np.sqrt(splits)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.errorbar(range(len(mean)), mean, yerr=se, label='Standard Error', zorder=1)\nax.plot(mean, 'o', c='r', markersize=2, label = 'Mean probability', zorder=2)\nax.axes.xaxis.set_ticklabels([])\nax.set_xlabel('Passenger')\nax.set_ylabel('Probability of survival')\nax.set_xticks([])\nax.grid()\nax.legend()\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Feature Importance and Prediction Uncertainty with Logistic Regression Models (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html",
    "href": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html",
    "title": "14  Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP",
    "section": "",
    "text": "14.1 Load data and fit model",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP</span>"
    ]
  },
  {
    "objectID": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html#load-data-and-fit-model",
    "href": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html#load-data-and-fit-model",
    "title": "14  Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP",
    "section": "",
    "text": "14.1.1 Load modules\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Import machine learning methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import PartialDependenceDisplay, permutation_importance\n\n# Import shap for shapley values\nimport shap\n\n# JavaScript Important for the interactive charts later on\nshap.initjs()\n\n\n\n\n\n\n14.1.2 Load data\nThe section below downloads pre-processed data, and saves it to a subfolder (from where this code is run). If data has already been downloaded that cell may be skipped.\nCode that was used to pre-process the data ready for machine learning may be found at: https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./datasets/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)\n\n\ndata = pd.read_csv('datasets/processed_data.csv')\n# Make all data 'float' type\ndata = data.astype(float)\n\n\n\n14.1.3 Divide into X (features) and y (labels)\nWe will separate out our features (the data we use to make a prediction) from our label (what we are truing to predict). By convention our features are called X (usually upper case to denote multiple features), and the label (survived or not) y.\n\n# Use `survived` field as y, and drop for X\ny = data['Survived'] # y = 'survived' column from 'data'\nX = data.drop('Survived', axis=1) # X = all 'data' except the 'survived' column\n\n# Drop PassengerId\nX.drop('PassengerId',axis=1, inplace=True)\n\n\n\n14.1.4 Divide into training and tets sets\nWhen we test a machine learning model we should always test it on data that has not been used to train the model. We will use sklearn’s train_test_split method to randomly split the data: 75% for training, and 25% for testing.\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    random_state=42,\n                                                    test_size=0.25)\n\n\n\n14.1.5 Fit Random Forest model\n\nmodel = RandomForestClassifier(n_estimators=100,\n                               n_jobs=-1,\n                               class_weight='balanced',\n                               random_state=42,\n                               max_depth=7)\n\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(class_weight='balanced', max_depth=7, n_jobs=-1,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(class_weight='balanced', max_depth=7, n_jobs=-1,\n                       random_state=42) \n\n\n\n\n14.1.6 Predict values and get probabilities of survival\nNow we can use the trained model to predict survival. We will test the accuracy of both the training and test data sets.\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Predict probabilities of survival\ny_prob_train = model.predict_proba(X_train)\ny_prob_test = model.predict_proba(X_test)\n\n\n\n14.1.7 Calculate accuracy\nIn this example we will measure accuracy simply as the proportion of passengers where we make the correct prediction. In a later notebook we will look at other measures of accuracy which explore false positives and false negatives in more detail.\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train:.2%}')\nprint (f'Accuracy of predicting test data = {accuracy_test:.2%}')\n\nAccuracy of predicting training data = 88.92%\nAccuracy of predicting test data = 81.17%",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP</span>"
    ]
  },
  {
    "objectID": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html#examining-the-model-importances",
    "href": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html#examining-the-model-importances",
    "title": "14  Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP",
    "section": "14.2 Examining the model importances",
    "text": "14.2 Examining the model importances\nAs we have used a tree-based model, we can easily pull out the feature importances using the MDI (mean decrease in impurity) approach, which are stored in model.feature_importances_.\n\nfeature_names = X.columns.tolist()\n\nfeature_importances_mdi = model.feature_importances_\n\nimportances = pd.DataFrame(index=feature_names)\nimportances['importance_mdi'] = feature_importances_mdi\nimportances['rank'] = importances['importance_mdi'].rank(ascending=False).values\n\n# View just the top 5\nimportances.sort_values('rank').head()\n\n\n\n\n\n\n\n\n\nimportance_mdi\nrank\n\n\n\n\nmale\n0.348332\n1.0\n\n\nFare\n0.148499\n2.0\n\n\nAge\n0.116853\n3.0\n\n\nPclass\n0.074443\n4.0\n\n\nCabinNumber\n0.064082\n5.0\n\n\n\n\n\n\n\n\nThe three most influential features are:\n\nmale\nFare\nage\n\nNote: random forest importances do not tell us anything about the direction of effect of features (as with random forests, the direction of effect may depend on the value oif other features).\n\nfeature_importances_mdi = pd.Series(feature_importances_mdi, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nfeature_importances_mdi.plot.barh(ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_xlabel(\"Mean decrease in impurity\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNote that because we’re using a random forest here, MDI is averaged across all the trees, so we can actually include error bars.\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n\nforest_importances = pd.Series(importances, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nforest_importances.plot.barh(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nNote that we could also use permutation feature importance here as an alternative approach.\nThis works with tree based models, but is actually a model-agnostic approach.\nLet’s take a quick look at the output of that function.\n\nresult_pfi = permutation_importance(\n    model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\nresult_pfi\n\n{'importances_mean': array([ 0.03183857,  0.02331839,  0.01076233,  0.00313901,  0.00089686,\n         0.00044843,  0.        , -0.00493274,  0.00538117, -0.00044843,\n         0.17399103, -0.00134529, -0.00044843,  0.00134529,  0.        ,\n        -0.00493274,  0.00358744, -0.00179372,  0.00044843, -0.00179372,\n         0.        ,  0.        ,  0.        , -0.00179372]),\n 'importances_std': array([0.00735481, 0.00998702, 0.00538117, 0.00403587, 0.00892365,\n        0.00509319, 0.        , 0.00372494, 0.00797147, 0.00678598,\n        0.01038192, 0.0063576 , 0.00134529, 0.00724462, 0.        ,\n        0.00134529, 0.00179372, 0.00219685, 0.00134529, 0.00219685,\n        0.        , 0.        , 0.        , 0.00410993]),\n 'importances': array([[ 0.02690583,  0.04035874,  0.02690583,  0.03139013,  0.03587444,\n          0.04484305,  0.03139013,  0.01793722,  0.02690583,  0.03587444],\n        [ 0.04035874,  0.02690583,  0.03587444,  0.00896861,  0.02690583,\n          0.02242152,  0.00896861,  0.02690583,  0.02242152,  0.01345291],\n        [ 0.00896861,  0.01345291,  0.00896861,  0.0044843 ,  0.01345291,\n          0.01793722,  0.00896861,  0.01345291,  0.01793722,  0.        ],\n        [ 0.0044843 ,  0.        ,  0.        ,  0.        ,  0.0044843 ,\n          0.0044843 ,  0.        ,  0.0044843 ,  0.        ,  0.01345291],\n        [ 0.01345291,  0.0044843 , -0.00896861,  0.        ,  0.0044843 ,\n         -0.0044843 ,  0.        ,  0.00896861, -0.01793722,  0.00896861],\n        [ 0.        , -0.0044843 ,  0.        , -0.0044843 ,  0.00896861,\n          0.00896861,  0.        , -0.0044843 ,  0.0044843 , -0.0044843 ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n        [ 0.        , -0.00896861, -0.0044843 , -0.0044843 , -0.0044843 ,\n         -0.0044843 , -0.0044843 , -0.0044843 ,  0.        , -0.01345291],\n        [ 0.00896861,  0.0044843 ,  0.01345291, -0.0044843 ,  0.01793722,\n         -0.00896861,  0.01345291,  0.0044843 ,  0.        ,  0.0044843 ],\n        [ 0.0044843 , -0.0044843 ,  0.0044843 ,  0.        ,  0.00896861,\n         -0.00896861,  0.00896861,  0.        , -0.00896861, -0.00896861],\n        [ 0.1838565 ,  0.1793722 ,  0.1838565 ,  0.17488789,  0.1838565 ,\n          0.16591928,  0.1838565 ,  0.16591928,  0.15246637,  0.16591928],\n        [ 0.        , -0.0044843 ,  0.        ,  0.0044843 ,  0.0044843 ,\n          0.        , -0.01793722,  0.        ,  0.0044843 , -0.0044843 ],\n        [ 0.        ,  0.        ,  0.        , -0.0044843 ,  0.        ,\n          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n        [ 0.        , -0.01345291,  0.00896861,  0.00896861,  0.        ,\n         -0.0044843 , -0.0044843 ,  0.00896861,  0.00896861,  0.        ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n        [-0.0044843 , -0.0044843 , -0.0044843 , -0.0044843 , -0.00896861,\n         -0.0044843 , -0.0044843 , -0.0044843 , -0.0044843 , -0.0044843 ],\n        [ 0.0044843 ,  0.0044843 ,  0.0044843 ,  0.        ,  0.        ,\n          0.0044843 ,  0.0044843 ,  0.0044843 ,  0.0044843 ,  0.0044843 ],\n        [ 0.        , -0.0044843 ,  0.        , -0.0044843 , -0.0044843 ,\n          0.        , -0.0044843 ,  0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n          0.        ,  0.0044843 ,  0.        ,  0.        ,  0.        ],\n        [-0.0044843 , -0.0044843 ,  0.        ,  0.        , -0.0044843 ,\n          0.        , -0.0044843 ,  0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n        [ 0.0044843 , -0.0044843 , -0.0044843 ,  0.        , -0.0044843 ,\n          0.        , -0.0044843 ,  0.        ,  0.0044843 , -0.00896861]])}\n\n\nNow let’s plot the output.\n\nfeature_importances_pfi = pd.Series(result_pfi.importances_mean, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nfeature_importances_pfi.plot.barh(yerr=result_pfi.importances_std, ax=ax)\nax.set_title(\"Feature importances using permutation on full model\")\nax.set_xlabel(\"Mean accuracy decrease\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLet’s add this to our table too.\n\nimportances['importance_pfi'] = feature_importances_pfi\nimportances['rank_pfi'] = importances['importance_pfi'].rank(ascending=False).values\nimportances.sort_values('rank_pfi').head()\n\n\n\n\n\n\n\n\n\nimportance_mdi\nrank\nimportance_pfi\nrank_pfi\n\n\n\n\nmale\n0.348332\n1.0\n0.173991\n1.0\n\n\nPclass\n0.074443\n4.0\n0.031839\n2.0\n\n\nAge\n0.116853\n3.0\n0.023318\n3.0\n\n\nSibSp\n0.049939\n6.0\n0.010762\n4.0\n\n\nCabinNumber\n0.064082\n5.0\n0.005381\n5.0",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP</span>"
    ]
  },
  {
    "objectID": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html#get-shapley-values",
    "href": "4g_pdp_ice_feature_importance_shap_table_tree_classification_titanic.html#get-shapley-values",
    "title": "14  Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP",
    "section": "14.3 Get Shapley values",
    "text": "14.3 Get Shapley values\nFirst we need to create a shap explainer object.\n\n\nexplainer = shap.Explainer(model, X_train)\nshap_values = explainer(X_test)\nshap_values_numeric = shap_values.values\n\nLook at the explainer object.\n\nexplainer\n\n&lt;shap.explainers._tree.TreeExplainer at 0x1c91bcf1b90&gt;\n\n\nLook at the shap_values variable.\n\ntype(shap_values)\n\nshap._explanation.Explanation\n\n\n\nshap_values\n\n.values =\narray([[[ 0.01060336, -0.01060336],\n        [ 0.02870872, -0.02870872],\n        [-0.00961641,  0.00961641],\n        ...,\n        [-0.00037315,  0.00037315],\n        [ 0.        ,  0.        ],\n        [ 0.00809963, -0.00809963]],\n\n       [[-0.01520491,  0.01520491],\n        [ 0.02459478, -0.02459477],\n        [-0.00241162,  0.00241162],\n        ...,\n        [-0.0004267 ,  0.0004267 ],\n        [ 0.        ,  0.        ],\n        [ 0.00473834, -0.00473834]],\n\n       [[ 0.01319181, -0.01319181],\n        [ 0.01232224, -0.01232224],\n        [-0.00262561,  0.00262561],\n        ...,\n        [-0.00049012,  0.00049012],\n        [ 0.        ,  0.        ],\n        [-0.00154064,  0.00154064]],\n\n       ...,\n\n       [[ 0.01926633, -0.01926633],\n        [ 0.0221672 , -0.0221672 ],\n        [-0.00363846,  0.00363846],\n        ...,\n        [-0.0004621 ,  0.00046211],\n        [ 0.        ,  0.        ],\n        [ 0.00050243, -0.00050243]],\n\n       [[-0.07960656,  0.07960655],\n        [ 0.01078217, -0.01078217],\n        [-0.01453268,  0.01453268],\n        ...,\n        [-0.00054181,  0.00054181],\n        [ 0.        ,  0.        ],\n        [ 0.0018481 , -0.0018481 ]],\n\n       [[ 0.02524583, -0.02524583],\n        [ 0.00986923, -0.00986923],\n        [-0.02022015,  0.02022015],\n        ...,\n        [-0.00034948,  0.00034948],\n        [ 0.        ,  0.        ],\n        [ 0.0036434 , -0.0036434 ]]])\n\n.base_values =\narray([[0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463],\n       [0.5630537, 0.4369463]])\n\n.data =\narray([[ 3., 28.,  1., ...,  0.,  0.,  1.],\n       [ 2., 31.,  0., ...,  0.,  0.,  1.],\n       [ 3., 20.,  0., ...,  0.,  0.,  1.],\n       ...,\n       [ 3., 28.,  0., ...,  0.,  0.,  1.],\n       [ 2., 24.,  0., ...,  0.,  0.,  1.],\n       [ 3., 18.,  1., ...,  0.,  0.,  1.]])\n\n\nLook at the shap_values_numeric variable.\n\nshap_values_numeric\n\narray([[[ 0.01060336, -0.01060336],\n        [ 0.02870872, -0.02870872],\n        [-0.00961641,  0.00961641],\n        ...,\n        [-0.00037315,  0.00037315],\n        [ 0.        ,  0.        ],\n        [ 0.00809963, -0.00809963]],\n\n       [[-0.01520491,  0.01520491],\n        [ 0.02459478, -0.02459477],\n        [-0.00241162,  0.00241162],\n        ...,\n        [-0.0004267 ,  0.0004267 ],\n        [ 0.        ,  0.        ],\n        [ 0.00473834, -0.00473834]],\n\n       [[ 0.01319181, -0.01319181],\n        [ 0.01232224, -0.01232224],\n        [-0.00262561,  0.00262561],\n        ...,\n        [-0.00049012,  0.00049012],\n        [ 0.        ,  0.        ],\n        [-0.00154064,  0.00154064]],\n\n       ...,\n\n       [[ 0.01926633, -0.01926633],\n        [ 0.0221672 , -0.0221672 ],\n        [-0.00363846,  0.00363846],\n        ...,\n        [-0.0004621 ,  0.00046211],\n        [ 0.        ,  0.        ],\n        [ 0.00050243, -0.00050243]],\n\n       [[-0.07960656,  0.07960655],\n        [ 0.01078217, -0.01078217],\n        [-0.01453268,  0.01453268],\n        ...,\n        [-0.00054181,  0.00054181],\n        [ 0.        ,  0.        ],\n        [ 0.0018481 , -0.0018481 ]],\n\n       [[ 0.02524583, -0.02524583],\n        [ 0.00986923, -0.00986923],\n        [-0.02022015,  0.02022015],\n        ...,\n        [-0.00034948,  0.00034948],\n        [ 0.        ,  0.        ],\n        [ 0.0036434 , -0.0036434 ]]])\n\n\n\n# Random forests seem to give us a slightly different output format,\n# so we adjust the line below to just bring back the results from the\n# positive class\nshap.plots.bar(shap_values[:, :, 1])\n\n\n\n\n\n\n\n\nAdd Shap values to coefficient table.\n\n# Calculate mean Shap value for each feature in training set\nimportances['mean_shap_values'] = np.mean(shap_values_numeric[:,:,1], axis=0)\n\n# Calculate mean absolute Shap value for each feature in training set\n# This will give us the average importance of each feature\nimportances['mean_abs_shap_values'] = np.mean(\n    np.abs(shap_values_numeric[:,:,1]),axis=0)\n\nimportances['rank_shap'] = importances['mean_abs_shap_values'].rank(ascending=False).values\nimportances.sort_values('rank_shap').head()\n\n\n\n\n\n\n\n\n\nimportance_mdi\nrank\nimportance_pfi\nrank_pfi\nmean_shap_values\nmean_abs_shap_values\nrank_shap\n\n\n\n\nmale\n0.348332\n1.0\n0.173991\n1.0\n0.009733\n0.163071\n1.0\n\n\nPclass\n0.074443\n4.0\n0.031839\n2.0\n0.011423\n0.043382\n2.0\n\n\nFare\n0.148499\n2.0\n0.000897\n9.0\n0.001480\n0.036203\n3.0\n\n\nAge\n0.116853\n3.0\n0.023318\n3.0\n-0.013086\n0.028130\n4.0\n\n\nCabinNumber\n0.064082\n5.0\n0.005381\n5.0\n0.005358\n0.023901\n5.0\n\n\n\n\n\n\n\n\nGet top 10 influential features by co-efficients for SHAP\n\n# Get top 10 features\nmdi_importance_top_10 = \\\n    importances.sort_values(by='importance_mdi', ascending=False).head(10).index\n\npfi_importance_top_10 = \\\n    importances.sort_values(by='importance_pfi', ascending=False).head(10).index\n\nshapley_top_10 = \\\n    importances.sort_values(\n    by='mean_abs_shap_values', ascending=False).head(10).index\n\n# Add to DataFrame\ntop_10_features = pd.DataFrame()\ntop_10_features['importances_mdi'] = mdi_importance_top_10.values\ntop_10_features['importances_pfii'] = pfi_importance_top_10.values\ntop_10_features['Shap'] = shapley_top_10.values\n\n# Display\ntop_10_features\n\n\n\n\n\n\n\n\n\nimportances_mdi\nimportances_pfii\nShap\n\n\n\n\n0\nmale\nmale\nmale\n\n\n1\nFare\nPclass\nPclass\n\n\n2\nAge\nAge\nFare\n\n\n3\nPclass\nSibSp\nAge\n\n\n4\nCabinNumber\nCabinNumber\nCabinNumber\n\n\n5\nSibSp\nCabinLetter_B\nCabinNumberImputed\n\n\n6\nParch\nParch\nEmbarked_S\n\n\n7\nCabinNumberImputed\nEmbarked_S\nAgeImputed\n\n\n8\nCabinLetter_missing\nFare\nEmbarked_C\n\n\n9\nCabinLetterImputed\nAgeImputed\nSibSp\n\n\n\n\n\n\n\n\nLet’s quickly compare our shap top 10 with the associated bar plot.\nWe can see a lot of overlap between the most import fatures as estimated by coefficients and those estimated using mean absolute Shapley values. But they are not identical.\nPlot comparison of Shapley and model coefficients:\n\nfig = plt.figure(figsize=(6,6))\nax = fig.add_subplot(111)\n\n# Plot points\nx = importances['importance_mdi']\ny = importances['mean_abs_shap_values']\n\nax.scatter(x, y)\nax.set_title('Shapley value vs model weight (coefficient) for each feature')\nax.set_ylabel('Mean absolute Shap value')\nax.set_xlabel('Feature importance')\n\nplt.grid()\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Explaining model predictions with PDPs, ICE plots, MDI, PFI and SHAP</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_classification_example_titanic.html",
    "href": "4g_shap_plots_tree_classification_example_titanic.html",
    "title": "15  SHAP with XGBoost (Titanic Dataset)",
    "section": "",
    "text": "16 SHAP - importance table\n# Calculate mean Shap value for each feature in training set\nimportances = pd.DataFrame()\nimportances['features'] = X.columns.tolist()\nimportances['mean_shap_values'] = np.mean(shap_values_numeric, axis=0)\n\n# Calculate mean absolute Shap value for each feature in training set\n# This will give us the average importance of each feature\nimportances['mean_abs_shap_values'] = np.mean(\n    np.abs(shap_values_numeric),axis=0)\n\nimportances['rank_shap'] = importances['mean_abs_shap_values'].rank(ascending=False).values\nimportances.sort_values('rank_shap').head()\n\n\n\n\n\n\n\n\n\nfeatures\nmean_shap_values\nmean_abs_shap_values\nrank_shap\n\n\n\n\n10\nmale\n0.047098\n1.872651\n1.0\n\n\n0\nPclass\n0.268006\n1.072309\n2.0\n\n\n4\nFare\n0.084978\n0.914949\n3.0\n\n\n1\nAge\n-0.325160\n0.846433\n4.0\n\n\n8\nCabinNumber\n0.126974\n0.344576\n5.0",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SHAP with XGBoost (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_classification_example_titanic.html#force-plots",
    "href": "4g_shap_plots_tree_classification_example_titanic.html#force-plots",
    "title": "15  SHAP with XGBoost (Titanic Dataset)",
    "section": "17.1 Force plots",
    "text": "17.1 Force plots\n\n# visualize the first prediction's explanation with a force plot\nshap.plots.force(shap_values[0])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# visualize all the predictions\nshap.plots.force(shap_values)\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SHAP with XGBoost (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_classification_example_titanic.html#dependence-plots",
    "href": "4g_shap_plots_tree_classification_example_titanic.html#dependence-plots",
    "title": "15  SHAP with XGBoost (Titanic Dataset)",
    "section": "17.2 Dependence Plots",
    "text": "17.2 Dependence Plots\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"Age\"])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:, \"male\"])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:, \"Pclass\"])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:, \"Fare\"])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values)\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"Fare\"], color=shap_values)\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"male\"], color=shap_values)\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"male\"], color=shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SHAP with XGBoost (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_classification_example_titanic.html#beeswarm",
    "href": "4g_shap_plots_tree_classification_example_titanic.html#beeswarm",
    "title": "15  SHAP with XGBoost (Titanic Dataset)",
    "section": "17.3 Beeswarm",
    "text": "17.3 Beeswarm\n\n# summarize the effects of all the features\nshap.plots.beeswarm(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SHAP with XGBoost (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_classification_example_titanic.html#violin",
    "href": "4g_shap_plots_tree_classification_example_titanic.html#violin",
    "title": "15  SHAP with XGBoost (Titanic Dataset)",
    "section": "17.4 Violin",
    "text": "17.4 Violin\n\n# summarize the effects of all the features\nshap.plots.violin(shap_values)\n\n\n\n\n\n\n\n\n\n17.4.1 Bar: Cohorts\n\nsex = [\"Women\" if shap_values[i, \"male\"].data == 0 else \"Men\" for i in range(shap_values.shape[0])]\nshap.plots.bar(shap_values.cohorts(sex).abs.mean(0))\n\n\n\n\n\n\n\n\nPlot the bars for an individual.\n\nshap.plots.bar(shap_values[1])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SHAP with XGBoost (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_classification_example_titanic.html#shap-probability-alternative",
    "href": "4g_shap_plots_tree_classification_example_titanic.html#shap-probability-alternative",
    "title": "15  SHAP with XGBoost (Titanic Dataset)",
    "section": "18.1 SHAP: Probability Alternative",
    "text": "18.1 SHAP: Probability Alternative\nRecalculate the SHAP values as changes in probability instead of log odds.\n\n# explain the model's predictions using SHAP\nexplainer_probability = shap.Explainer(model, X_train, model_output=\"probability\")\nshap_values_probability = explainer_probability(X_test)\n\nshap_values_probability\n\n.values =\narray([[-0.05238777, -0.13358663,  0.01009056, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.0511844 , -0.03143588,  0.0286387 , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.05579166, -0.05355721, -0.00460973, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-0.0458467 , -0.10278892,  0.01064102, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.14912559, -0.06840275,  0.00767626, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.04991312, -0.10697621,  0.00952425, ...,  0.        ,\n         0.        ,  0.        ]])\n\n.base_values =\narray([0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332, 0.38372332, 0.38372332,\n       0.38372332, 0.38372332, 0.38372332])\n\n.data =\narray([[ 3., 28.,  1., ...,  0.,  0.,  1.],\n       [ 2., 31.,  0., ...,  0.,  0.,  1.],\n       [ 3., 20.,  0., ...,  0.,  0.,  1.],\n       ...,\n       [ 3., 28.,  0., ...,  0.,  0.,  1.],\n       [ 2., 24.,  0., ...,  0.,  0.,  1.],\n       [ 3., 18.,  1., ...,  0.,  0.,  1.]])\n\n\n\n18.1.1 Beeswarm Plot: Probability\n\nshap.plots.beeswarm(shap_values_probability)\n\n\n\n\n\n\n\n\n\n18.1.1.1 Comparison with log odds plot\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,20))\n\nplt.sca(ax1) ## NEW\nshap.plots.beeswarm(shap_values, show=False)\nplt.title(\"Log Odds\")\n\n# Change to the second axis\nplt.sca(ax2) ## NEW\nshap.plots.beeswarm(shap_values_probability, show=False)\nplt.title(\"Probability\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n18.1.2 Waterfall Plot: Probability\nIf we pull out the predicted probability for this passenger, we can see that the predicted probability of class 0 (died) is 0.69, while the predicted probability of survival (class 1) is 0.301.\n\npd.DataFrame(model.predict_proba(X_test)).reset_index(drop=True).iloc[56]\n\n0    0.69867\n1    0.30133\nName: 56, dtype: float32\n\n\nThis matches what is now shown in the waterfall plot.\n\nshap.plots.waterfall(shap_values_probability[56])\n\n\n\n\n\n\n\n\n\npd.DataFrame(model.predict_proba(X_test)).reset_index(drop=True).iloc[115]\n\n0    0.97308\n1    0.02692\nName: 115, dtype: float32\n\n\n\nshap.plots.waterfall(shap_values_probability[115])\n\n\n\n\n\n\n\n\n\npd.DataFrame(model.predict_proba(X_test)).reset_index(drop=True).iloc[195]\n\n0    0.000583\n1    0.999417\nName: 195, dtype: float32\n\n\n\nshap.plots.waterfall(shap_values_probability[195])\n\n\n\n\n\n\n\n\n\n18.1.2.1 Comparison with log odds plot\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,20))\n\nplt.sca(ax1) ## NEW\nshap.plots.waterfall(shap_values[56], show=False)\nplt.title(\"Log Odds\")\n# Change to the second axis\nplt.sca(ax2) ## NEW\nshap.plots.waterfall(shap_values_probability[56], show=False)\nplt.title(\"Probability\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>SHAP with XGBoost (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "",
    "text": "17 Plots\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0])\n# visualize a later prediction's explanation\nshap.plots.waterfall(shap_values[7])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#force-plots",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#force-plots",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.1 Force plots",
    "text": "17.1 Force plots\n\n# visualize the first prediction's explanation with a force plot\nshap.plots.force(shap_values[0])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# visualize all the predictions\nshap.plots.force(shap_values)\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#dependence-plots",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#dependence-plots",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.2 Dependence Plots",
    "text": "17.2 Dependence Plots\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"age\"])\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"bmi\"], color=shap_values)\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"blood_sugar\"], color=shap_values)\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"age\"], color=shap_values[:, \"bmi\"])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"bmi\"], color=shap_values[:, \"age\"])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, \"age\"], color=shap_values[:, \"bmi\"])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#beeswarm",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#beeswarm",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.3 Beeswarm",
    "text": "17.3 Beeswarm\n\n# summarize the effects of all the features\nshap.plots.beeswarm(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#violin",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#violin",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.4 Violin",
    "text": "17.4 Violin\n\n# summarize the effects of all the features\nshap.plots.violin(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#bar",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#bar",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.5 Bar",
    "text": "17.5 Bar\n\nshap.plots.bar(shap_values)\n\n\n\n\n\n\n\n\n\n17.5.1 Splitting by cohorts\n\nbmi_category = [\"&lt;30\" if shap_values[i, \"bmi\"].data &lt; 30 else \"&gt;=30\" for i in range(shap_values.shape[0])]\nshap.plots.bar(shap_values.cohorts(bmi_category).abs.mean(0))\n\n\n\n\n\n\n\n\n\n\n17.5.2 Automatic cohort splitting\n\nshap.plots.bar(shap_values.cohorts(2).abs.mean(0))\n\n\n\n\n\n\n\n\nPlot the bars for an individual.\n\nshap.plots.bar(shap_values[0])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#heatmap",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#heatmap",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.6 Heatmap",
    "text": "17.6 Heatmap\n\nshap.plots.heatmap(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#decision",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#decision",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.7 Decision",
    "text": "17.7 Decision\n\nshap.plots.decision(\n    explainer.expected_value,\n    explainer.shap_values(diabetes_X_test),\n    feature_names=X.columns.tolist()\n    )\n\n\n\n\n\n\n\n\n\n17.7.1 Explanation plot for individual\n\nshap.plots.decision(\n    explainer.expected_value,\n    explainer.shap_values(diabetes_X_test)[0],\n    feature_names=X.columns.tolist()\n    )\n\n\n\n\n\n\n\n\n\nshap.plots.decision(\n    explainer.expected_value,\n    explainer.shap_values(diabetes_X_test)[104],\n    feature_names=X.columns.tolist()\n    )",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#group-difference",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#group-difference",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.8 Group Difference",
    "text": "17.8 Group Difference\n\nbmi_category_obese = np.array([False if shap_values[i, \"bmi\"].data &lt; 30 else True for i in range(shap_values.shape[0])])\n# bmi_category_obese\nshap.plots.group_difference(shap_values_numeric, bmi_category_obese, feature_names=X.columns.tolist())",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#modifying-plots",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#modifying-plots",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.9 Modifying plots",
    "text": "17.9 Modifying plots\n\nshap.plots.decision(\n    explainer.expected_value,\n    explainer.shap_values(X)[125],\n    feature_names=X.columns.tolist(),\n    show=False ## NEW\n    )\n\nplt.gcf() ## NEW\n\nax = plt.title(\"Here is my added title\") ## NEW",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_plots_tree_regression_example_diabetes.html#subplots",
    "href": "4g_shap_plots_tree_regression_example_diabetes.html#subplots",
    "title": "16  SHAP with regression trees (Diabetes Progression Dataset)",
    "section": "17.10 Subplots",
    "text": "17.10 Subplots\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,10)) ## NEW\n\n# sca is 'set current axis'\n# ensures next plot is put onto the axis we specify here - our first\n# of the two subplots\nplt.sca(ax1) ## NEW\nshap.plots.decision(\n    explainer.expected_value,\n    explainer.shap_values(diabetes_X_test)[104],\n    feature_names=X.columns.tolist(),\n    show=False ## NEW\n    )\n\n# Change to the second axis\nplt.sca(ax2) ## NEW\nshap.plots.decision(\n    explainer.expected_value,\n    explainer.shap_values(diabetes_X_test)[15],\n    feature_names=X.columns.tolist(),\n    show=False ## NEW\n    )\n\n# note that the use of %matplotlib inline at the start has led to the figsize parameter being\n# partly ignored",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>SHAP with regression trees (Diabetes Progression Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html",
    "href": "4g_shap_neural_nets.html",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "",
    "text": "17.1 The neural network unit - a neuron or perceptron\nThe building block of a neural network is a neuron, which is essentially the same as the ‘perceptron’ described by Frank Rosenblatt in 1958.\nThe neuron, or perceptron, takes inputs X and weights W (each individual input has a weight; a bias weight is also introduced by creating a dummy input with value 1). The neuron sums the input multiplied by the weight and passes the output to an activation function. The simplest activation function is a step function, whereby if the output is &gt;0 the output of the activation function is 1, otherwise the output is 0.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#neural-networks",
    "href": "4g_shap_neural_nets.html#neural-networks",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.2 Neural networks",
    "text": "17.2 Neural networks\nHaving understood a neuron - which calculates the weighted sum of its inputs and passes it through an activation function, neural networks are easy(ish)!\nThey are ‘just’ a network of such neurons, where the output of one becomes one of the inputs to the neurons in the next layer.\nThis allows any complexity of function to be mimicked by a neural network (so long as the network includes a non-linear activation function, like ReLU - see below).\nNote the output layer may be composed of a single neuron, to predict a single value or single probability, or may be multiple neurons, to predict multiple values or multiple probabilities.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#activation-functions",
    "href": "4g_shap_neural_nets.html#activation-functions",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.3 Activation functions",
    "text": "17.3 Activation functions\nEach neuron calculates the weighted sum of its inputs and passes that sum to an activation function. The two simplest functions are:\n\nLinear: The weighted output is passed forward with no change.\nStep: The output of the activation function is 0 or 1 depending on whether a threshold is reached.\n\nOther common activation functions are:\n\nSigmoid: Scales output 0-1 using a logistic function. Note that our simple single perceptron becomes a logistic regression model if we use a sigmoid activation function. The sigmoid function is often used to produce a probability output at the final layer.\ntanh: Scales output -1 to 1. Commonly used in older neural network models. Not commonly used now.\nReLU (rectifying linear unit): Simply converts all negative values to zero, and leaves positive values unchanged. This very simple method is very common in deep neural networks, and is sufficient to allow networks to model non-linear functions.\nLeaky ReLU and Exponential Linear Unit (ELU): Common modifications to ReLU that do not have such a hard constraint on negative inputs, and can be useful if we run into the Dying ReLU problem (in which - typically due to high learning rates - our weights are commonly set to negative values, leading to them effectively being switched off (set to 0) under ReLU). Try them out as replacements to ReLU.\nMaxout: A generalised activation function that can model a complex non-linear activation function.\nSoftMax: SoftMax is the final layer to use if you wish to normalise probability outputs from a network which has multiple class outputs (e.g. you want the total of your probabilities for “this is dog”, “this is a cat”, “this is a fish” etc to add up to 1).",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#loss-functions",
    "href": "4g_shap_neural_nets.html#loss-functions",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.4 Loss functions",
    "text": "17.4 Loss functions\nLoss functions are critical to neural networks as they provide the measure by which the neural network is in error, allowing modification of the network to reduce error.\nThe most common loss functions are:\n\nMean Squared Error Loss: Common loss function for regression (predicting values rather than class).\nCross Entropy Loss: Common loss function for classification. Binary Cross Entropy Loss is used when the output is a binary classifier (like survive/die in the Titanic model).",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#how-do-neural-networks-learn-backpropagation-and-optimisation",
    "href": "4g_shap_neural_nets.html#how-do-neural-networks-learn-backpropagation-and-optimisation",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.5 How do neural networks learn? Backpropagation and optimisation",
    "text": "17.5 How do neural networks learn? Backpropagation and optimisation\nBackpropagation is the process by which the final loss is distributed back through the network, allowing each weight to be updated in proportion to its contribution to the final error.\nFor more on backpropagation see: https://youtu.be/Ilg3gGewQ5U\nFor deeper maths on backpropagation see: https://youtu.be/tIeHLnjs5U8\nOptimisation is the step-wise process by which weights are updated. The basic underlying method, gradient descent, is that weights are adjusted in the direction that improves fit, and that weights are adjust more when the gradient (how much the output changes with each unit change to the weight) is higher.\nCommon optimisers used are:\n\nStochastic gradient descent: Updates gradients based on single samples. Can be inefficient, so can be modified to use gradients based on a small batch (e.g. 8-64) of samples. Momentum may also be added to avoid becoming trapped in local minima.\nRMSprop: A ‘classic’ benchmark optimiser. Adjusts steps based on a weighted average of all weight gradients.\nAdam: The most common optimiser used today. Has complex adaptive momentum for speeding up learning.\n\nFor more on optimisers see: https://youtu.be/mdKjMPmcWjY",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#training-a-neural-network---the-practicalities",
    "href": "4g_shap_neural_nets.html#training-a-neural-network---the-practicalities",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.6 Training a neural network - the practicalities",
    "text": "17.6 Training a neural network - the practicalities\nThe training process of a neural network consists of three general phases which are repeated across all the data. All of the data is passed through the network multiple times (the number of iterations, which may be as few as 3-5 or may be 1000+) until all of the data has been fed forward and backpropogated - this then represents an “Epoch”. The three phases of an iteration are :\n\nPass training X data to the network and predict y\nCalculate the ‘loss’ (error) between the predicted and observed (actual) values of y\nBackpropagate the loss and update the weights (the job of the optimiser).\n\nThe learning is repeated until maximum accuracy is achieved (but keep an eye on accuracy of test data as well as training data as the network may develop significant over-fitting to training data unless steps are taken to offset the potential for over-fitting, such as use of ‘drop-out’ layers described below).",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#architectures",
    "href": "4g_shap_neural_nets.html#architectures",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.7 Architectures",
    "text": "17.7 Architectures\nThe most common fully connected architecture design is to have the same number of neurons in each layer, and adjust that number and the number of layers. This makes exploring the size of the neural net relatively easy (if sometimes slow).\nAs a rough guide - the size of the neural net should be increased until it over-fits data (increasing accuracy of training data with reducing accuracy of test data), and then use a form of regularisation to reduce the over-fitting (we will go through this process below).\nSome common architecture designs, which may be mixed in a single larger network, are:\n\nFully connected: The output of each neuron goes to all neurons in the next layer.\nConvolutional: Common in image analysis. Small ‘mini-nets’ that look for patterns across the data - like a ‘sliding window’, but that can look at the whole picture at the same time. May also be used, for example, in time series to look for fingerprints of events anywhere in the time series.\nRecurrent: Introduce the concept of some (limited) form of memory into the network - at any one time a number of input steps are affecting the network output. Useful, for example, in sound or video analysis.\nTransformers: Sequence-to-sequence architecture. Convert sequences to sequences (e.g. translation). Big in Natural Language Processing - we’ll cover them in the NLP module.\nEmbedding: Converts a categorical value to a vector of numbers, e.g. word-2-vec converts words to vectors such that similar meaning words are positioned close together.\nEncoding: Reduce many input features to fewer. This ‘compresses’ the data. De-coding layers may convert back to the original data.\nGenerative: Rather than regression, or classification, generative networks output some form of synthetic data (such as fake images; see https://www.thispersondoesnotexist.com/).\n\nFor the kind of classification problem we’re looking at here, a Fully Connected Neural Network is the most commonly used architecture now, and typically you keep all layers the same size (the same number of Neurons) apart from your output layer. This makes it easy to test different sizes of network.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#additional-resources",
    "href": "4g_shap_neural_nets.html#additional-resources",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.8 Additional resources",
    "text": "17.8 Additional resources\nAlso see the excellent introductory video (20 minutes) from 3brown1blue: https://youtu.be/aircAruvnKk",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#lets-go",
    "href": "4g_shap_neural_nets.html#lets-go",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.9 Let’s go !!!!!!!!!!!!!",
    "text": "17.9 Let’s go !!!!!!!!!!!!!\nIn this first cell, we’re going to be a bit naughty, and turn off warnings (such as “you’re using an out-of-date version of this” etc). This will make the notebook cleaner and easier to interpret as you learn this, but in real-world work you shouldn’t really do this unless you know what you’re doing. But we’ll do it here because we do (I think).\nDon’t forget to select the tf_hsma environment when you run the first cell. If you’re prompted that you need to install the ipykernel, click that you want to do it.\n\n# Turn warnings off to keep notebook tidy\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#load-modules",
    "href": "4g_shap_neural_nets.html#load-modules",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.10 Load modules",
    "text": "17.10 Load modules\nFirst we need to import the packages we’re going to use. The first three (MatPlotLib, NumPy and Pandas) are the stuff we use in pretty much everything in data science. From SciKitLearn, we import functions to automatically split our data into training and test data (as we did for the Logistic Regression example) and to min-max normalise our data (remember we said that normalising our data is typical with Neural Networks (“Neural Networks are Normal”), and standardising our data - what we did last time - is typical with Logistic Regression). Remember, when we normalise we’ll scale all our feature values so they fall between 0 and 1.\nThen, we import a load of things we’ll need from TensorFlow (and particularly Keras). TensorFlow is the Neural Network architecture developed by Google, but the interface (API) for TensorFlow is not easy to use. So instead, we use Keras, which sits on top of TensorFlow, and allows us to interact with TensorFlow in a much more straightforward way. Don’t worry about what each of things that we import are at this stage - we’ll see them in use as we move through the notebook.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# sklearn for pre-processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# TensorFlow sequential model\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#download-data-if-not-previously-downloaded",
    "href": "4g_shap_neural_nets.html#download-data-if-not-previously-downloaded",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.11 Download data if not previously downloaded",
    "text": "17.11 Download data if not previously downloaded\nThis cell downloads the Titanic data that we’re going to use. You don’t need to do this if you’ve already downloaded the data, but if you’re unsure, run the cell anyway (it takes seconds!).\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#define-function-to-scale-data",
    "href": "4g_shap_neural_nets.html#define-function-to-scale-data",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.12 Define function to scale data",
    "text": "17.12 Define function to scale data\nIn neural networks it is common to normalise (scale input data 0-1) rather than use standardise (subtracting mean and dividing by standard deviation) each feature. As with the Logistic Regression example, we’ll set up a function here that we can call whenever we want to do this (the only difference being that in the Logistic Regression example we standardised our data, rather than normalising it).\n\ndef scale_data(X_train, X_test):\n    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n\n    # Initialise a new scaling object for normalising input data\n    sc = MinMaxScaler()\n\n    # Apply the scaler to the training and test sets\n    train_sc = sc.fit_transform(X_train)\n    test_sc = sc.fit_transform(X_test)\n\n    return train_sc, test_sc",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#load-data",
    "href": "4g_shap_neural_nets.html#load-data",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "17.13 Load data",
    "text": "17.13 Load data\nWe’re going to load up and do a bit of initial prep on our data, much as we did before for the Logistic Regression. We’re going to load our data (which is stored in a .csv file) into a Pandas DataFrame. We’ll convert all the data into floating point numbers so everything is consistent. We’ll drop the Passenger ID column, as that isn’t part of the original data, and we don’t want the machine to learn anything from this.\nThen we define our input (X) and output (y) data. Remember we’re trying to predict y from X. X is all of our columns (features) except for the “Survived” column (which is our label - the thing we’re trying to predict). The axis=1 argument tells Pandas we’re referring to columns when we tell it to drop stuff.\nWe also set up NumPy versions of our X and y data - this is a necessary step if we were going to do k-fold splits (remember we talked about those in the last session - it’s where we split up our data into training and test sets in multiple different ways to try to avoid biasing the data) as it requires the data to be in NumPy arrays, not Pandas DataFrames. We’re not actually going to use k-fold splits in this workbook, but we’ll still go through the step of getting the data into the right format for when we do. Because, in real world applications, you should use k-fold splits.\n\ndata = pd.read_csv('data/processed_data.csv')\n# Make all data 'float' type\ndata = data.astype(float)\ndata.drop('PassengerId', inplace=True, axis=1)\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'\n# Convert to NumPy as required for k-fold splits\nX_np = X.values\ny_np = y.values",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#show-summary-of-the-model-structure",
    "href": "4g_shap_neural_nets.html#show-summary-of-the-model-structure",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "18.1 Show summary of the model structure",
    "text": "18.1 Show summary of the model structure\nHere we will create an arbitrary model (that we won’t use) with 10 input features, just to show the function we wrote above being used and so you can see how you can use the summary() function of a model to see an overview of the structure of it.\nWe can see what the layers are in order. Remember we have five main layers in total (input, 3 x hidden, output) but you won’t see the input layer here. When you run the below cell, you should see three hidden layers (each with a dropout layer immediately after) with 128 neurons in each layer followed by a final output layer with just one neuron. You’ll also see that there are over 34,500 parameters (weights) that it needs to optimise, just in a very simple network like this on a very small dataset with 10 features. Now you can see why they’re so complicated (and magical!).\n\nmodel = make_net(10)\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 128)            │         1,408 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 128)            │        16,512 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 1)              │           129 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 34,561 (135.00 KB)\n\n\n\n Trainable params: 34,561 (135.00 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#split-and-scale-data",
    "href": "4g_shap_neural_nets.html#split-and-scale-data",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "18.2 Split and Scale data",
    "text": "18.2 Split and Scale data\nNow, as we did before with the Logistic Regression, we split our data into training and test sets. We’ve got 25% carved off for our test set. But what is this random_state=42 thing? So, pulling back the curtain now, but the random numbers we tend to generate in our computers are not strictly random. They are pseudo-random - they use complex algorithms to generate numbers that appear random (and which are good enough for the vast majority of things you will ever do). Because they are pseudo-random, this means that we can fix the random number generator to use a pre-defined seed - a number that feeds into the algorithm which will ensure we always get the same random numbers being generated. This can be useful if we’re 1) teaching, and you want everyone to get the same thing, or 2) validating our outputs whilst we build our model. Since we’re doing both of those things here, we use a fixed seed.\nBut why the number 42? Those of you who have read, watched and / or listened to The Hitchiker’s Guide to the Galaxy will know why. Those that haven’t, go off and read, watch or listen to it and then you’ll get the “joke” (Computer Scientists love doing stuff like this..)\nOnce we’ve established our training and testing data, we scale the data by normalising it, using the function we wrote earlier (which uses min-max normalisation).\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_np, y_np, test_size = 0.25, random_state=42)\n\n# Scale X data\nX_train_sc, X_test_sc = scale_data(X_train, X_test)\n\nLet’s just have a look at the scaled data for the first two records (passengers) in our input data. We should see that all of the feature values have scaled between 0 and 1.\n\nX_train_sc[0:2]\n\narray([[0.        , 0.34656949, 0.        , 0.        , 0.05953204,\n        1.        , 0.        , 0.        , 0.828125  , 0.        ,\n        1.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        , 1.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        ],\n       [1.        , 0.30887158, 0.        , 0.        , 0.01376068,\n        0.        , 0.        , 1.        , 0.        , 1.        ,\n        1.        , 0.        , 0.        , 1.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 1.        ]])\n\n\nWe can compare this with the unscaled data for the same two passengers to see the original values.\n\nX_train[0:2]\n\narray([[  1.  ,  28.  ,   0.  ,   0.  ,  30.5 ,   1.  ,   0.  ,   0.  ,\n        106.  ,   0.  ,   1.  ,   0.  ,   0.  ,   1.  ,   0.  ,   0.  ,\n          0.  ,   1.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n       [  3.  ,  25.  ,   0.  ,   0.  ,   7.05,   0.  ,   0.  ,   1.  ,\n          0.  ,   1.  ,   1.  ,   0.  ,   0.  ,   1.  ,   0.  ,   0.  ,\n          0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   1.  ]])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#function-to-calculate-accuracy",
    "href": "4g_shap_neural_nets.html#function-to-calculate-accuracy",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "18.3 Function to calculate accuracy",
    "text": "18.3 Function to calculate accuracy\nWe’re now going to write a little function that will report the accuracy of the model on the training set and the test set. This will help us assess how well our model is performing. We pass into the function the model, the (normalised) input data for both the training and test sets, and the output data for both the training and test sets.\nThe function uses the predict function of the model to grab out the probability predictions based on the input data for the training set. We specify that a classification of 1 (in the case of Titanic, this means “survived”) should be made if the probability predicted is greater than 0.5. Then we “flatten” the data to get it in the right shape (because it comes out as a complex shape - a tensor. Don’t worry about this. Just imagine a blob of data, and we squish it so we can read it). Then we use y_pred_train == y_train to return boolean True values for each time where the prediction (survived or died) matched the real answer, and take the average of those matches (that effectively gives us accuracy - what proportion of times did prediction match real answer). (Python interprets Trues and Falses as 1s and 0s, in case you’re wondering how that works!).\nThen we do the same as above but for the test set.\nFinally we print the accuracy on both the training and test sets.\n\ndef calculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test):\n    \"\"\"Calculate and print accuracy of training and test data fits\"\"\"\n\n    ### Get accuracy of fit to training data\n    probability = model.predict(X_train_sc)\n    y_pred_train = probability &gt;= 0.5\n    y_pred_train = y_pred_train.flatten()\n    accuracy_train = np.mean(y_pred_train == y_train)\n\n    ### Get accuracy of fit to test data\n    probability = model.predict(X_test_sc)\n    y_pred_test = probability &gt;= 0.5\n    y_pred_test = y_pred_test.flatten()\n    accuracy_test = np.mean(y_pred_test == y_test)\n\n    # Show acuracy\n    print (f'Training accuracy {accuracy_train:0.3f}')\n    print (f'Test accuracy {accuracy_test:0.3f}')\n\nWe’ll also write a little function to plot the accuracy on the training set and the test set over time. Keras keeps a “history” (which is a dictionary) of the learning which allows us to do this easily. It’s quite useful to plot the performance over time, as it allows us to look for indications as to when the model is becoming overfitted etc.\nIn our function, we’ll grab out the values from the passed in history dictionary, and then plot them using standard matplotlib plotting methods.\n\ndef plot_training(history_dict):\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n    epochs = range(1, len(acc_values) + 1)\n\n    fig, ax = plt.subplots()\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Accuracy')\n\n    ax.plot(epochs, acc_values, color='blue', label='Training acc')\n    ax.plot(epochs, val_acc_values, color='red', label='Test accuracy')\n    ax.set_title('Training and validation accuracy')\n\n    ax.legend()\n\n    fig.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#run-the-model",
    "href": "4g_shap_neural_nets.html#run-the-model",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "18.4 Run the model",
    "text": "18.4 Run the model\nWe’ve now defined everything that will allow us to build the model. So we’ll now define the model we want and train it!\nTo work out how many features we need (which we then need to pass into the make_net function we defined earlier), we can simply look at the number of columns in our X (input) data (where we’ve removed the ‘label’ (output) column). We can grab this from the standardised training data, by looking at index 1 of the shape tuple (index 0 would be rows (passengers in the Titanic data), and index 1 would be columns). We can see this if we run the code X_train_sc.shape. Try it yourself (just insert a code cell below this markdown cell)! You should see there are 668 rows, and 24 columns. Therefore, we’ve got 668 passengers and 24 features.\nNext we call our make_net function, passing in the number of features we calculated above. This will create our Neural Network. As we’ve passed in nothing else, we’ll have defaults for the rest of the network - 3 hidden layers, 128 neurons per layer, a learning rate of 0.003 and no dropout (although, we will still have dropput layers, they just won’t do anything).\nThen, we fit (train) the model. To do that, we call the fit method of the model, and pass it :\n\nthe standardised training data\nthe output (label) data\nthe number of epochs (training generations - full passes of all of the data through the network). Initially, we want enough epochs that we see overfitting start to happen (the training accuracy starts to plateau) because then we know we’ve trained “enough” (albeit a bit too much) and can then look to reduce it back a bit\nthe batch size (how much data we shunt through the network at once. Yann LeCun (French Computer Scientist) advises “Friends shouldn’t let friends use batch sizes of more than 32”. But we will here… :))\nthe data we want to use as our “validation data” (which we use to fine tune the parameters of the model). Keras will check performance on this validation data. Here we just use our test set, but you should really have a separate “validation set” that you’d use whilst tuning the model.\nwhether we want to see all the things it’s doing as it’s learning. If we set verbose to 0, all of this will be hidden (keeping things tidier), but as we’re experimenting with our model, it’s a good idea to set verbose to 1 so we can monitor what it’s doing.\n\nYou’ll also see that we not only call model.fit but we store the output of that function in a variable called history. This allows us to access all the useful information that keras was keeping track of whilst the model was training. We’ll use that later.\nNote - when you run the cell below, the model will be built and then start training. How long this takes will depend on your computer specs, including whether you have a CUDA-enabled GPU (if you’re running locally) or your priority in the queue for cloud computing (if you’re running this on CoLab).\nDan has a very fast computer with a high performance CUDA-enabled GPU, and the below (with 250 epochs) takes about 6 seconds on the GPU and about 11 seconds on the CPU. It might take a little while longer on yours - don’t worry, as long as you can see it moving through the epochs.\nFor each epoch, you’ll see various information, including the epoch number, the loss (error) that’s been calculated in that epoch (for both the training and validation data), and the accuracy (for both the training and validation data). You should see loss gradually reduce, and accuracy gradually increase. But you’ll likely see that training accuracy tends to keep getting better (before it reaches a plateau) and validation accuracy gets better but then starts to drop a bit. That’s a sign of overfitting (our model’s become increasingly brilliant for the training data, but starting to get increasingly rubbish at being more generally useful).\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n\n### Train model (and store training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=1)\n\nEpoch 1/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 1s 15ms/step - accuracy: 0.5220 - loss: 0.6346 - val_accuracy: 0.7668 - val_loss: 0.4853\nEpoch 2/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8016 - loss: 0.4891 - val_accuracy: 0.7623 - val_loss: 0.4980\nEpoch 3/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.7624 - loss: 0.4921 - val_accuracy: 0.7937 - val_loss: 0.4633\nEpoch 4/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.7927 - loss: 0.4452 - val_accuracy: 0.8072 - val_loss: 0.4384\nEpoch 5/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8404 - loss: 0.3911 - val_accuracy: 0.8027 - val_loss: 0.4779\nEpoch 6/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8139 - loss: 0.4190 - val_accuracy: 0.8072 - val_loss: 0.4446\nEpoch 7/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8292 - loss: 0.4112 - val_accuracy: 0.8161 - val_loss: 0.4487\nEpoch 8/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8325 - loss: 0.4004 - val_accuracy: 0.7982 - val_loss: 0.4917\nEpoch 9/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8229 - loss: 0.3964 - val_accuracy: 0.8117 - val_loss: 0.4634\nEpoch 10/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8224 - loss: 0.3993 - val_accuracy: 0.8117 - val_loss: 0.4736\nEpoch 11/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8318 - loss: 0.3887 - val_accuracy: 0.7982 - val_loss: 0.4758\nEpoch 12/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.8422 - loss: 0.3642 - val_accuracy: 0.8072 - val_loss: 0.4650\nEpoch 13/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8409 - loss: 0.3764 - val_accuracy: 0.8072 - val_loss: 0.5091\nEpoch 14/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8451 - loss: 0.3602 - val_accuracy: 0.8161 - val_loss: 0.4979\nEpoch 15/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8572 - loss: 0.3608 - val_accuracy: 0.8206 - val_loss: 0.4955\nEpoch 16/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8596 - loss: 0.3429 - val_accuracy: 0.8206 - val_loss: 0.5212\nEpoch 17/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8340 - loss: 0.3670 - val_accuracy: 0.8161 - val_loss: 0.5113\nEpoch 18/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8427 - loss: 0.3578 - val_accuracy: 0.8161 - val_loss: 0.5183\nEpoch 19/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8450 - loss: 0.3675 - val_accuracy: 0.8072 - val_loss: 0.5034\nEpoch 20/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8528 - loss: 0.3511 - val_accuracy: 0.8027 - val_loss: 0.6194\nEpoch 21/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8702 - loss: 0.3266 - val_accuracy: 0.8386 - val_loss: 0.5552\nEpoch 22/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8721 - loss: 0.3260 - val_accuracy: 0.8206 - val_loss: 0.5928\nEpoch 23/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8455 - loss: 0.3448 - val_accuracy: 0.8161 - val_loss: 0.5578\nEpoch 24/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8522 - loss: 0.3513 - val_accuracy: 0.8161 - val_loss: 0.6054\nEpoch 25/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8382 - loss: 0.3779 - val_accuracy: 0.8206 - val_loss: 0.5552\nEpoch 26/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8505 - loss: 0.3378 - val_accuracy: 0.8117 - val_loss: 0.6071\nEpoch 27/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8542 - loss: 0.3427 - val_accuracy: 0.8072 - val_loss: 0.6401\nEpoch 28/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8840 - loss: 0.3062 - val_accuracy: 0.8206 - val_loss: 0.5816\nEpoch 29/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8710 - loss: 0.3367 - val_accuracy: 0.8206 - val_loss: 0.7158\nEpoch 30/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8831 - loss: 0.2982 - val_accuracy: 0.8161 - val_loss: 0.6560\nEpoch 31/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.8776 - loss: 0.3194 - val_accuracy: 0.8251 - val_loss: 0.6616\nEpoch 32/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8638 - loss: 0.3288 - val_accuracy: 0.8072 - val_loss: 0.7531\nEpoch 33/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8729 - loss: 0.3286 - val_accuracy: 0.8072 - val_loss: 0.7418\nEpoch 34/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8748 - loss: 0.3093 - val_accuracy: 0.8386 - val_loss: 0.8018\nEpoch 35/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8796 - loss: 0.2952 - val_accuracy: 0.8072 - val_loss: 0.7556\nEpoch 36/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8800 - loss: 0.2931 - val_accuracy: 0.8072 - val_loss: 0.8790\nEpoch 37/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8922 - loss: 0.2733 - val_accuracy: 0.8117 - val_loss: 0.7454\nEpoch 38/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8723 - loss: 0.2994 - val_accuracy: 0.8161 - val_loss: 0.7706\nEpoch 39/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8859 - loss: 0.2816 - val_accuracy: 0.7758 - val_loss: 1.0022\nEpoch 40/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8622 - loss: 0.3264 - val_accuracy: 0.8161 - val_loss: 0.8016\nEpoch 41/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8879 - loss: 0.2798 - val_accuracy: 0.8161 - val_loss: 0.8013\nEpoch 42/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8959 - loss: 0.2523 - val_accuracy: 0.8251 - val_loss: 0.8324\nEpoch 43/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9060 - loss: 0.2501 - val_accuracy: 0.8206 - val_loss: 0.8456\nEpoch 44/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8749 - loss: 0.3054 - val_accuracy: 0.7937 - val_loss: 0.7708\nEpoch 45/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8741 - loss: 0.3065 - val_accuracy: 0.8251 - val_loss: 0.9630\nEpoch 46/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8937 - loss: 0.2790 - val_accuracy: 0.7892 - val_loss: 1.0599\nEpoch 47/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8729 - loss: 0.3031 - val_accuracy: 0.8341 - val_loss: 0.8219\nEpoch 48/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8647 - loss: 0.3387 - val_accuracy: 0.8072 - val_loss: 0.8795\nEpoch 49/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8852 - loss: 0.3007 - val_accuracy: 0.8117 - val_loss: 0.9999\nEpoch 50/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9068 - loss: 0.2517 - val_accuracy: 0.7982 - val_loss: 0.9568\nEpoch 51/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8747 - loss: 0.2841 - val_accuracy: 0.7892 - val_loss: 0.9607\nEpoch 52/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8853 - loss: 0.2668 - val_accuracy: 0.8161 - val_loss: 1.1279\nEpoch 53/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9008 - loss: 0.2482 - val_accuracy: 0.7982 - val_loss: 1.1420\nEpoch 54/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8862 - loss: 0.2634 - val_accuracy: 0.7848 - val_loss: 1.0028\nEpoch 55/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8881 - loss: 0.2657 - val_accuracy: 0.8161 - val_loss: 0.9148\nEpoch 56/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8965 - loss: 0.2456 - val_accuracy: 0.7982 - val_loss: 1.0872\nEpoch 57/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8855 - loss: 0.2954 - val_accuracy: 0.8072 - val_loss: 1.1653\nEpoch 58/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8852 - loss: 0.2744 - val_accuracy: 0.8117 - val_loss: 1.1293\nEpoch 59/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9056 - loss: 0.2485 - val_accuracy: 0.8117 - val_loss: 1.1251\nEpoch 60/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8960 - loss: 0.2603 - val_accuracy: 0.8117 - val_loss: 1.1403\nEpoch 61/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8868 - loss: 0.2643 - val_accuracy: 0.7848 - val_loss: 1.1178\nEpoch 62/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9005 - loss: 0.2566 - val_accuracy: 0.7848 - val_loss: 1.2160\nEpoch 63/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8909 - loss: 0.2759 - val_accuracy: 0.8072 - val_loss: 1.0456\nEpoch 64/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8890 - loss: 0.2723 - val_accuracy: 0.7937 - val_loss: 1.1907\nEpoch 65/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8978 - loss: 0.2492 - val_accuracy: 0.7803 - val_loss: 1.3086\nEpoch 66/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8926 - loss: 0.2685 - val_accuracy: 0.8072 - val_loss: 0.9547\nEpoch 67/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.8921 - loss: 0.2715 - val_accuracy: 0.8072 - val_loss: 1.1706\nEpoch 68/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.9096 - loss: 0.2271 - val_accuracy: 0.7758 - val_loss: 1.3196\nEpoch 69/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9112 - loss: 0.2482 - val_accuracy: 0.8027 - val_loss: 1.0041\nEpoch 70/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.8938 - loss: 0.2669 - val_accuracy: 0.7937 - val_loss: 1.2029\nEpoch 71/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.8975 - loss: 0.2477 - val_accuracy: 0.7937 - val_loss: 1.2430\nEpoch 72/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.9042 - loss: 0.2454 - val_accuracy: 0.8027 - val_loss: 1.2154\nEpoch 73/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.9118 - loss: 0.2310 - val_accuracy: 0.7937 - val_loss: 1.3900\nEpoch 74/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.8956 - loss: 0.2605 - val_accuracy: 0.8296 - val_loss: 1.0950\nEpoch 75/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.8945 - loss: 0.2625 - val_accuracy: 0.8117 - val_loss: 1.1707\nEpoch 76/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8832 - loss: 0.2599 - val_accuracy: 0.7937 - val_loss: 1.4299\nEpoch 77/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9098 - loss: 0.2230 - val_accuracy: 0.8072 - val_loss: 1.1999\nEpoch 78/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8881 - loss: 0.2640 - val_accuracy: 0.7848 - val_loss: 1.3696\nEpoch 79/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8944 - loss: 0.2498 - val_accuracy: 0.8027 - val_loss: 1.2801\nEpoch 80/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9000 - loss: 0.2552 - val_accuracy: 0.8027 - val_loss: 1.1221\nEpoch 81/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8827 - loss: 0.2829 - val_accuracy: 0.7982 - val_loss: 1.2346\nEpoch 82/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8957 - loss: 0.2551 - val_accuracy: 0.7937 - val_loss: 1.3308\nEpoch 83/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8840 - loss: 0.2455 - val_accuracy: 0.7937 - val_loss: 1.3641\nEpoch 84/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8902 - loss: 0.2523 - val_accuracy: 0.8027 - val_loss: 1.3187\nEpoch 85/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8941 - loss: 0.2521 - val_accuracy: 0.8072 - val_loss: 1.3394\nEpoch 86/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9120 - loss: 0.2243 - val_accuracy: 0.8027 - val_loss: 1.3846\nEpoch 87/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9026 - loss: 0.2413 - val_accuracy: 0.7803 - val_loss: 1.4724\nEpoch 88/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8950 - loss: 0.2460 - val_accuracy: 0.8117 - val_loss: 1.2653\nEpoch 89/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9141 - loss: 0.2253 - val_accuracy: 0.7937 - val_loss: 1.4099\nEpoch 90/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9043 - loss: 0.2424 - val_accuracy: 0.7892 - val_loss: 1.4957\nEpoch 91/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9122 - loss: 0.2359 - val_accuracy: 0.8027 - val_loss: 1.4049\nEpoch 92/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9013 - loss: 0.2290 - val_accuracy: 0.8027 - val_loss: 1.3082\nEpoch 93/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9005 - loss: 0.2377 - val_accuracy: 0.8027 - val_loss: 1.4615\nEpoch 94/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9142 - loss: 0.2111 - val_accuracy: 0.8072 - val_loss: 1.3954\nEpoch 95/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9111 - loss: 0.2272 - val_accuracy: 0.8206 - val_loss: 1.2964\nEpoch 96/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8983 - loss: 0.2312 - val_accuracy: 0.8117 - val_loss: 1.3584\nEpoch 97/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8834 - loss: 0.2587 - val_accuracy: 0.7713 - val_loss: 1.4547\nEpoch 98/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8742 - loss: 0.2793 - val_accuracy: 0.8072 - val_loss: 1.1755\nEpoch 99/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9021 - loss: 0.2493 - val_accuracy: 0.7937 - val_loss: 1.3387\nEpoch 100/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8816 - loss: 0.2739 - val_accuracy: 0.7937 - val_loss: 1.3617\nEpoch 101/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8889 - loss: 0.2691 - val_accuracy: 0.7982 - val_loss: 1.0392\nEpoch 102/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8596 - loss: 0.3534 - val_accuracy: 0.8027 - val_loss: 0.8105\nEpoch 103/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8835 - loss: 0.2831 - val_accuracy: 0.8161 - val_loss: 1.0777\nEpoch 104/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8865 - loss: 0.2666 - val_accuracy: 0.8072 - val_loss: 1.0721\nEpoch 105/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8890 - loss: 0.2704 - val_accuracy: 0.8161 - val_loss: 0.9997\nEpoch 106/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8941 - loss: 0.2511 - val_accuracy: 0.7937 - val_loss: 1.1235\nEpoch 107/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9218 - loss: 0.2115 - val_accuracy: 0.7848 - val_loss: 1.2830\nEpoch 108/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8726 - loss: 0.2902 - val_accuracy: 0.8161 - val_loss: 1.1376\nEpoch 109/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8923 - loss: 0.2530 - val_accuracy: 0.7982 - val_loss: 1.2976\nEpoch 110/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8986 - loss: 0.2291 - val_accuracy: 0.8027 - val_loss: 1.2541\nEpoch 111/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8974 - loss: 0.2428 - val_accuracy: 0.8027 - val_loss: 1.3138\nEpoch 112/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8944 - loss: 0.2347 - val_accuracy: 0.8027 - val_loss: 1.4871\nEpoch 113/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9049 - loss: 0.2338 - val_accuracy: 0.8161 - val_loss: 1.2730\nEpoch 114/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8876 - loss: 0.2455 - val_accuracy: 0.7982 - val_loss: 1.4668\nEpoch 115/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9057 - loss: 0.2310 - val_accuracy: 0.7982 - val_loss: 1.4888\nEpoch 116/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8984 - loss: 0.2347 - val_accuracy: 0.8072 - val_loss: 1.4858\nEpoch 117/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9287 - loss: 0.1941 - val_accuracy: 0.8027 - val_loss: 1.4348\nEpoch 118/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9066 - loss: 0.2236 - val_accuracy: 0.8117 - val_loss: 1.5230\nEpoch 119/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9100 - loss: 0.2464 - val_accuracy: 0.8117 - val_loss: 1.5101\nEpoch 120/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9029 - loss: 0.2341 - val_accuracy: 0.8072 - val_loss: 1.5486\nEpoch 121/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9212 - loss: 0.2184 - val_accuracy: 0.8117 - val_loss: 1.5365\nEpoch 122/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9090 - loss: 0.2174 - val_accuracy: 0.8072 - val_loss: 1.6072\nEpoch 123/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9045 - loss: 0.2321 - val_accuracy: 0.7848 - val_loss: 1.7370\nEpoch 124/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.8969 - loss: 0.2371 - val_accuracy: 0.8117 - val_loss: 1.4864\nEpoch 125/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9090 - loss: 0.2230 - val_accuracy: 0.7937 - val_loss: 1.7364\nEpoch 126/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9018 - loss: 0.2505 - val_accuracy: 0.7937 - val_loss: 1.5991\nEpoch 127/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8979 - loss: 0.2209 - val_accuracy: 0.7982 - val_loss: 1.6824\nEpoch 128/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9193 - loss: 0.2003 - val_accuracy: 0.8072 - val_loss: 1.7326\nEpoch 129/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9205 - loss: 0.2088 - val_accuracy: 0.8027 - val_loss: 1.6802\nEpoch 130/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9156 - loss: 0.2154 - val_accuracy: 0.8072 - val_loss: 1.6642\nEpoch 131/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9009 - loss: 0.2263 - val_accuracy: 0.8072 - val_loss: 1.7276\nEpoch 132/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.9135 - loss: 0.2203 - val_accuracy: 0.7668 - val_loss: 2.0504\nEpoch 133/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9107 - loss: 0.2494 - val_accuracy: 0.7982 - val_loss: 1.3150\nEpoch 134/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9244 - loss: 0.2070 - val_accuracy: 0.7937 - val_loss: 1.7711\nEpoch 135/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9093 - loss: 0.2317 - val_accuracy: 0.7937 - val_loss: 1.6947\nEpoch 136/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.8996 - loss: 0.2549 - val_accuracy: 0.8072 - val_loss: 1.2675\nEpoch 137/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9020 - loss: 0.2501 - val_accuracy: 0.7982 - val_loss: 1.7846\nEpoch 138/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8876 - loss: 0.2620 - val_accuracy: 0.7892 - val_loss: 1.3535\nEpoch 139/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8992 - loss: 0.2493 - val_accuracy: 0.8027 - val_loss: 1.0729\nEpoch 140/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9066 - loss: 0.2274 - val_accuracy: 0.7937 - val_loss: 1.2675\nEpoch 141/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9125 - loss: 0.2301 - val_accuracy: 0.8161 - val_loss: 1.1466\nEpoch 142/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8837 - loss: 0.2583 - val_accuracy: 0.8072 - val_loss: 1.2097\nEpoch 143/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9109 - loss: 0.2331 - val_accuracy: 0.8072 - val_loss: 1.2481\nEpoch 144/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9129 - loss: 0.2170 - val_accuracy: 0.8072 - val_loss: 1.3051\nEpoch 145/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9121 - loss: 0.2167 - val_accuracy: 0.8117 - val_loss: 1.3930\nEpoch 146/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.8915 - loss: 0.2310 - val_accuracy: 0.8117 - val_loss: 1.3616\nEpoch 147/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9032 - loss: 0.2323 - val_accuracy: 0.8161 - val_loss: 1.4314\nEpoch 148/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9173 - loss: 0.2044 - val_accuracy: 0.8072 - val_loss: 1.3449\nEpoch 149/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9142 - loss: 0.2116 - val_accuracy: 0.8027 - val_loss: 1.3485\nEpoch 150/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9076 - loss: 0.2196 - val_accuracy: 0.8027 - val_loss: 1.4069\nEpoch 151/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9156 - loss: 0.2221 - val_accuracy: 0.7937 - val_loss: 1.4930\nEpoch 152/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9079 - loss: 0.2181 - val_accuracy: 0.8161 - val_loss: 1.4216\nEpoch 153/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8950 - loss: 0.2319 - val_accuracy: 0.7937 - val_loss: 1.4703\nEpoch 154/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.8985 - loss: 0.2244 - val_accuracy: 0.8027 - val_loss: 1.4268\nEpoch 155/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9033 - loss: 0.2248 - val_accuracy: 0.8117 - val_loss: 1.5091\nEpoch 156/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9085 - loss: 0.2340 - val_accuracy: 0.8072 - val_loss: 1.4207\nEpoch 157/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.9121 - loss: 0.2090 - val_accuracy: 0.7848 - val_loss: 1.6311\nEpoch 158/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9178 - loss: 0.2088 - val_accuracy: 0.8072 - val_loss: 1.4069\nEpoch 159/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.9054 - loss: 0.2237 - val_accuracy: 0.8027 - val_loss: 1.5438\nEpoch 160/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9087 - loss: 0.2260 - val_accuracy: 0.8072 - val_loss: 1.5045\nEpoch 161/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9297 - loss: 0.1761 - val_accuracy: 0.7892 - val_loss: 1.7100\nEpoch 162/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.8983 - loss: 0.2487 - val_accuracy: 0.8251 - val_loss: 1.3475\nEpoch 163/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9032 - loss: 0.2386 - val_accuracy: 0.7758 - val_loss: 1.8291\nEpoch 164/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9017 - loss: 0.2404 - val_accuracy: 0.8072 - val_loss: 1.4911\nEpoch 165/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.9024 - loss: 0.2353 - val_accuracy: 0.8027 - val_loss: 1.5464\nEpoch 166/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9070 - loss: 0.2146 - val_accuracy: 0.7848 - val_loss: 1.6543\nEpoch 167/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9141 - loss: 0.2086 - val_accuracy: 0.8072 - val_loss: 1.4921\nEpoch 168/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9108 - loss: 0.2224 - val_accuracy: 0.8072 - val_loss: 1.4650\nEpoch 169/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9134 - loss: 0.2101 - val_accuracy: 0.8117 - val_loss: 1.4726\nEpoch 170/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9162 - loss: 0.2053 - val_accuracy: 0.8117 - val_loss: 1.5681\nEpoch 171/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9036 - loss: 0.2182 - val_accuracy: 0.7982 - val_loss: 1.5275\nEpoch 172/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9192 - loss: 0.1954 - val_accuracy: 0.8117 - val_loss: 1.5756\nEpoch 173/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9083 - loss: 0.2066 - val_accuracy: 0.8161 - val_loss: 1.5623\nEpoch 174/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9031 - loss: 0.2223 - val_accuracy: 0.8072 - val_loss: 1.5926\nEpoch 175/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9088 - loss: 0.2061 - val_accuracy: 0.8117 - val_loss: 1.5562\nEpoch 176/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8997 - loss: 0.2248 - val_accuracy: 0.8161 - val_loss: 1.6881\nEpoch 177/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9119 - loss: 0.2067 - val_accuracy: 0.8251 - val_loss: 1.5140\nEpoch 178/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9299 - loss: 0.1738 - val_accuracy: 0.7892 - val_loss: 1.7963\nEpoch 179/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9008 - loss: 0.2238 - val_accuracy: 0.8161 - val_loss: 1.5443\nEpoch 180/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.9028 - loss: 0.2227 - val_accuracy: 0.7982 - val_loss: 1.7437\nEpoch 181/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9137 - loss: 0.2019 - val_accuracy: 0.8296 - val_loss: 1.5513\nEpoch 182/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.9153 - loss: 0.2068 - val_accuracy: 0.8117 - val_loss: 1.7034\nEpoch 183/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9191 - loss: 0.1988 - val_accuracy: 0.8027 - val_loss: 1.6171\nEpoch 184/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.9137 - loss: 0.2154 - val_accuracy: 0.8072 - val_loss: 1.6226\nEpoch 185/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9117 - loss: 0.2157 - val_accuracy: 0.7892 - val_loss: 1.7047\nEpoch 186/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9126 - loss: 0.2210 - val_accuracy: 0.8206 - val_loss: 1.6319\nEpoch 187/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9143 - loss: 0.1976 - val_accuracy: 0.7982 - val_loss: 1.6132\nEpoch 188/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9221 - loss: 0.1928 - val_accuracy: 0.8296 - val_loss: 1.5227\nEpoch 189/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9188 - loss: 0.2075 - val_accuracy: 0.7982 - val_loss: 1.7064\nEpoch 190/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9123 - loss: 0.2300 - val_accuracy: 0.8161 - val_loss: 1.4519\nEpoch 191/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9143 - loss: 0.2056 - val_accuracy: 0.8296 - val_loss: 1.5646\nEpoch 192/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9179 - loss: 0.1946 - val_accuracy: 0.8161 - val_loss: 1.6922\nEpoch 193/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.9244 - loss: 0.1892 - val_accuracy: 0.8161 - val_loss: 1.6771\nEpoch 194/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9065 - loss: 0.2060 - val_accuracy: 0.7803 - val_loss: 1.7636\nEpoch 195/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9150 - loss: 0.1991 - val_accuracy: 0.8296 - val_loss: 1.5970\nEpoch 196/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9075 - loss: 0.2207 - val_accuracy: 0.8027 - val_loss: 1.6141\nEpoch 197/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9229 - loss: 0.1900 - val_accuracy: 0.7803 - val_loss: 1.7998\nEpoch 198/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.8983 - loss: 0.2226 - val_accuracy: 0.8251 - val_loss: 1.6507\nEpoch 199/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.9121 - loss: 0.2159 - val_accuracy: 0.7892 - val_loss: 1.7625\nEpoch 200/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9076 - loss: 0.2123 - val_accuracy: 0.8027 - val_loss: 1.6716\nEpoch 201/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9110 - loss: 0.2010 - val_accuracy: 0.8161 - val_loss: 1.5940\nEpoch 202/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9046 - loss: 0.1957 - val_accuracy: 0.8027 - val_loss: 1.6226\nEpoch 203/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.8989 - loss: 0.2143 - val_accuracy: 0.8206 - val_loss: 1.6222\nEpoch 204/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.8979 - loss: 0.2310 - val_accuracy: 0.8206 - val_loss: 1.6589\nEpoch 205/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9291 - loss: 0.1850 - val_accuracy: 0.8251 - val_loss: 1.6528\nEpoch 206/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9228 - loss: 0.1867 - val_accuracy: 0.8117 - val_loss: 1.7127\nEpoch 207/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.8990 - loss: 0.2301 - val_accuracy: 0.8296 - val_loss: 1.6299\nEpoch 208/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step - accuracy: 0.9186 - loss: 0.1931 - val_accuracy: 0.8117 - val_loss: 1.6686\nEpoch 209/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9249 - loss: 0.1891 - val_accuracy: 0.8161 - val_loss: 1.7065\nEpoch 210/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9219 - loss: 0.1932 - val_accuracy: 0.8161 - val_loss: 1.6918\nEpoch 211/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9069 - loss: 0.2246 - val_accuracy: 0.8206 - val_loss: 1.7006\nEpoch 212/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9246 - loss: 0.1807 - val_accuracy: 0.8161 - val_loss: 1.7603\nEpoch 213/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9105 - loss: 0.2055 - val_accuracy: 0.8251 - val_loss: 1.7321\nEpoch 214/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9216 - loss: 0.2016 - val_accuracy: 0.8117 - val_loss: 1.7267\nEpoch 215/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9137 - loss: 0.2080 - val_accuracy: 0.8251 - val_loss: 1.7309\nEpoch 216/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9146 - loss: 0.2223 - val_accuracy: 0.8206 - val_loss: 1.7001\nEpoch 217/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.9224 - loss: 0.2081 - val_accuracy: 0.8161 - val_loss: 1.7008\nEpoch 218/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.9163 - loss: 0.1868 - val_accuracy: 0.8296 - val_loss: 1.6816\nEpoch 219/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.9170 - loss: 0.1922 - val_accuracy: 0.7982 - val_loss: 1.8853\nEpoch 220/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.9106 - loss: 0.2282 - val_accuracy: 0.8430 - val_loss: 1.6426\nEpoch 221/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.9142 - loss: 0.2130 - val_accuracy: 0.7713 - val_loss: 2.6545\nEpoch 222/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8944 - loss: 0.2590 - val_accuracy: 0.8341 - val_loss: 1.3881\nEpoch 223/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.8884 - loss: 0.2603 - val_accuracy: 0.7803 - val_loss: 1.7425\nEpoch 224/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9066 - loss: 0.2352 - val_accuracy: 0.7982 - val_loss: 2.4547\nEpoch 225/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9164 - loss: 0.2157 - val_accuracy: 0.8072 - val_loss: 1.5633\nEpoch 226/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9052 - loss: 0.2426 - val_accuracy: 0.7848 - val_loss: 1.7439\nEpoch 227/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9098 - loss: 0.2006 - val_accuracy: 0.8251 - val_loss: 1.7120\nEpoch 228/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9235 - loss: 0.1874 - val_accuracy: 0.7848 - val_loss: 1.8926\nEpoch 229/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9199 - loss: 0.2068 - val_accuracy: 0.8072 - val_loss: 1.7982\nEpoch 230/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9036 - loss: 0.2080 - val_accuracy: 0.8027 - val_loss: 1.9066\nEpoch 231/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9290 - loss: 0.1820 - val_accuracy: 0.8072 - val_loss: 1.8584\nEpoch 232/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9153 - loss: 0.2029 - val_accuracy: 0.8027 - val_loss: 1.9620\nEpoch 233/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9025 - loss: 0.2050 - val_accuracy: 0.7937 - val_loss: 2.0587\nEpoch 234/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9122 - loss: 0.2033 - val_accuracy: 0.8296 - val_loss: 1.8938\nEpoch 235/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9120 - loss: 0.2164 - val_accuracy: 0.8206 - val_loss: 1.9102\nEpoch 236/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9268 - loss: 0.1871 - val_accuracy: 0.8161 - val_loss: 2.0527\nEpoch 237/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9312 - loss: 0.1832 - val_accuracy: 0.8027 - val_loss: 2.0967\nEpoch 238/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9256 - loss: 0.1854 - val_accuracy: 0.8161 - val_loss: 2.1328\nEpoch 239/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9308 - loss: 0.1760 - val_accuracy: 0.8072 - val_loss: 2.0815\nEpoch 240/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9219 - loss: 0.1926 - val_accuracy: 0.8161 - val_loss: 2.0429\nEpoch 241/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9213 - loss: 0.1950 - val_accuracy: 0.7937 - val_loss: 2.2493\nEpoch 242/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9179 - loss: 0.1972 - val_accuracy: 0.8206 - val_loss: 2.2415\nEpoch 243/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9150 - loss: 0.2007 - val_accuracy: 0.7982 - val_loss: 2.3266\nEpoch 244/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9110 - loss: 0.2106 - val_accuracy: 0.8027 - val_loss: 2.2281\nEpoch 245/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9164 - loss: 0.1912 - val_accuracy: 0.8296 - val_loss: 2.2458\nEpoch 246/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9170 - loss: 0.1969 - val_accuracy: 0.8161 - val_loss: 2.3133\nEpoch 247/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9407 - loss: 0.1670 - val_accuracy: 0.8251 - val_loss: 2.2056\nEpoch 248/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9136 - loss: 0.2052 - val_accuracy: 0.7892 - val_loss: 2.3967\nEpoch 249/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9193 - loss: 0.1973 - val_accuracy: 0.8206 - val_loss: 2.2215\nEpoch 250/250\n11/11 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.9176 - loss: 0.2038 - val_accuracy: 0.8117 - val_loss: 2.3318\n\n\nLet’s calculate and print the final accuracy scores for both the training and test (validation) data. Remember, we’ll call the function we wrote to do this earlier. You should see training accuracy is much better than test accuracy. We’ve overfitted. Don’t worry - we’ll try and improve that in a moment.\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 667us/step\nTraining accuracy 0.924\nTest accuracy 0.812\n\n\n\n18.4.1 Get training history\nhistory is a dictionary containing data collected during training. Remember - we stored it when we called the model.fit() method. Let’s take a look at the keys in this dictionary (these are the metrics monitored during training).\n\nhistory_dict = history.history\nhistory_dict.keys()\n\ndict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n\n\nWe see from the above that we have four keys in our history dictionary - loss, accuracy, validation loss and validation accuracy.\n\n\n18.4.2 Plot training history\nNow let’s plot our history data using the plotting function we wrote earlier.\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see from the plot above that the training accuracy gets better and better before reaching a plateau, but for the test data the accuracy initially improves, but then reduces a bit and plateaus at poorer performance. As we thought, we’ve overfitted. So let’s look at how we can now try to reduce the overfitting.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#improving-fit-by-avoiding-or-reducing-over-fitting",
    "href": "4g_shap_neural_nets.html#improving-fit-by-avoiding-or-reducing-over-fitting",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "18.5 Improving fit by avoiding or reducing-over fitting",
    "text": "18.5 Improving fit by avoiding or reducing-over fitting\nIn the lecture, we discussed a number of strategies we can take to try to reduce overfitting. Let’s look at each in turn.\n\n18.5.1 1) Reduce complexity of model\nA simple initial strategy is to reduce the complexity of the model, so that the “dividing line” it learns becomes less complex (and less likely to be an overfit).\nHere, we create a new model where we reduce the number of hidden layers to 1 (from the default we used of 3), and we reduce the number of neurons on each hidden layer to 32 (from the default we used of 128).\nThen we fit (train) this new model, exactly as we did before. We’ll set verbose to 0 though, so we don’t see everything as it trains (if you’d rather see it, just change verbose to 1 below).\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                hidden_layers=1,\n                hidden_layer_neurones=32)\n\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)\n\nLet’s calculate, print and plot accuracy as we did before.\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 833us/step\nTraining accuracy 0.868\nTest accuracy 0.839\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see that the simplification of the model above has improved things a bit (though it may not, there’s randomness at play here, and your network may have learned differently) - training accuracy has reduced, but test accuracy (our measure of how generally useful our model will be beyond the training set) has improved - a little bit. But there’s still a bit of a gap between them - we’re still overfitting.\n\n\n18.5.2 2) Reduce training time\nFor the moment, let’s do one change at a time, so we’ll go back to our original model before trying our next strategy.\nAnother approach we can use is simply to stop training for so long. We can see from our earlier plots that things improve in the test set initially but then reduces. So, by not training for so long, we can stop training before it significantly overfits.\nHere, we’ll run the model exactly as we did the first time, except we’ll only run it for 25 epochs, rather than 250 - just 10% of the original training time.\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features)\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=25,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)\n\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 933us/step\nTraining accuracy 0.882\nTest accuracy 0.816\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see that reducing the training time has also led to an improvement in test accuracy, much as simplifying the model did, although you might not. You might find that this measure is slightly more effective than the simplifying measure. You should also see from the plot that the test set accuracy tends to plateau, and it doesn’t get to the bit where it starts dropping significantly.\n\n\n18.5.3 3) Add dropout\nUsing dropout, in each training epoch a random selection of weights are “switched off” (the selection changes from epoch to epoch). It does this by using the Dropout layers after each hidden layer (remember when we added those earlier?), and randomly switching some of the incoming weights to 0. When predicting (after fitting) all weights are used. Dropout ensures that, during training, the model can’t rely too much on any set of weights (because they’ll occasionally be turned off), and looks to explore them more globally.\nThis is probably the most common method for reducing overfitting. Dropout values of 0.2 to 0.5 are common.\nHere, we’ll use a dropout value of 0.5. So 50% of the weights coming out of each hidden layer will be set to 0 in each epoch.\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(number_features,\n                dropout=0.5)\n\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0)\n\n\n# Show acuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 923us/step\nTraining accuracy 0.894\nTest accuracy 0.789\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nAgain, we should see that Dropout has improved performance on the test set over the base case (although it might not).\n\n\n18.5.4 4) Combination of the above and with automatic early stopping\nRather than just doing one of these things above, we tend to combine these measures. We’ll also use a Keras callback called EarlyStopping to automate the measure where we try to stop the training sooner. A callback is simply a function that Keras can use to perform various actions continually throughout the training.\nEarlyStopping will automatically stop the training when it appears the validation accuracy isn’t getting any better. It allows us to specify a patience level, which is the number of epochs we are prepared to wait (to give it a chance to improve) before EarlyStopping cuts things off. We can also optionally specify the minimum level we want our metric(s) (e.g. accuracy) to improve between epochs to count as an “improvement” - this allows us to say that we don’t consider a very small improvement as significant enough. You’ll see examples of this later in the course, but here we’ll just specify patience, and we’ll allow any improvement to count as improvement.\nHere, we specify a patience of 25 epochs - this means that we are prepared to wait 25 epochs to see if we can get a better accuracy score on the validation set. By setting restore_best_weights=True we tell it that, once it stops (if it didn’t manage to improve things in 25 epochs), then it should roll back the network to how it was when it reached its peak performance.\nSo, here we set up our EarlyStopping callback. Then we define a simpler network with 1 hidden layer and 64 neurons per layer, have a 50% dropout rate, and run for 250 epochs but add in the EarlyStopping callback so that Keras will stop the training when things stop improving in the validation set, and revert back to the best version it’s seen.\nIn the below, you’ll see we’ve also added another callback called ModelCheckpoint. This callback just automatically saves the model at its best point so we can easily retrieve it. In combination with EarlyStopping, this means we have a model that won’t keep going beyond when it should, and it’ll save the best version for later use.\nNote that as well as creating and defining the callbacks, you also need to ensure you add them into the list of inputs you pass in when you call model.fit.\n\n# Define save checkpoint callback (only save if new best validation results)\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    'model_checkpoint.keras', save_best_only=True)\n\n# Define early stopping callback\n# Stop when no validation improvement for 25 epochs\n# Restore weights to best validation accuracy\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=25, restore_best_weights=True)\n\n# Define network\nnumber_features = X_train_sc.shape[1]\nmodel = make_net(\n    number_features,\n    hidden_layers=1,\n    hidden_layer_neurones=64,\n    dropout=0.5)\n\n### Train model (and stote training info in history)\nhistory = model.fit(X_train_sc,\n                    y_train,\n                    epochs=250,\n                    batch_size=64,\n                    validation_data=(X_test_sc, y_test),\n                    verbose=0,\n                    callbacks=[checkpoint_cb, early_stopping_cb])\n\n\n# Show accuracy\ncalculate_accuracy(model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 778us/step\nTraining accuracy 0.844\nTest accuracy 0.812\n\n\n\nplot_training(history.history)\n\n\n\n\n\n\n\n\nYou should see from the above, where we’ve combined the three anti-overfitting measures, that we get quite a decent improvement in test accuracy and a closing of the gap between training and test accuracy. This indicates that our model is far less overfitted than it was originally.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#saving-and-reloading-the-model",
    "href": "4g_shap_neural_nets.html#saving-and-reloading-the-model",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "18.6 Saving and reloading the model",
    "text": "18.6 Saving and reloading the model\nFinally, we’ll look at how we can save our models so we can come back to them another time, and we don’t have to retrain them each time. For a small model like this, it’s not hugely inconvenient, but if we had a large model (that could take hours or even days to run) we don’t want to have to retrain it every time we want to use it!\nHere, we can use the save() function of the model to easily save a model. We just pass in a filename - we use the new .keras file extension. The model will be saved in the present working directory for the code.\nYou can also see in the cell below how to load a model back in, and then use it again. You can verify this if you run the two cells below, which will save the model, then load it back up, and recalculate its accuracy - you should see that the reported training and test accuracies are the same as you had above (because that’s the model we saved and then loaded back up).\n\n# Save model\nmodel.save('titanic_tf_model.keras')\n\n# Load and use saved model - we need to first set up a model\nrestored_model = keras.models.load_model('titanic_tf_model.keras')\n\n# Predict classes as normal\npredicted_proba = restored_model.predict(X_test_sc)\n\n# Show examples of predicted probability\nprint(predicted_proba[0:5].flatten())\n\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n[0.11886972 0.12785248 0.1493567  0.91443735 0.64788854]\n\n\n\ncalculate_accuracy(restored_model, X_train_sc, X_test_sc, y_train, y_test)\n\n21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 652us/step\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 833us/step\nTraining accuracy 0.844\nTest accuracy 0.812",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#table",
    "href": "4g_shap_neural_nets.html#table",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.1 Table",
    "text": "19.1 Table\n\n# Get top 10 features\n\nfeatures = X.columns.to_list()\n\nimportances = pd.DataFrame(index=features)\n\nimportances['mean_shap_values'] = np.mean(shap_values.values, axis=0)\n\n# Calculate mean absolute Shap value for each feature in training set\n# This will give us the average importance of each feature\nimportances['mean_abs_shap_values'] = np.mean(\n    np.abs(shap_values.values),axis=0)\n\nimportances.sort_values('mean_abs_shap_values', ascending=False)\n\n\n\n\n\n\n\n\n\nmean_shap_values\nmean_abs_shap_values\n\n\n\n\nFare\n0.000371\n0.342530\n\n\nAge\n-0.025216\n0.094059\n\n\nCabinNumber\n0.007254\n0.069325\n\n\nmale\n-0.006816\n0.021942\n\n\nPclass\n0.001667\n0.016691\n\n\nSibSp\n-0.000217\n0.003015\n\n\nEmbarked_C\n0.000236\n0.002582\n\n\nParch\n-0.000531\n0.001552\n\n\nAgeImputed\n-0.000190\n0.001277\n\n\nCabinLetter_D\n0.001010\n0.001218\n\n\nEmbarked_Q\n0.000132\n0.001059\n\n\nCabinLetterImputed\n-0.000241\n0.000909\n\n\nCabinLetter_missing\n-0.000154\n0.000809\n\n\nCabinLetter_B\n-0.000074\n0.000572\n\n\nCabinLetter_E\n0.000027\n0.000550\n\n\nEmbarked_S\n-0.000126\n0.000457\n\n\nCabinLetter_F\n0.000109\n0.000317\n\n\nCabinLetter_A\n0.000182\n0.000277\n\n\nEmbarked_missing\n-0.000017\n0.000189\n\n\nCabinLetter_G\n0.000034\n0.000172\n\n\nCabinLetter_C\n0.000036\n0.000156\n\n\nEmbarkedImputed\n-0.000014\n0.000078\n\n\nCabinNumberImputed\n-0.000016\n0.000070\n\n\nCabinLetter_T\n0.000000\n0.000000",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#beeswarm",
    "href": "4g_shap_neural_nets.html#beeswarm",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.2 Beeswarm",
    "text": "19.2 Beeswarm\n\nshap.plots.beeswarm(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#violin",
    "href": "4g_shap_neural_nets.html#violin",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.3 Violin",
    "text": "19.3 Violin\n\n# summarize the effects of all the features\nshap.plots.violin(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#bar",
    "href": "4g_shap_neural_nets.html#bar",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.4 Bar",
    "text": "19.4 Bar\n\nshap.plots.bar(shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#dependence",
    "href": "4g_shap_neural_nets.html#dependence",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.5 Dependence",
    "text": "19.5 Dependence\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\n# colour by most strongly interacting feature\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values)\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"Fare\"], color=shap_values)\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"male\"], color=shap_values)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#waterfall",
    "href": "4g_shap_neural_nets.html#waterfall",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.6 Waterfall",
    "text": "19.6 Waterfall\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0])\n\n\n\n\n\n\n\n\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[1])\n\n\n\n\n\n\n\n\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[15])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#force",
    "href": "4g_shap_neural_nets.html#force",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.7 Force",
    "text": "19.7 Force\n\n# visualize the first prediction's explanation with a force plot\nshap.plots.force(shap_values[0])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.plots.force(shap_values[15])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n19.7.1 Global Force Plot\n\nshap.plots.force(shap_values)\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_shap_neural_nets.html#decision",
    "href": "4g_shap_neural_nets.html#decision",
    "title": "17  SHAP and PFI with Neural Networks (Titanic Dataset)",
    "section": "19.8 Decision",
    "text": "19.8 Decision\n\nshap.plots.decision(\n    explainer.expected_value,\n    shap_values.values,\n    feature_names=X.columns.tolist()\n    )\n\n\n\n\n\n\n\n\n\nshap.plots.decision(\n    explainer.expected_value,\n    shap_values.values[22],\n    feature_names=X.columns.tolist()\n    )\n\n\n\n\n\n\n\n\n\nshap.plots.decision(\n    explainer.expected_value,\n    shap_values.values[103],\n    feature_names=X.columns.tolist()\n    )",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>SHAP and PFI with Neural Networks (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_1_xai_classification_exercise_SOLUTION.html",
    "href": "4g_1_xai_classification_exercise_SOLUTION.html",
    "title": "18  Exercise Solution: Explainable AI (Penguins Classification Dataset)",
    "section": "",
    "text": "18.0.0.1 Library Imports\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.inspection import PartialDependenceDisplay\n\nimport shap\n\n# Java Script for SHAP Plots\nshap.initjs()\n\n# Helper function to see methods in object\n# Might be useful when working through this exercise\n\ndef object_methods(obj):\n    '''\n    Helper function to list methods associated with an object\n    '''\n    try:\n        methods = [method_name for method_name in dir(obj)\n                   if callable(getattr(obj, method_name))]\n        print('Below are the methods for object: ', obj)\n        for method in methods:\n            print(method)\n    except:\n        print(\"Error\")",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Penguins Classification Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_1_xai_classification_exercise_SOLUTION.html#pdp-plots",
    "href": "4g_1_xai_classification_exercise_SOLUTION.html#pdp-plots",
    "title": "18  Exercise Solution: Explainable AI (Penguins Classification Dataset)",
    "section": "18.1 PDP Plots",
    "text": "18.1 PDP Plots\nNow let’s create a partial dependence plot for flipper length.\n\nfig, ax = plt.subplots(figsize=(10, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,  # Your fitted model\n    X,  # Your feature matrix\n    features=['flipper_length_mm'],  # List of features to plot\n    target=0,\n    kind='average',  # Type of PDP\n    ax=ax,\n    random_state=42\n)\nplt.show()\n\n\n\n\n\n\n\n\nNow create two plots side-by-side for bill length and bill depth.\nHINT: You don’t need to create multiple separate plots using matplotlib for this - you can do it from within the graphing function we’re using from scikit-learn.\n\nfig, ax = plt.subplots(figsize=(10, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,  # Your fitted model\n    X,  # Your feature matrix\n    features=['bill_length_mm', 'bill_depth_mm'],  # List of features to plot\n    target=0,\n    kind='average',  # Type of PDP\n    ax=ax,\n    random_state=42\n)\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Penguins Classification Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_1_xai_classification_exercise_SOLUTION.html#ice-plots",
    "href": "4g_1_xai_classification_exercise_SOLUTION.html#ice-plots",
    "title": "18  Exercise Solution: Explainable AI (Penguins Classification Dataset)",
    "section": "18.2 ICE Plots",
    "text": "18.2 ICE Plots\nNow create three ICE plots of the same feature - one for each class. Make sure to give each plot a name.\n\nfig, ax = plt.subplots(figsize=(10, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,  # Your fitted model\n    X,  # Your feature matrix\n    features=['bill_length_mm'],  # List of features to plot\n    target=0,\n    kind='individual',  # Type of PDP\n    ax=ax,\n    random_state=42\n)\nplt.title(\"Adelie Penguins - Bill Length ICE Plot\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,  # Your fitted model\n    X,  # Your feature matrix\n    features=['bill_length_mm'],  # List of features to plot\n    target=1,\n    kind='individual',  # Type of PDP\n    ax=ax,\n    random_state=42\n)\nplt.title(\"Chinstrap Penguins - Bill Length ICE Plot\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,  # Your fitted model\n    X,  # Your feature matrix\n    features=['bill_length_mm'],  # List of features to plot\n    target=2,\n    kind='individual',  # Type of PDP\n    ax=ax,\n    random_state=42\n)\nplt.title(\"Gentoo Penguins - Bill Length ICE Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nNow, just for one of the classes, create an ICE plot for bill_length_mm that also shows the average of all the ICE plots - a joint PDP/ICE plot, effectively!\nAgain, make sure you provide a title.\n\nfig, ax = plt.subplots(figsize=(10, 6))\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,  # Your fitted model\n    X,  # Your feature matrix\n    features=['bill_length_mm'],  # List of features to plot\n    target=2,\n    kind='both',  # Type of PDP\n    ax=ax,\n    random_state=42\n)\nplt.title(\"Adelie Penguins - Joint ICE/PDP Plot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n18.2.0.1 BONUS: 2D PDP Plots\nNow create a 2D plot of bill length and bill depth.\n\nPartialDependenceDisplay.from_estimator(\n    model,\n    X_test,\n    features=[('bill_length_mm', 'bill_depth_mm')],\n    kind='average',\n    target=0,\n    random_state=0\n)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Penguins Classification Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_1_xai_classification_exercise_SOLUTION.html#shap",
    "href": "4g_1_xai_classification_exercise_SOLUTION.html#shap",
    "title": "18  Exercise Solution: Explainable AI (Penguins Classification Dataset)",
    "section": "18.3 SHAP",
    "text": "18.3 SHAP\nWe have a multiclass problem with our penguins dataset.\nThis results in some slightly different outputs from our SHAP code, which can be confusing to deal with, so for now we’re just going to focus on a binary classification problem - is a penguin an Adelie, or not?\nRun the code below to turn this into a binary classification problem and retrain the model.\n\npenguins_binary = penguins.copy()\n\n# If Adelie penguin, return 1, else return 0\npenguins_binary['target'] = np.where(penguins_binary['target'] == 0, 1, 0)\npenguins_binary['species'] = np.where(penguins_binary['species'] == \"Adelie\", \"Adelie\", \"Not Adelie\")\n\n# Droping the target and species since we only need the measurements\nX = penguins_binary.drop(['target','species'], axis=1)\n\n# Define features (X) and target (y)\nX = X\ny = penguins_binary['target']\n\n# get class and features names\nclass_names = penguins_binary.species.unique()\nfeature_names = X.columns\n\n# Splitting into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=42)\n\n# Instantiate an XGBoost model and fit it\nmodel = XGBClassifier(random_state=42)\n\nmodel.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...) \n\n\n\n18.3.1 Obtaining Shap Values\nGenerate a SHAP explainer for our model, using X_train as the background data.\n\n# Compute SHAP values\nexplainer = shap.Explainer(\n    model,\n    X_train\n    )\n\nNow create the shap_values object, using X_test as the foreground data.\n\nshap_values = explainer.shap_values(X_test)\nshap_values\n\narray([[-5.17051824e+00,  1.21827941e+00, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01, -1.36875771e-01],\n       [ 1.13075847e-01,  1.78565094e+00, -4.36455202e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.35543021e-01, -1.91024542e-01],\n       [-2.72566813e-02, -3.30757442e+00, -2.22220667e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [-5.10160202e+00,  1.83535931e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [-4.14983763e+00, -1.87998490e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.01430746e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 5.45284030e+00,  1.74075824e+00, -3.02373941e-01,\n         1.54761915e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [ 1.91534915e+00, -3.58559378e+00, -3.02373941e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -2.18033250e-01, -1.91024542e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 4.68026578e+00,  1.44937979e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.42733446e+00, -1.30585920e-01],\n       [ 6.04219020e+00,  1.30955449e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.35887119e-02,  1.08201245e-01],\n       [ 4.79211754e+00, -1.02131687e-03,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.54396007e+00, -1.30585920e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 6.44204529e+00, -5.33143660e-01,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.01846663e-01, -1.30585920e-01],\n       [ 5.71326724e+00,  1.66361659e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -8.35388462e-02,  1.08201245e-01],\n       [-4.44524486e-02, -3.30757442e+00, -2.22220667e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [-4.04089575e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 4.97731472e+00,  1.03570520e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.54396007e+00, -1.30585920e-01],\n       [ 3.33277721e+00,  2.57971638e+00,  1.75061948e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.46053878e-01,  1.59685997e-01],\n       [ 5.91554046e+00,  1.47107445e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.02227896e-02, -1.30585920e-01],\n       [-4.10589588e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.58941987e+00,  8.37323978e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -9.51627192e-02, -1.30585920e-01],\n       [ 2.60384000e+00, -4.47425562e-01,  9.22989947e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.50891294e-01, -1.91024542e-01],\n       [-5.13327016e+00, -1.40029112e-01,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01,  1.13559266e-01],\n       [ 5.68476218e+00,  1.66361659e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -9.02227896e-02,  1.08201245e-01],\n       [-5.16359000e+00,  9.08194313e-01,  7.00742684e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.44841380e-01,  1.13559266e-01],\n       [-3.45011731e+00,  2.42737406e+00,  7.00742684e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.18269527e-01,  1.59685997e-01],\n       [-3.79519642e+00, -2.13501758e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.71910748e-01, -1.91024542e-01],\n       [ 6.07069525e+00,  1.30955449e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.69047685e-02,  1.08201245e-01],\n       [ 4.36485172e+00,  1.75760771e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [-5.06125901e+00,  4.69742856e-02,  4.39282524e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.85268838e-01, -1.36875771e-01],\n       [ 2.94871511e+00,  1.16633540e+00,  1.27291407e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  2.49448815e+00, -1.91024542e-01],\n       [ 5.94404552e+00,  1.47107445e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02, -1.30585920e-01],\n       [ 3.45756491e+00,  6.67898319e-01,  1.75061948e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.16614424e-01,  1.59685997e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.94008613e+00,  1.97020284e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 4.76751433e+00,  1.38394169e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.43894375e+00, -1.30585920e-01],\n       [-5.19412194e+00,  9.21530487e-01, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.44841380e-01,  1.13559266e-01],\n       [-4.41536294e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [-4.81358717e+00, -8.71694393e-01, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.83302972e-01,  1.13559266e-01],\n       [-4.81358717e+00, -1.02284420e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.83302972e-01,  1.13559266e-01],\n       [ 4.60306368e+00,  1.51015618e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.43894375e+00,  1.08201245e-01],\n       [ 6.44204529e+00, -3.81993851e-01,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -1.01846663e-01, -1.30585920e-01],\n       [ 4.78287797e+00, -2.68434791e-01,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00, -1.30585920e-01],\n       [-7.61286116e-01, -3.12572213e+00, -1.47042132e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [ 4.49503812e+00,  1.58729784e+00,  1.60711127e-01,\n         1.54761915e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [ 4.49106620e+00,  1.63139322e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [-5.11681622e+00,  1.45285302e+00, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 4.96807515e+00,  1.03570520e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00, -1.30585920e-01],\n       [-3.27354855e+00, -2.53607305e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.71910748e-01, -1.91024542e-01],\n       [-4.01430746e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.08042636e+00,  1.30955449e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.35887119e-02, -1.30585920e-01],\n       [ 4.93718394e+00, -3.09827117e-01,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00,  1.08201245e-01],\n       [-5.17991076e+00,  1.21827941e+00, -5.74065812e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 3.65377114e+00,  2.45184202e+00,  9.22989947e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.42782735e-01,  1.59685997e-01],\n       [-5.07603471e+00,  3.36381122e-02,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01, -1.36875771e-01],\n       [-4.53595532e+00, -1.58787129e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.84721798e-01, -1.36875771e-01],\n       [-3.45512069e+00, -2.34506634e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.81345309e-01, -1.91024542e-01],\n       [-5.12608560e+00,  9.08194313e-01,  4.39282524e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.46807246e-01, -1.36875771e-01],\n       [ 5.71326724e+00,  1.66361659e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [-4.10589588e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.58941987e+00,  8.37323978e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.51627192e-02, -1.30585920e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-5.07603471e+00,  3.36381122e-02,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01, -1.36875771e-01],\n       [ 4.57339775e+00,  1.58729784e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00, -1.30585920e-01],\n       [ 5.53214335e+00,  1.84474047e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [ 6.66092991e+00,  7.65813945e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.51627192e-02, -1.30585920e-01]])\n\n\nIt looks like it’s returned our outputs just as an array instead of a SHAP explanation object. Run the code below to turn our object into a proper shap.Explanation() object, as this is what all the plotting functions will be expecting.\n\n# Create an Explanation object\nshap_values = shap.Explanation(\n    values=shap_values,\n    base_values=explainer.expected_value,\n    data=X_test.values,\n    feature_names=X.columns\n    )\n\nNow let’s see what this looks like instead.\n\nshap_values\n\n.values =\narray([[-5.17051824e+00,  1.21827941e+00, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01, -1.36875771e-01],\n       [ 1.13075847e-01,  1.78565094e+00, -4.36455202e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.35543021e-01, -1.91024542e-01],\n       [-2.72566813e-02, -3.30757442e+00, -2.22220667e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [-5.10160202e+00,  1.83535931e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [-4.14983763e+00, -1.87998490e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.01430746e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 5.45284030e+00,  1.74075824e+00, -3.02373941e-01,\n         1.54761915e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [ 1.91534915e+00, -3.58559378e+00, -3.02373941e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -2.18033250e-01, -1.91024542e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 4.68026578e+00,  1.44937979e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.42733446e+00, -1.30585920e-01],\n       [ 6.04219020e+00,  1.30955449e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.35887119e-02,  1.08201245e-01],\n       [ 4.79211754e+00, -1.02131687e-03,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.54396007e+00, -1.30585920e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 6.44204529e+00, -5.33143660e-01,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.01846663e-01, -1.30585920e-01],\n       [ 5.71326724e+00,  1.66361659e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -8.35388462e-02,  1.08201245e-01],\n       [-4.44524486e-02, -3.30757442e+00, -2.22220667e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [-4.04089575e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 4.97731472e+00,  1.03570520e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.54396007e+00, -1.30585920e-01],\n       [ 3.33277721e+00,  2.57971638e+00,  1.75061948e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.46053878e-01,  1.59685997e-01],\n       [ 5.91554046e+00,  1.47107445e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.02227896e-02, -1.30585920e-01],\n       [-4.10589588e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.58941987e+00,  8.37323978e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -9.51627192e-02, -1.30585920e-01],\n       [ 2.60384000e+00, -4.47425562e-01,  9.22989947e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.50891294e-01, -1.91024542e-01],\n       [-5.13327016e+00, -1.40029112e-01,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01,  1.13559266e-01],\n       [ 5.68476218e+00,  1.66361659e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -9.02227896e-02,  1.08201245e-01],\n       [-5.16359000e+00,  9.08194313e-01,  7.00742684e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.44841380e-01,  1.13559266e-01],\n       [-3.45011731e+00,  2.42737406e+00,  7.00742684e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.18269527e-01,  1.59685997e-01],\n       [-3.79519642e+00, -2.13501758e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.71910748e-01, -1.91024542e-01],\n       [ 6.07069525e+00,  1.30955449e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.69047685e-02,  1.08201245e-01],\n       [ 4.36485172e+00,  1.75760771e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [-5.06125901e+00,  4.69742856e-02,  4.39282524e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.85268838e-01, -1.36875771e-01],\n       [ 2.94871511e+00,  1.16633540e+00,  1.27291407e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  2.49448815e+00, -1.91024542e-01],\n       [ 5.94404552e+00,  1.47107445e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02, -1.30585920e-01],\n       [ 3.45756491e+00,  6.67898319e-01,  1.75061948e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.16614424e-01,  1.59685997e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.94008613e+00,  1.97020284e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 4.76751433e+00,  1.38394169e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.43894375e+00, -1.30585920e-01],\n       [-5.19412194e+00,  9.21530487e-01, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.44841380e-01,  1.13559266e-01],\n       [-4.41536294e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [-4.81358717e+00, -8.71694393e-01, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.83302972e-01,  1.13559266e-01],\n       [-4.81358717e+00, -1.02284420e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.83302972e-01,  1.13559266e-01],\n       [ 4.60306368e+00,  1.51015618e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.43894375e+00,  1.08201245e-01],\n       [ 6.44204529e+00, -3.81993851e-01,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -1.01846663e-01, -1.30585920e-01],\n       [ 4.78287797e+00, -2.68434791e-01,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00, -1.30585920e-01],\n       [-7.61286116e-01, -3.12572213e+00, -1.47042132e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [ 4.49503812e+00,  1.58729784e+00,  1.60711127e-01,\n         1.54761915e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [ 4.49106620e+00,  1.63139322e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [-5.11681622e+00,  1.45285302e+00, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 4.96807515e+00,  1.03570520e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00, -1.30585920e-01],\n       [-3.27354855e+00, -2.53607305e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.71910748e-01, -1.91024542e-01],\n       [-4.01430746e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.08042636e+00,  1.30955449e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.35887119e-02, -1.30585920e-01],\n       [ 4.93718394e+00, -3.09827117e-01,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00,  1.08201245e-01],\n       [-5.17991076e+00,  1.21827941e+00, -5.74065812e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 3.65377114e+00,  2.45184202e+00,  9.22989947e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.42782735e-01,  1.59685997e-01],\n       [-5.07603471e+00,  3.36381122e-02,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01, -1.36875771e-01],\n       [-4.53595532e+00, -1.58787129e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.84721798e-01, -1.36875771e-01],\n       [-3.45512069e+00, -2.34506634e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.81345309e-01, -1.91024542e-01],\n       [-5.12608560e+00,  9.08194313e-01,  4.39282524e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.46807246e-01, -1.36875771e-01],\n       [ 5.71326724e+00,  1.66361659e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [-4.10589588e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.58941987e+00,  8.37323978e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.51627192e-02, -1.30585920e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-5.07603471e+00,  3.36381122e-02,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01, -1.36875771e-01],\n       [ 4.57339775e+00,  1.58729784e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00, -1.30585920e-01],\n       [ 5.53214335e+00,  1.84474047e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [ 6.66092991e+00,  7.65813945e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.51627192e-02, -1.30585920e-01]])\n\n.base_values =\n-0.7156927563131259\n\n.data =\narray([[ 32.1,  15.5, 188. , ...,   1. ,   0. ,   0. ],\n       [ 33.1,  16.1, 178. , ...,   1. ,   0. ,   0. ],\n       [ 33.5,  19. , 190. , ...,   0. ,   1. ,   0. ],\n       ...,\n       [ 55.9,  17. , 228. , ...,   0. ,   0. ,   1. ],\n       [ 59.6,  17. , 230. , ...,   0. ,   0. ,   1. ],\n       [  nan,   nan,   nan, ...,   0. ,   0. ,   nan]])\n\n\nFinally, let’s grab just the numeric component (our actual shap values).\n\nshap_values_numeric = shap_values.values\nshap_values_numeric\n\narray([[-5.17051824e+00,  1.21827941e+00, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01, -1.36875771e-01],\n       [ 1.13075847e-01,  1.78565094e+00, -4.36455202e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.35543021e-01, -1.91024542e-01],\n       [-2.72566813e-02, -3.30757442e+00, -2.22220667e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [-5.10160202e+00,  1.83535931e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [-4.14983763e+00, -1.87998490e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.01430746e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 5.45284030e+00,  1.74075824e+00, -3.02373941e-01,\n         1.54761915e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [ 1.91534915e+00, -3.58559378e+00, -3.02373941e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -2.18033250e-01, -1.91024542e-01],\n       [-4.44195123e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 4.68026578e+00,  1.44937979e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.42733446e+00, -1.30585920e-01],\n       [ 6.04219020e+00,  1.30955449e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.35887119e-02,  1.08201245e-01],\n       [ 4.79211754e+00, -1.02131687e-03,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.54396007e+00, -1.30585920e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 6.44204529e+00, -5.33143660e-01,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.01846663e-01, -1.30585920e-01],\n       [ 5.71326724e+00,  1.66361659e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -8.35388462e-02,  1.08201245e-01],\n       [-4.44524486e-02, -3.30757442e+00, -2.22220667e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [-4.04089575e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [ 4.97731472e+00,  1.03570520e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.54396007e+00, -1.30585920e-01],\n       [ 3.33277721e+00,  2.57971638e+00,  1.75061948e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.46053878e-01,  1.59685997e-01],\n       [ 5.91554046e+00,  1.47107445e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.02227896e-02, -1.30585920e-01],\n       [-4.10589588e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.58941987e+00,  8.37323978e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -9.51627192e-02, -1.30585920e-01],\n       [ 2.60384000e+00, -4.47425562e-01,  9.22989947e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.50891294e-01, -1.91024542e-01],\n       [-5.13327016e+00, -1.40029112e-01,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01,  1.13559266e-01],\n       [ 5.68476218e+00,  1.66361659e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -9.02227896e-02,  1.08201245e-01],\n       [-5.16359000e+00,  9.08194313e-01,  7.00742684e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.44841380e-01,  1.13559266e-01],\n       [-3.45011731e+00,  2.42737406e+00,  7.00742684e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.18269527e-01,  1.59685997e-01],\n       [-3.79519642e+00, -2.13501758e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.71910748e-01, -1.91024542e-01],\n       [ 6.07069525e+00,  1.30955449e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.69047685e-02,  1.08201245e-01],\n       [ 4.36485172e+00,  1.75760771e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [-5.06125901e+00,  4.69742856e-02,  4.39282524e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.85268838e-01, -1.36875771e-01],\n       [ 2.94871511e+00,  1.16633540e+00,  1.27291407e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  2.49448815e+00, -1.91024542e-01],\n       [ 5.94404552e+00,  1.47107445e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02, -1.30585920e-01],\n       [ 3.45756491e+00,  6.67898319e-01,  1.75061948e-01,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -2.16614424e-01,  1.59685997e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-4.94008613e+00,  1.97020284e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 4.76751433e+00,  1.38394169e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.43894375e+00, -1.30585920e-01],\n       [-5.19412194e+00,  9.21530487e-01, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.44841380e-01,  1.13559266e-01],\n       [-4.41536294e+00, -1.58787129e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [-4.81358717e+00, -8.71694393e-01, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.83302972e-01,  1.13559266e-01],\n       [-4.81358717e+00, -1.02284420e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.83302972e-01,  1.13559266e-01],\n       [ 4.60306368e+00,  1.51015618e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.43894375e+00,  1.08201245e-01],\n       [ 6.44204529e+00, -3.81993851e-01,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -1.01846663e-01, -1.30585920e-01],\n       [ 4.78287797e+00, -2.68434791e-01,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00, -1.30585920e-01],\n       [-7.61286116e-01, -3.12572213e+00, -1.47042132e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.31137460e-01, -1.91024542e-01],\n       [ 4.49503812e+00,  1.58729784e+00,  1.60711127e-01,\n         1.54761915e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [ 4.49106620e+00,  1.63139322e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00,  1.08201245e-01],\n       [-5.11681622e+00,  1.45285302e+00, -5.74065812e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 4.96807515e+00,  1.03570520e+00,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00, -1.30585920e-01],\n       [-3.27354855e+00, -2.53607305e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.71910748e-01, -1.91024542e-01],\n       [-4.01430746e+00, -1.98892677e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.08042636e+00,  1.30955449e+00,  9.22989947e-02,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.35887119e-02, -1.30585920e-01],\n       [ 4.93718394e+00, -3.09827117e-01,  1.27291407e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.58661936e+00,  1.08201245e-01],\n       [-5.17991076e+00,  1.21827941e+00, -5.74065812e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.35151969e-01,  1.13559266e-01],\n       [ 3.65377114e+00,  2.45184202e+00,  9.22989947e-02,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -1.42782735e-01,  1.59685997e-01],\n       [-5.07603471e+00,  3.36381122e-02,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01, -1.36875771e-01],\n       [-4.53595532e+00, -1.58787129e+00,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.84721798e-01, -1.36875771e-01],\n       [-3.45512069e+00, -2.34506634e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.81345309e-01, -1.91024542e-01],\n       [-5.12608560e+00,  9.08194313e-01,  4.39282524e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.46807246e-01, -1.36875771e-01],\n       [ 5.71326724e+00,  1.66361659e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [-4.10589588e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01, -1.36875771e-01],\n       [ 6.58941987e+00,  8.37323978e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.51627192e-02, -1.30585920e-01],\n       [-4.13248416e+00, -1.89733836e+00, -1.60803193e-01,\n         4.70017666e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01, -3.84721798e-01,  1.13559266e-01],\n       [-5.07603471e+00,  3.36381122e-02,  7.00742684e-02,\n        -5.04409203e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -3.83302972e-01, -1.36875771e-01],\n       [ 4.57339775e+00,  1.58729784e+00,  1.60711127e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n         2.77963270e-01,  1.39628446e+00, -1.30585920e-01],\n       [ 5.53214335e+00,  1.84474047e+00,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -8.35388462e-02,  1.08201245e-01],\n       [ 6.66092991e+00,  7.65813945e-01,  1.75061948e-01,\n        -2.46472679e-02,  0.00000000e+00,  0.00000000e+00,\n        -3.68462939e-01, -9.51627192e-02, -1.30585920e-01]])\n\n\n\n\n18.3.2 Exploring the SHAP outputs\nFirst, let’s just get a list of our most important features according to SHAP.\n\n# get feature importance for comparison using MDI method\nfeatures = list(X_train)\nfeature_importances = model.feature_importances_\nimportances = pd.DataFrame(index=features)\nimportances['importance'] = feature_importances\nimportances['rank'] = importances['importance'].rank(ascending=False).values\nimportances.sort_values('rank').head()\n\n# Get shapley importances\n# Calculate mean Shapley value for each feature in trainign set\nimportances['mean_shapley_values'] = np.mean(\n    shap_values_numeric, axis=0\n    )\n\n# Calculate mean absolute Shapley value for each feature in trainign set\n# This will give us the average importance of each feature\nimportances['mean_abs_shapley_values'] = np.mean(\n    np.abs(shap_values_numeric), axis=0\n    )\n\nimportance_top = \\\n    importances.sort_values(\n        by='importance', ascending=False\n        ).index\n\nshapley_top = \\\n    importances.sort_values(\n        by='mean_abs_shapley_values',\n        ascending=False).index\n\n# Add to DataFrame\ntop_features = pd.DataFrame()\ntop_features['importances'] = importance_top.values\ntop_features['Shapley'] = shapley_top.values\n\n# Display\ntop_features\n\n\n\n\n\n\n\n\n\nimportances\nShapley\n\n\n\n\n0\nbill_length_mm\nbill_length_mm\n\n\n1\nisland_Torgersen\nbill_depth_mm\n\n\n2\nbill_depth_mm\nisland_Torgersen\n\n\n3\nflipper_length_mm\nisland_Dream\n\n\n4\nmale\nflipper_length_mm\n\n\n5\nisland_Dream\nmale\n\n\n6\nbody_mass_g\nbody_mass_g\n\n\n7\nyear\nyear\n\n\n8\nisland_Biscoe\nisland_Biscoe\n\n\n\n\n\n\n\n\n\n18.3.2.1 SHAP plots\nGenerate a bar plot of the SHAP values.\n\nshap.plots.bar(shap_values)\n\n\n\n\n\n\n\n\nGenerate a beeswarm plot.\n\nshap.plots.beeswarm(shap_values)\n\nDimensionError: Feature and SHAP matrices must have the same number of rows!\n\n\n\n\n\n\n\n\n\nGenerate a waterfall plot for 5 different examples from the dataset.\n\nshap.plots.waterfall(shap_values[0])\n\n\nshap.plots.waterfall(shap_values[3])\n\n\nshap.plots.waterfall(shap_values[5])\n\n\nshap.plots.waterfall(shap_values[-1])\n\n\nshap.plots.waterfall(shap_values[194])\n\n\n\n18.3.2.2 Dependence Plots for each Class (Species)\nLet’s look at the columns in our dataset and the indices.\n\n# Lets see the features and respective index numbers\nfor e, i in enumerate(X_test.columns):\n    print(f\"{e} - {i}\")\n\nFirst, generate a scatter plot for the bill length.\n\nshap.plots.scatter(shap_values[:, 'bill_length_mm'])\n\nNow colour this by bill depth.\n\nshap.plots.scatter(shap_values[:, 'bill_length_mm'], color=shap_values[:,\"bill_depth_mm\"])\n\nNow colour it by the most strongly interacting feature.\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"bill_length_mm\"], color=shap_values)\n\nNow let’s iterate through and create scatter plots per column.\n\n# dependence plots\nfig, ax = plt.subplots(3, 3, figsize=(20,10))\nax = ax.ravel()\n\nfor idx, col_name in enumerate(feature_names):\n    shap.plots.scatter(shap_values[:, col_name], show=False, ax=ax[idx])\n\n\n\n18.3.2.3 Force Plots\nCreate a force plot for the whole dataset.\n\nshap.plots.force(shap_values)\n\nCreate a force plot for five randomly chosen pieces of data.\n\nshap.plots.force(shap_values[0])\n\n\nshap.plots.force(shap_values[1])\n\n\nshap.plots.force(shap_values[-1])\n\n\nshap.plots.force(shap_values[185])\n\n\nshap.plots.force(shap_values[247])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Penguins Classification Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_2a_classification_boosting_exercise_SOLUTION.html",
    "href": "4g_2a_classification_boosting_exercise_SOLUTION.html",
    "title": "19  Exercise Solution: Explainable AI (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "20 Feature Importance",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_2a_classification_boosting_exercise_SOLUTION.html#local-waterfall-plots",
    "href": "4g_2a_classification_boosting_exercise_SOLUTION.html#local-waterfall-plots",
    "title": "19  Exercise Solution: Explainable AI (Stroke Thromobolysis Dataset)",
    "section": "22.1 Local: Waterfall plots",
    "text": "22.1 Local: Waterfall plots\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0])\n\n\n\n\n\n\n\n\n\n# visualize another prediction's explanation\nshap.plots.waterfall(shap_values[7], max_display=15)\n\n\n\n\n\n\n\n\n\n# visualize another prediction's explanation\nshap.plots.waterfall(shap_values[145], max_display=15)\n\n\n\n\n\n\n\n\n\n22.1.1 Local: Force Plots\n\n# visualize the first prediction's explanation with a force plot\nshap.plots.force(shap_values[0])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n22.1.2 Global: Force Plots\n\n# visualize all the predictions\n# this struggles with a large number of values so we'll sample a small set\nshap.plots.force(shap.utils.sample(shap_values, 1000))\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n22.1.3 Dependence Plots\n\nX.columns\n\nIndex(['Hosp_1', 'Hosp_2', 'Hosp_3', 'Hosp_4', 'Hosp_5', 'Hosp_6', 'Hosp_7',\n       'Male', 'Age', '80+', 'Onset Time Known Type_BE',\n       'Onset Time Known Type_NK', 'Onset Time Known Type_P',\n       '# Comorbidities', '2+ comorbidotes', 'Congestive HF', 'Hypertension',\n       'Atrial Fib', 'Diabetes', 'TIA', 'Co-mordity', 'Antiplatelet_0',\n       'Antiplatelet_1', 'Antiplatelet_NK', 'Anticoag before stroke_0',\n       'Anticoag before stroke_1', 'Anticoag before stroke_NK',\n       'Stroke severity group_1. No stroke symtpoms',\n       'Stroke severity group_2. Minor', 'Stroke severity group_3. Moderate',\n       'Stroke severity group_4. Moderate to severe',\n       'Stroke severity group_5. Severe', 'Stroke Type_I', 'Stroke Type_PIH',\n       'S2RankinBeforeStroke', 'S2NihssArrival', 'S2NihssArrivalLocQuestions',\n       'S2NihssArrivalLocCommands', 'S2NihssArrivalBestGaze',\n       'S2NihssArrivalVisual', 'S2NihssArrivalFacialPalsy',\n       'S2NihssArrivalMotorArmLeft', 'S2NihssArrivalMotorArmRight',\n       'S2NihssArrivalMotorLegLeft', 'S2NihssArrivalMotorLegRight',\n       'S2NihssArrivalLimbAtaxia', 'S2NihssArrivalSensory',\n       'S2NihssArrivalBestLanguage', 'S2NihssArrivalDysarthria',\n       'S2NihssArrivalExtinctionInattention'],\n      dtype='object')\n\n\n\n22.1.3.1 Simple scatter of a single feature\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, 'Age'])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, 'Diabetes'])\n\n\n\n\n\n\n\n\n\nshap.plots.scatter(shap_values[:, '# Comorbidities'])\n\n\n\n\n\n\n\n\n\n\n\n22.1.4 Scatter of multiple features\nPassing in shap_values to the colour will colour the value by the most strongly interacting other value.\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"Male\"], color=shap_values)\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values)\n\n\n\n\n\n\n\n\nAlternatively we can choose to colour by a specific column.\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"Age\"], color=shap_values[:,\"Male\"])\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"Male\"], color=shap_values[:,\"Age\"])",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_2a_classification_boosting_exercise_SOLUTION.html#prediction-uncertainty",
    "href": "4g_2a_classification_boosting_exercise_SOLUTION.html#prediction-uncertainty",
    "title": "19  Exercise Solution: Explainable AI (Stroke Thromobolysis Dataset)",
    "section": "22.2 Prediction Uncertainty",
    "text": "22.2 Prediction Uncertainty\n\nsplits = 50\ntrain_set = []\n\nfor i in range(splits):\n    train_set.append(X_train.join(y_train).sample(frac=1, replace=True))\n\n\n# Set up lists for models and probability predictions\nmodels = []\nresults  = []\naccuracies = []\n\nfor i in range(splits):\n\n    # Get X and y\n    X_train = train_set[i].drop('Clotbuster given', axis=1)\n    y_train = train_set[i]['Clotbuster given']\n\n    # Define and train model; use different random seed for each model\n    model = XGBClassifier(random_state=42+i)\n    model.fit(X_train, y_train)\n    models.append(model)\n\n    # Get predicted probabilities and class\n    y_probs = model.predict_proba(X_test)[:,1]\n    y_pred = y_probs &gt; 0.5\n    results.append([y_probs])\n\n    # Show accuracy\n    accuracy = np.mean(y_pred == y_test)\n    accuracies.append(accuracy)\n\nresults = np.array(results)\nresults = results.T.reshape(-1, splits)\n\n\nprint (f'Mean accuracy: {np.mean(accuracies):0.3f}')\n\nMean accuracy: 0.803\n\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.boxplot(accuracies, whis=999)\nax.set_ylabel('Model accuracy')\nax.axes.xaxis.set_ticklabels([]) # Remove xtick labels\nplt.show()\n\n\n\n\n\n\n\n\n\nclassification = results &gt;= 0.5\nconsensus = classification.sum(axis=1) &gt;= splits/2\nconsensus_accuracy = np.mean(consensus == y_test)\nprint (f'Consensus accuracy: {consensus_accuracy:0.3f}')\n\nConsensus accuracy: 0.826\n\n\n\nresults = results[np.mean(results,axis=1).argsort()]\nmean = np.mean(results,axis=1)\nstdev = np.std(results,axis=1)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.errorbar(range(len(mean)), mean, yerr=stdev, label='Standard Deviation', zorder=1)\nax.plot(mean, 'o', c='r', markersize=2, label = 'Mean probability', zorder=2)\nax.axes.xaxis.set_ticklabels([])\nax.set_xlabel('Patient')\nax.set_ylabel('Probability of Clotbuster Being Given')\nax.set_xticks([])\nax.grid()\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nresults = results[np.mean(results,axis=1).argsort()]\n\nmean = np.mean(results,axis=1)\nstdev = np.std(results,axis=1)\nse = stdev / np.sqrt(splits)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.errorbar(range(len(mean)), mean, yerr=se, label='Standard Error', zorder=1)\nax.plot(mean, 'o', c='r', markersize=2, label = 'Mean probability', zorder=2)\nax.axes.xaxis.set_ticklabels([])\nax.set_xlabel('Patient')\nax.set_ylabel('Probability of Clotbuster Being Given')\nax.set_xticks([])\nax.grid()\nax.legend()\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_2b_regression_tree_exercise_shap_SOLUTION.html",
    "href": "4g_2b_regression_tree_exercise_shap_SOLUTION.html",
    "title": "20  Exercise Solution: Explainable AI (LOS Dataset)",
    "section": "",
    "text": "20.1 Core\nWe’re going to work with a dataset to try to predict patient length of stay.\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import the relevant models from Sklearn, XGBoost, CatBoost and LightGBM\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n# import any other libraries you need\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, \\\n                            r2_score, root_mean_squared_error\n# Additional imports for explainable AI\nfrom sklearn.inspection import PartialDependenceDisplay, permutation_importance\n\n# Import shap for shapley values\nimport shap\n\n# JavaScript Important for the interactive charts later on\nshap.initjs()\nOpen the data dictionary in the los_dataset folder and take a look at what data is available.\nNext, load in the dataframe containing the LOS data.\nlos_df = pd.read_csv(\"../datasets/los_dataset/LengthOfStay.csv\", index_col=\"eid\")\nView the dataframe.\nlos_df.head()\n\n\n\n\n\n\n\n\n\nvdate\nrcount\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\n...\nsodium\nglucose\nbloodureanitro\ncreatinine\nbmi\npulse\nrespiration\nsecondarydiagnosisnonicd9\nfacid\nlengthofstay\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n8/29/2012\n0\nF\n0\n0\n0\n0\n0\n0\n0\n...\n140.361132\n192.476918\n12.0\n1.390722\n30.432418\n96\n6.5\n4\nB\n3\n\n\n2\n5/26/2012\n5+\nF\n0\n0\n0\n0\n0\n0\n0\n...\n136.731692\n94.078507\n8.0\n0.943164\n28.460516\n61\n6.5\n1\nA\n7\n\n\n3\n9/22/2012\n1\nF\n0\n0\n0\n0\n0\n0\n0\n...\n133.058514\n130.530524\n12.0\n1.065750\n28.843812\n64\n6.5\n2\nB\n3\n\n\n4\n8/9/2012\n0\nF\n0\n0\n0\n0\n0\n0\n0\n...\n138.994023\n163.377028\n12.0\n0.906862\n27.959007\n76\n6.5\n1\nA\n1\n\n\n5\n12/20/2012\n0\nF\n0\n0\n0\n1\n0\n1\n0\n...\n138.634836\n94.886654\n11.5\n1.242854\n30.258927\n67\n5.6\n2\nE\n4\n\n\n\n\n5 rows × 26 columns\nConsider what columns to remove.\nHINT: Is there a column in the dataset that doesn’t really make much sense to predict from? If you’re not sure, use the full dataset for now and come back to this later.\nNOTE: For now, we’re going to assume that all of the included measures will be available to us at the point we need to make a prediction - they’re not things that will be calculated later in the patient’s stay.\nlos_df = los_df.drop(columns=\"vdate\")\nlos_df.head(1)\n\n\n\n\n\n\n\n\n\nrcount\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\n...\nsodium\nglucose\nbloodureanitro\ncreatinine\nbmi\npulse\nrespiration\nsecondarydiagnosisnonicd9\nfacid\nlengthofstay\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\nF\n0\n0\n0\n0\n0\n0\n0\n0\n...\n140.361132\n192.476918\n12.0\n1.390722\n30.432418\n96\n6.5\n4\nB\n3\n\n\n\n\n1 rows × 25 columns\nConvert categories with only two options into a boolean value (e.g. for a gender column in which gender has only been provided as M or F, you could encode M as 0 and F as 1).\nlos_df.gender.unique()\n\narray(['F', 'M'], dtype=object)\nlos_df['gender'].replace('M', 0, inplace=True)\nlos_df['gender'].replace('F', 1, inplace=True)\n\nlos_df.gender.unique()\n\narray([1, 0], dtype=int64)\nConvert columns with multiple options per category into multiple columns using one-hot encoding.\nlos_df.facid.unique()\n\n# Bonus - astype('int') will convert the true/false values to 0/1\n# not necessary - it will work regardless\none_hot = pd.get_dummies(los_df['facid']).astype('int')\nlos_df = los_df.drop('facid', axis=1)\nlos_df = los_df.join(one_hot)\nlos_df.head()\n\n\n\n\n\n\n\n\n\nrcount\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\n...\nbmi\npulse\nrespiration\nsecondarydiagnosisnonicd9\nlengthofstay\nA\nB\nC\nD\nE\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n30.432418\n96\n6.5\n4\n3\n0\n1\n0\n0\n0\n\n\n2\n5+\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n28.460516\n61\n6.5\n1\n7\n1\n0\n0\n0\n0\n\n\n3\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n28.843812\n64\n6.5\n2\n3\n0\n1\n0\n0\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n27.959007\n76\n6.5\n1\n1\n1\n0\n0\n0\n0\n\n\n5\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n...\n30.258927\n67\n5.6\n2\n4\n0\n0\n0\n0\n1\n\n\n\n\n5 rows × 29 columns\nlos_df.rcount.value_counts()\n\n# Bonus - astype('int') will convert the true/false values to 0/1\n# not necessary - it will work regardless\none_hot = pd.get_dummies(los_df['rcount'], prefix=\"rcount\").astype('int')\nlos_df = los_df.drop('rcount', axis=1)\nlos_df = los_df.join(one_hot)\nlos_df.head()\n\n\n\n\n\n\n\n\n\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\nfibrosisandother\n...\nB\nC\nD\nE\nrcount_0\nrcount_1\nrcount_2\nrcount_3\nrcount_4\nrcount_5+\n\n\neid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n5\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 34 columns\nTrain a decision tree model to predict length of stay based on the variables in this dataset.\nX = los_df.drop(columns='lengthofstay')\ny = los_df['lengthofstay']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size = 0.25,\n    random_state=42\n    )\n\nregr_dt = DecisionTreeRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_dt.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_train = regr_dt.predict(X_train)\ny_pred_test = regr_dt.predict(X_test)\ny_pred_test\n\narray([3., 1., 3., ..., 8., 2., 5.])\nAssess the performance of this model.\nprint(\"TRAINING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_train, y_pred_train):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_train, y_pred_train):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_train, y_pred_train))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_train, y_pred_train))\n\nTRAINING DATA\nMean absolute error: 0.00\nMean absolute percentage error: 0.00%\nRoot Mean squared error: 0.00\nCoefficient of determination: 1.00\nprint(\"TESTING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred_test):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred_test):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_test, y_pred_test))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_test))\n\nTESTING DATA\nMean absolute error: 0.50\nMean absolute percentage error: 12.94%\nRoot Mean squared error: 0.93\nCoefficient of determination: 0.84\ndef plot_residuals(actual, predicted):\n    residuals = actual - predicted\n\n    plt.figure(figsize=(10, 5))\n    plt.hist(residuals, bins=20)\n    plt.axvline(x = 0, color = 'r')\n    plt.xlabel('Residual')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Residuals')\n    plt.show()\n\nplot_residuals(y_test, y_pred_test)\ndef plot_actual_vs_predicted(actual, predicted):\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.scatter(actual, predicted, color=\"black\", alpha=0.05)\n    ax.axline((1, 1), slope=1)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('True vs Predicted Values')\n    plt.show()\n\nplot_actual_vs_predicted(y_test, y_pred_test)\nTrain a boosting model to predict length of stay based on the variables in this dataset.\nX = los_df.drop(columns='lengthofstay')\ny = los_df['lengthofstay']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size = 0.25,\n    random_state=42\n    )\n\nregr_xgb = XGBRegressor(random_state=42)\n\n# Train the model using the training sets\nregr_xgb.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred_train = regr_xgb.predict(X_train)\ny_pred_test = regr_xgb.predict(X_test)\ny_pred_test\n\narray([3.6313417, 0.8304186, 2.4800673, ..., 5.5354204, 1.5881324,\n       5.249632 ], dtype=float32)\nAssess the performance of this model and compare it with your decision tree model.\nprint(\"TRAINING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_train, y_pred_train):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_train, y_pred_train):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_train, y_pred_train))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_train, y_pred_train))\n\nTRAINING DATA\nMean absolute error: 0.29\nMean absolute percentage error: 10.66%\nRoot Mean squared error: 0.37\nCoefficient of determination: 0.98\nprint(\"TESTING DATA\")\nprint(f\"Mean absolute error: {mean_absolute_error(y_test, y_pred_test):.2f}\")\nprint(f\"Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred_test):.2%}\" )\nprint(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_test, y_pred_test))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_test))\n\nTESTING DATA\nMean absolute error: 0.33\nMean absolute percentage error: 11.60%\nRoot Mean squared error: 0.44\nCoefficient of determination: 0.96\nplot_residuals(y_test, y_pred_test)\nplot_actual_vs_predicted(y_test, y_pred_test)",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (LOS Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_2b_regression_tree_exercise_shap_SOLUTION.html#exercise-4g-explainable-ai",
    "href": "4g_2b_regression_tree_exercise_shap_SOLUTION.html#exercise-4g-explainable-ai",
    "title": "20  Exercise Solution: Explainable AI (LOS Dataset)",
    "section": "20.2 Exercise 4G: Explainable AI",
    "text": "20.2 Exercise 4G: Explainable AI\n\n20.2.1 Explore feature importance\n\n20.2.1.1 Importance with MDI\n\nfeatures = list(X_train)\n\nfeature_importances_dt = regr_dt.feature_importances_\nimportances_dt = pd.DataFrame(index=features)\nimportances_dt['importance_dt'] = feature_importances_dt\nimportances_dt['rank_dt'] = importances_dt['importance_dt'].rank(ascending=False).values\nimportances_dt.sort_values('rank_dt').head()\n\n\n\n\n\n\n\n\n\nimportance_dt\nrank_dt\n\n\n\n\nrcount_0\n0.358057\n1.0\n\n\nrcount_1\n0.140998\n2.0\n\n\nE\n0.114736\n3.0\n\n\nhematocrit\n0.044496\n4.0\n\n\nrcount_2\n0.040130\n5.0\n\n\n\n\n\n\n\n\n\nfeature_importances_xgb = regr_xgb.feature_importances_\nimportances_xgb = pd.DataFrame(index=features)\nimportances_xgb['importance_xgb'] = feature_importances_xgb\nimportances_xgb['rank_xgb'] = importances_xgb['importance_xgb'].rank(ascending=False).values\nimportances_xgb.sort_values('rank_xgb').head()\n\n\n\n\n\n\n\n\n\nimportance_xgb\nrank_xgb\n\n\n\n\nrcount_0\n0.298638\n1.0\n\n\nrcount_1\n0.223313\n2.0\n\n\nE\n0.104948\n3.0\n\n\nrcount_2\n0.095113\n4.0\n\n\nrcount_3\n0.044833\n5.0\n\n\n\n\n\n\n\n\n\n\n\n20.2.2 Repeat using PFI\n\nfeature_names = X.columns.tolist()\n\nresult_dt_pfi = permutation_importance(\n    regr_dt, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\nimportances_pfi_dt = pd.Series(result_dt_pfi.importances_mean, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nimportances_pfi_dt.plot.bar(yerr=result_dt_pfi.importances_std, ax=ax)\nax.set_title(\"Feature importances using permutation on full model\")\nax.set_ylabel(\"Mean accuracy decrease\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfeature_names = X.columns.tolist()\n\nresult_xgb_pfi = permutation_importance(\n    regr_xgb, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\nimportances_pfi_xgb = pd.Series(result_xgb_pfi.importances_mean, index=feature_names)\n\nfig, ax = plt.subplots(figsize=(15,10))\nimportances_pfi_xgb.plot.bar(yerr=result_xgb_pfi.importances_std, ax=ax)\nax.set_title(\"Feature importances using permutation on full model\")\nax.set_ylabel(\"Mean accuracy decrease\")\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (LOS Dataset)</span>"
    ]
  },
  {
    "objectID": "4g_2b_regression_tree_exercise_shap_SOLUTION.html#shap",
    "href": "4g_2b_regression_tree_exercise_shap_SOLUTION.html#shap",
    "title": "20  Exercise Solution: Explainable AI (LOS Dataset)",
    "section": "20.3 SHAP",
    "text": "20.3 SHAP\nAll code below has just been applied to the xg boost version of the model.\nWhen do we pass in different bits of data?\n\nThe foreground data is the input to explainer.shap_values and the background data is the data parameter of shap.TreeExplainer’s init.\nIf you don’t input foreground data you won’t get SHAP values, so it wouldn’t make much sense to not input foreground data.\nIf you don’t input the background data, it will actually use a different version of TreeExplainer (path dependent) that implicitly uses the training data as the background data set.\n\n\nHugh Chen, https://github.com/shap/shap/issues/1366\n\n\n# explain the model's predictions using SHAP\nexplainer = shap.Explainer(regr_xgb, X_train)\n\nshap_values = explainer(X_test)\n\nshap_values\n\n 99%|===================| 24754/25000 [00:49&lt;00:00]        \n\n\n.values =\narray([[ 0.00764417,  0.        , -0.03838515, ...,  0.02178417,\n        -0.01230581, -0.05431815],\n       [ 0.00406493,  0.        , -0.08355626, ...,  0.02667989,\n        -0.01247949, -0.05238643],\n       [-0.00260847,  0.        , -0.07398675, ...,  0.02891657,\n        -0.01341456, -0.05173721],\n       ...,\n       [-0.00061298,  0.        , -0.03833514, ...,  0.02290138,\n        -0.0153973 , -0.05772406],\n       [-0.00180168,  0.        , -0.06790128, ...,  0.02414472,\n        -0.01215895, -0.05733014],\n       [ 0.00211131,  0.        , -0.04626162, ...,  0.02808055,\n        -0.00878728, -0.045735  ]])\n\n.base_values =\narray([3.79460651, 3.79460651, 3.79460651, ..., 3.79460651, 3.79460651,\n       3.79460651])\n\n.data =\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n20.3.0.1 Returning just the values\nIt can be useful to have access to just the shap values in an object as they are required as the input to some steps.\nNote that we have used ‘shap_values’ as the variable to store the output of explainer(). So we will need to give it another name!\n\nshap_values_numeric = shap_values.values\nshap_values_numeric\n\narray([[ 0.00764417,  0.        , -0.03838515, ...,  0.02178417,\n        -0.01230581, -0.05431815],\n       [ 0.00406493,  0.        , -0.08355626, ...,  0.02667989,\n        -0.01247949, -0.05238643],\n       [-0.00260847,  0.        , -0.07398675, ...,  0.02891657,\n        -0.01341456, -0.05173721],\n       ...,\n       [-0.00061298,  0.        , -0.03833514, ...,  0.02290138,\n        -0.0153973 , -0.05772406],\n       [-0.00180168,  0.        , -0.06790128, ...,  0.02414472,\n        -0.01215895, -0.05733014],\n       [ 0.00211131,  0.        , -0.04626162, ...,  0.02808055,\n        -0.00878728, -0.045735  ]])\n\n\n\n\n20.3.0.2 Feature Table\n\n# get feature importance for comparison using MDI method\nfeatures = list(X_train)\nfeature_importances = regr_xgb.feature_importances_\nimportances = pd.DataFrame(index=features)\nimportances['importance'] = feature_importances\nimportances['rank'] = importances['importance'].rank(ascending=False).values\nimportances.sort_values('rank').head()\n\n# Get shapley importances\n# Calculate mean Shapley value for each feature in trainign set\nimportances['mean_shapley_values'] = np.mean(\n    shap_values_numeric, axis=0\n    )\n\n# Calculate mean absolute Shapley value for each feature in trainign set\n# This will give us the average importance of each feature\nimportances['mean_abs_shapley_values'] = np.mean(\n    np.abs(shap_values_numeric), axis=0\n    )\n\nimportances\n\n\n\n\n\n\n\n\n\nimportance\nrank\nmean_shapley_values\nmean_abs_shapley_values\n\n\n\n\ngender\n0.000152\n33.0\n0.000497\n0.003516\n\n\ndialysisrenalendstage\n0.004975\n23.0\n0.022847\n0.022847\n\n\nasthma\n0.004017\n25.0\n-0.029080\n0.084459\n\n\nirondef\n0.016412\n9.0\n-0.016085\n0.134217\n\n\npneum\n0.006473\n16.0\n0.027612\n0.027612\n\n\nsubstancedependence\n0.005187\n19.0\n-0.031714\n0.132673\n\n\npsychologicaldisordermajor\n0.014801\n10.0\n0.014358\n0.361950\n\n\ndepress\n0.002699\n28.0\n-0.025356\n0.082002\n\n\npsychother\n0.001460\n29.0\n0.009927\n0.046745\n\n\nfibrosisandother\n0.002701\n27.0\n-0.001231\n0.006100\n\n\nmalnutrition\n0.005148\n20.0\n0.012160\n0.029192\n\n\nhemo\n0.020425\n8.0\n-0.007621\n0.122109\n\n\nhematocrit\n0.008906\n12.0\n0.027616\n0.265825\n\n\nneutrophils\n0.003719\n26.0\n0.006639\n0.125201\n\n\nsodium\n0.005029\n21.0\n0.018857\n0.254019\n\n\nglucose\n0.005008\n22.0\n0.052205\n0.235265\n\n\nbloodureanitro\n0.007172\n15.0\n0.038006\n0.056848\n\n\ncreatinine\n0.004722\n24.0\n-0.030582\n0.275831\n\n\nbmi\n0.005255\n18.0\n0.033641\n0.254814\n\n\npulse\n0.006221\n17.0\n-0.011668\n0.261527\n\n\nrespiration\n0.008615\n13.0\n0.003487\n0.236613\n\n\nsecondarydiagnosisnonicd9\n0.000228\n32.0\n0.000217\n0.002621\n\n\nA\n0.001182\n30.0\n-0.005640\n0.031220\n\n\nB\n0.000977\n31.0\n-0.003409\n0.042982\n\n\nC\n0.008053\n14.0\n-0.006742\n0.026159\n\n\nD\n0.028479\n7.0\n-0.000704\n0.021886\n\n\nE\n0.104948\n3.0\n-0.001272\n0.065105\n\n\nrcount_0\n0.298638\n1.0\n0.142340\n1.741959\n\n\nrcount_1\n0.223313\n2.0\n-0.021455\n0.514860\n\n\nrcount_2\n0.095113\n4.0\n-0.020626\n0.171157\n\n\nrcount_3\n0.044833\n5.0\n-0.003072\n0.054635\n\n\nrcount_4\n0.014782\n11.0\n0.007687\n0.031047\n\n\nrcount_5+\n0.040360\n6.0\n-0.015835\n0.098719\n\n\n\n\n\n\n\n\n\nimportance_top_10 = \\\n    importances.sort_values(\n        by='importance', ascending=False\n        ).head(10).index\n\nshapley_top_10 = \\\n    importances.sort_values(\n        by='mean_abs_shapley_values',\n        ascending=False).head(10).index\n\n# Add to DataFrame\ntop_10_features = pd.DataFrame()\ntop_10_features['importances'] = importance_top_10.values\ntop_10_features['Shapley'] = shapley_top_10.values\n\n# Display\ntop_10_features\n\n\n\n\n\n\n\n\n\nimportances\nShapley\n\n\n\n\n0\nrcount_0\nrcount_0\n\n\n1\nrcount_1\nrcount_1\n\n\n2\nE\npsychologicaldisordermajor\n\n\n3\nrcount_2\ncreatinine\n\n\n4\nrcount_3\nhematocrit\n\n\n5\nrcount_5+\npulse\n\n\n6\nD\nbmi\n\n\n7\nhemo\nsodium\n\n\n8\nirondef\nrespiration\n\n\n9\npsychologicaldisordermajor\nglucose\n\n\n\n\n\n\n\n\n\n\n20.3.1 Global: Beeswarm\n\n# summarize the effects of all the features\nshap.plots.beeswarm(shap_values, max_display=25)\n\n\n\n\n\n\n\n\n\n\n20.3.2 Global: Bar\n\nshap.plots.bar(shap_values, max_display=20)\n\n\n\n\n\n\n\n\n\n20.3.2.1 Bar: by another factor\nHere we are creating a bar chart by another cohort.\nWe can see there is almost no gender difference in this dataset.\n\nsex = [\"Women\" if shap_values[i, \"gender\"].data == 1 else \"Men\" for i in range(shap_values.shape[0])]\nshap.plots.bar(shap_values.cohorts(sex).abs.mean(0))\n\n\n\n\n\n\n\n\n\nreadmission_status = [\"Not Readmitted\" if shap_values[i, \"rcount_0\"].data == 1 else \"Readmitted\" for i in range(shap_values.shape[0])]\nshap.plots.bar(shap_values.cohorts(readmission_status).abs.mean(0), max_display=25)\n\n\n\n\n\n\n\n\n\n\n\n20.3.3 Local: Waterfall Plots\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0])\n\n\n\n\n\n\n\n\n\n# visualize another prediction's explanation\nshap.plots.waterfall(shap_values[7], max_display=15)\n\n\n\n\n\n\n\n\n\n20.3.3.1 Visualise an example with a high LoS\n\nhighest_los = los_df.sort_values('lengthofstay', ascending=False).head(1)\n\nhigh_los_index = highest_los.index\nhighest_los.lengthofstay\n\neid\n7493    17\nName: lengthofstay, dtype: int64\n\n\n\nshap.plots.waterfall(shap_values[7493], max_display=15)\n\n\n\n\n\n\n\n\n\n\n20.3.3.2 Visualise an example with a low LoS\n\nshap.plots.waterfall(shap_values[y_test.reset_index(drop=True).sort_values().head(1).index[0]], max_display=20)\n\n\n\n\n\n\n\n\n\n\n20.3.3.3 Visualise an example with a high LoS\n\nshap.plots.waterfall(shap_values[y_test.reset_index(drop=True).sort_values().tail(1).index[0]], max_display=20)\n\n\n\n\n\n\n\n\n\n\n\n20.3.4 Local: Force Plots\n\n# visualize the first prediction's explanation with a force plot\nshap.plots.force(shap_values[0])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.plots.force(shap_values[y_test.reset_index(drop=True).sort_values().head(1).index[0]])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.plots.force(shap_values[y_test.reset_index(drop=True).sort_values().tail(1).index[0]])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n\n0        3.631342\n1        0.830419\n2        2.480067\n3        1.077456\n4        5.112955\n           ...   \n24995    7.090035\n24996    1.122499\n24997    5.535420\n24998    1.588132\n24999    5.249632\nLength: 25000, dtype: float32\n\n\n\n20.3.4.1 Visualise the lowest predicted LoS\n\nshap.plots.force(shap_values[pd.Series(y_pred_test).sort_values().head(1).index[0]])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n20.3.4.2 Visualise the highest predicted LoS\n\nshap.plots.force(shap_values[pd.Series(y_pred_test).sort_values().tail(1).index[0]])\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n\n20.3.5 Global: Force Plots\n\n# visualize all the predictions\n# this struggles with a large number of values so we'll sample a small set\nshap.plots.force(shap.utils.sample(shap_values, 1000))\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n20.3.6 Dependence Plots\n\n20.3.6.1 Simple scatter of a single feature\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, 'respiration'])\n\n\n\n\n\n\n\n\n\n\n\n20.3.7 Scatter of multiple features\nPassing in shap_values to the colour will colour the value by the most strongly interacting other value.\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"gender\"], color=shap_values)\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"respiration\"], color=shap_values)\n\n\n\n\n\n\n\n\nAlternatively we can choose to colour by a specific column.\n\nX.columns\n\nIndex(['gender', 'dialysisrenalendstage', 'asthma', 'irondef', 'pneum',\n       'substancedependence', 'psychologicaldisordermajor', 'depress',\n       'psychother', 'fibrosisandother', 'malnutrition', 'hemo', 'hematocrit',\n       'neutrophils', 'sodium', 'glucose', 'bloodureanitro', 'creatinine',\n       'bmi', 'pulse', 'respiration', 'secondarydiagnosisnonicd9', 'A', 'B',\n       'C', 'D', 'E', 'rcount_0', 'rcount_1', 'rcount_2', 'rcount_3',\n       'rcount_4', 'rcount_5+'],\n      dtype='object')\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"sodium\"], color=shap_values[:,\"glucose\"])\n\n\n\n\n\n\n\n\n\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, \"rcount_1\"], color=shap_values[:,\"gender\"])\n\n\n\n\n\n\n\n\n\n\n20.3.8 BONUS: SHAP interactions\nFull details of SHAP interactions can be found here: https://michaelallen1966.github.io/titanic/90_shap_interactions_on_titanic.html\nFirst we need to get the interaction values from the explainer object. Like before, we pass in our foreground data. The interactions take a long time to calculate, even using xgboost, so I’ve just asked for 1000.\n\nshap_interaction = explainer.shap_interaction_values(shap.utils.sample(X, 1000))\n\n\nshap_interaction.shape\n\n(1000, 33, 33)\n\n\n\n20.3.8.1 Table\n\nmean_abs_interactions = pd.DataFrame(\n    np.abs(shap_interaction).mean(axis=(0)),\n    index=X.columns, columns=X.columns)\n\nmean_abs_interactions.round(2)\n\n\n\n\n\n\n\n\n\ngender\ndialysisrenalendstage\nasthma\nirondef\npneum\nsubstancedependence\npsychologicaldisordermajor\ndepress\npsychother\nfibrosisandother\n...\nB\nC\nD\nE\nrcount_0\nrcount_1\nrcount_2\nrcount_3\nrcount_4\nrcount_5+\n\n\n\n\ngender\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\ndialysisrenalendstage\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nasthma\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nirondef\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\npneum\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nsubstancedependence\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\npsychologicaldisordermajor\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\ndepress\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\npsychother\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nfibrosisandother\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nmalnutrition\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nhemo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nhematocrit\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nneutrophils\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nsodium\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nglucose\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nbloodureanitro\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\ncreatinine\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nbmi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\npulse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrespiration\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nsecondarydiagnosisnonicd9\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nA\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nB\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nC\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nD\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nE\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_5+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n33 rows × 33 columns\n\n\n\n\n\n\n\n20.3.9 Interactions with most important features only\nHere we’ve created an scatterplot matrix showing the interactions for the four most important features.\n\nfeatures = shapley_top_10.values[:4]\n\n# limit the dataframe to just those features\nsubset_interactions = mean_abs_interactions[features] # limit columns\nsubset_interactions = subset_interactions[subset_interactions.index.isin(features)]\nsubset_interactions\n\n\n\n\n\n\n\n\n\nrcount_0\nrcount_1\npsychologicaldisordermajor\ncreatinine\n\n\n\n\npsychologicaldisordermajor\n0.0\n0.0\n0.0\n0.0\n\n\ncreatinine\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_0\n0.0\n0.0\n0.0\n0.0\n\n\nrcount_1\n0.0\n0.0\n0.0\n0.0",
    "crumbs": [
      "4G - Explainable AI",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Exercise Solution: Explainable AI (LOS Dataset)</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html",
    "href": "4i_synthetic_data_SMOTE.html",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "",
    "text": "21.1 Description of SMOTE\nSMOTE stands for Synthetic Minority Oversampling Technique [1]. SMOTE is most commonly used to create additional data to enhance modelling fitting, especially when one or more classes have low prevalence in the data set. Hence the description of oversampling.\nSMOTE works by finding near-neighbor points in the original data, and creating new data points from interpolating between two near-neighbor points.\nIn this example, we remove the real data used to create the synthetic data, leaving only the synthetic data. This is the process you would go through if you were looking to use synthetic data INSTEAD OF real data. After generating synthetic data we remove any data points that, by chance, are identical to original real data points, and also remove 10% of points that are closest to the original data points. We measure ‘closeness’ by the Cartesian distance between standardised data values.\nDemonstration of SMOTE method. (a) Data points with two features (shown on x and y axes) are represented. Points are colour-coded by class label. (b) A data point from a class is picked at random, shown by the black point, and then the closest neighbours of the same class are identified, as shown by yellow points. Here we show 3 closest neighbours, but the default in the SMOTE Imbalanced-Learn library is 6. One of those near-neighbour points is selected at random (shown by the dark brown point). A new data point, shown in red, is created at a random distance between the two selected data points.",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#description-of-smote",
    "href": "4i_synthetic_data_SMOTE.html#description-of-smote",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "",
    "text": "21.1.1 Handling integer, binary, and categorical data\nThe standard SMOTE method generates floating point (non-integer) values between data points. There are alternative ways of handing integer, binary, and categorical data using the SMOTE method. Here the methods we use are:\n\nInteger values: Round the resulting synthetic data point value to the closest integer.\nBinary: Code the value as 0 or 1, and round the resulting synthetic data point value to the closest integer (0 or 1).\nCategorical: One-hot encode the categorical feature. Generate the synthetic data for each category value. Identify the category with the highest value and set to 1 while setting all others to 0.\n\n\n\n21.1.2 Implementation with IMBLEARN\nHere use the implementation in the IMBLEARN IMBALANCED-LEARN [2]\n[1] Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P. “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.\n[2] Lemaitre, G., Nogueira, F. and Aridas, C. (2016), Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning. arXiv:1609.06570 (https://pypi.org/project/imbalanced-learn/, pip install imbalanced-learn).\nMany of the initial steps in this notebook will be familiar to you from the earlier Machine Learning sessions, and therefore there is not an extensive explanation of these sections.\nFirst, we will fit a Logistic Regression model using the real Titanic data, just as we did before. Then, we’ll create some synthetic data from the Titanic data, and use that to fit a Logistic Regression model instead. We’ll compare performance on the two models - one using the real data, and one using synthetic data.",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#load-packages",
    "href": "4i_synthetic_data_SMOTE.html#load-packages",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "21.2 Load packages",
    "text": "21.2 Load packages\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Import machine learning methods\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.neighbors import NearestNeighbors\n\n# Import package for SMOTE\nimport imblearn\n\n# import SMOTE from imblearn so we can use it\nfrom imblearn.over_sampling import SMOTE\n\n# Turn warnings off to keep notebook clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#load-and-process-data",
    "href": "4i_synthetic_data_SMOTE.html#load-and-process-data",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "21.3 Load and process data",
    "text": "21.3 Load and process data\n\n21.3.1 Load data\nThe section below downloads pre-processed data, and saves it to a subfolder (from where this code is run). If data has already been downloaded that cell may be skipped (though if in doubt, run it - it takes seconds).\nCode that was used to pre-process the data ready for machine learning may be found at: https://michaelallen1966.github.io/titanic/01_preprocessing.html\n\ndownload_required = True\n\nif download_required:\n\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)\n\n\ndata = pd.read_csv('data/processed_data.csv')\n# Make all data 'float' type and drop Passenger ID\ndata = data.astype(float)\ndata.drop('PassengerId', axis=1, inplace=True) # Remove passenger ID\n\n\n# Record number in each class\nnumber_died = np.sum(data['Survived'] == 0)\nnumber_survived = np.sum(data['Survived'] == 1)\n\n\n\n21.3.2 Divide into X (features) and y (labels)\n\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'\n\n\n\n21.3.3 Divide into training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\n\n\n21.3.4 Show examples from the training data.\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n705\n2.0\n39.0\n0.0\n0.0\n26.0000\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n306\n1.0\n28.0\n0.0\n0.0\n110.8833\n1.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n240\n3.0\n28.0\n1.0\n0.0\n14.4542\n1.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n237\n2.0\n8.0\n0.0\n2.0\n26.2500\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n381\n3.0\n1.0\n0.0\n2.0\n15.7417\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\n\n21.3.5 Standardise data\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.fit_transform(X_train)\n    test_std=sc.fit_transform(X_test)\n\n    return train_std, test_std\n\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#fit-and-test-logistic-regression-model-on-real-data",
    "href": "4i_synthetic_data_SMOTE.html#fit-and-test-logistic-regression-model-on-real-data",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "21.4 Fit and test logistic regression model on real data",
    "text": "21.4 Fit and test logistic regression model on real data\n\n21.4.1 Fit model\nWe will fit a logistic regression model, using sklearn’s LogisticRegression method.\n\nmodel = LogisticRegression()\nmodel.fit(X_train_std,y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n21.4.2 Predict values\nNow we can use the trained model to predict survival. We will test the accuracy of both the training and test data sets.\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train_std)\ny_pred_test = model.predict(X_test_std)\n\n\n\n21.4.3 Calculate accuracy\nHere we measure accuracy simply as the proportion of passengers where we make the correct prediction (later we will use Receiver Operator Characteristic curves for a more thorough analysis).\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train:0.3f}')\nprint (f'Accuracy of predicting test data = {accuracy_test:0.3f}')\n\nAccuracy of predicting training data = 0.808\nAccuracy of predicting test data = 0.821",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#make-synthetic-data",
    "href": "4i_synthetic_data_SMOTE.html#make-synthetic-data",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "21.5 Make Synthetic data",
    "text": "21.5 Make Synthetic data\n\n21.5.1 Function to create synthetic data\nThis function generates synthetic data for feature (X) and label (y) data in a binary classification problem. We pass the feature and label data when we call the number of functions, and (optionally) the number of synthetic data points that we want to create for each class.\nWe first count the number in each class in the data passed into the function. The SMOTE implementation in the imbalanced-learn library requires us to specify the TOTAL number of data points we will end up with for each class. This is the total number we already have for each class + the number of synthetic data points we want to create. In this example, we’re only going to use the synthetic data points as we’re using them to replace the existing data, rather than augment, but we still have to go through the same process.\nWe then tell SMOTE to generate some synthetic data points for each class from the original data points using the fit_resample() method.\nOnce the new synthetic data points have been generated, we have two NumPy arrays - one for each class - which contain all the real data points for that class followed by all the synthetic points for that class. Here, we don’t need the real data points, as we’re going to use the synthetic data points to replace this original data. So we’ll simply grab out the synthetic data (we can work out where it starts from in each class from the number of real data points, which are the first points in the arrays), and we’ll return these NumPy arrays.\n\ndef make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n    \"\"\"\n    Synthetic data generation for two classes.\n\n    Inputs\n    ------\n    original_data: X, y numpy arrays (y should have label 0 and 1)\n    number_of_samples: number of samples to generate (list for y=0, y=1)\n    (Note - number_of_samples has default of 1000 samples for each class\n    if no numbers are specified at the point of calling the function)\n\n    Returns\n    -------\n    X_synthetic: NumPy array\n    y_synthetic: NumPy array\n\n    \"\"\"\n\n    # Count instances in each class\n    count_label_0 = np.sum(y==0)\n    count_label_1 = np.sum(y==1)\n\n    # SMOTE requires final class counts; add current counts to required counts\n    # (which are passed into the function)\n    n_class_0 = number_of_samples[0] + count_label_0\n    n_class_1 = number_of_samples[1] + count_label_1\n\n    # Use SMOTE to sample data points.  The number of points that we pass over\n    # to SMOTE is calculated above (the number of synthetic data samples we\n    # want, which we passed into the function + the counts from the original\n    # data).  This tells SMOTE how many TOTAL data points are needed (original\n    # + synthetic) for each class.  It then uses the original data to generate\n    # new synthetic data points.\n    # For example, imagine our original data has 100 samples for class 0 and 50\n    # for class 1, and we tell SMOTE we want 100 synthetic data points for\n    # class 0 and 150 synthetic data points for class 1.  We tell SMOTE that we\n    # need a total of 200 data points for class 0 (100 original + 100 synthetic)\n    # and 200 data points for class 1 (50 original + 150 synthetic).  It will\n    # then fill those data points by taking the original data (which will fill\n    # up the first 100 \"slots\" for class 0 & the first 50 \"slots\" for class 1)\n    # and then use the original data points to sample new synthetic data points\n    # to fill the remaining \"slots\" in each class.\n    X_resampled, y_resampled = SMOTE(\n        sampling_strategy = {0:n_class_0, 1:n_class_1}).fit_resample(X, y)\n\n    # Get just the additional (synthetic) data points.  By using len(X) for the\n    # X (input feature) data, and len(y) for the y (output label) data, we skip\n    # the original data, and just start from the newly created synthetic data,\n    # generated by SMOTE (above)\n    X_synthetic = X_resampled[len(X):]\n    y_synthetic = y_resampled[len(y):]\n\n    return X_synthetic, y_synthetic\n\n\n\n21.5.2 Generate raw synthetic data\nAs we’re using synthetic data to replace the original data, we want to keep the same ratios of survived vs died. However, we will generate twice as much raw synthetic data for each class as the current data has. This will give us leeway to allow us to remove points that are identical to, or close to, original data.\nOnce we’ve calculated what those numbers are, we’ll pass them in to the function we wrote above to generate our synthetic data.\n\n# Get counts of classes from y_train\nunique, original_frequency = np.unique(y_train, return_counts = True)\nrequired_smote_count = list(original_frequency * 2)\n\n\n# Call the function we wrote above to generate and extract the synthetic data\nX_synthetic, y_synthetic = make_synthetic_data_smote(\n        X_train, y_train, number_of_samples=required_smote_count)\n\n\n\n21.5.3 Processing of raw synthetic data\n\n21.5.3.1 Prepare lists of categorical, integer, and binary features\nSMOTE will generate floating point numbers for everything. If we have features that are integers, binary or one-hot encoded categorical, we’ll need to convert the synthetic data points for these features in different ways.\nThe first step is to specify the features (columns) that are integer, binary or categorical rather than float.\n\n# Get full list of column names (the names of our features)\nX_col_names = list(X_train)\n\n# Set categorical one-hots cols using common prefix\n# First, let's set up the categorical columns, which we'll need to\n# \"one hot encode\".  We've got two categorical features in the\n# Titanic data - where they embarked, and their cabin letter.\n# Here, we'll use some code to grab out all the categorical columns\n# (remember, they're set up to be one hot encoded in the original data,\n# so if there are three places from which a passenger can embark, then\n# there are three columns for the embarked feature, and one of them will\n# have a 1 value, whilst the others will have a 0 value.).\n# We do this here by giving the common prefix (start of the name) of the\n# columns we want, and then use a list comprehension to find all column\n# names that start with that prefix, and store those in a list of one hot\n# columns.  Remember, strings (such as the names of columns here) can be\n# treated as lists of characters (so x[0] would give the first character)\n# The list comprehension code below may look confusing initially, but it\n# basically says \"give me the column name if it starts with \"Embarked_\" (in the\n# first iteration of the loop) or \"CabinLetter_\" (in the second iteration of\n# the loop).  That will grab out all of our one-hot encoded categorical columns,\n# and it'll do so as two lists - one with the list of column names relating to\n# where they embarked, and one with the list of column names relating to their\n# cabin letter.\ncategorical = ['Embarked_', 'CabinLetter_']\none_hot_cols = []\nfor col in categorical:\n    one_hot_cols.append([x for x in X_col_names if x[0:len(col)] == col])\n\n# Set integer columns\ninteger_cols = ['Pclass',\n                'Age',\n                'Parch',\n                'Fare',\n                'SibSp',\n                'CabinNumber']\n\n# Set binary columns\nbinary_cols = ['male',\n               'AgeImputed',\n               'EmbarkedImputed',\n               'CabinNumberImputed']\n\n\n\n21.5.3.2 Function to process raw synthetic categorical data to one-hot encoded\nWe’ll write a function where we can pass in an array of floating point numbers, and it’ll find the highest, set that to a value of 1, and the rest to a value of 0. We can then use this function to do this for the synthetic data values for our one-hot encoded categorical columns.\n\ndef make_one_hot(x):\n    \"\"\"\n    Takes a list/array/series and turns it into a one-hot encoded\n    list/array series, by setting 1 for highest value and 0 for all\n    others\n\n    \"\"\"\n    # Get argmax (this returns the index of the highest values in\n    # the list / array / series passed in to the function)\n    highest = np.argmax(x)\n    # Set all values to zero (just multiply all values by 0)\n    x *= 0.0\n    # Set the value that was found to be the highest to 1, by\n    # using the index we found using argmax above\n    x[highest] = 1.0\n\n    return x\n\n\n\n21.5.3.3 Process raw synthetic data\nNow we have the raw synthetic data, we need to process it so it can be used (primarily sorting out the integer, binary and categorical columns).\nSpecifically, we will :\n\nTransfer data to a DataFrame and add column names\nProcess one-hot categorical data fields\nProcess integer data fields\nProcess binary data fields\nAdd y data with label\nShuffle data\n\n\n# Set y_label (our outcome column)\ny_label = 'Survived'\n\n# Create a data frame to store the synthetic data\nsynth_df = pd.DataFrame()\n\n# Transfer X (feature) values to the new DataFrame\nsynth_df=pd.concat([synth_df,\n                    pd.DataFrame(X_synthetic, columns=X_col_names)],\n                    axis=1)\n\n# Make columns (that need to be) one hot encoded using the\n# function we wrote above, using the raw synthetic data\n# For each sublist of one hot columns we specified (ie the embarked list and\n# the cabin letter list)\nfor one_hot_col_name_list in one_hot_cols:\n    # For each new synthetic \"passenger\"\n    for index, row in synth_df.iterrows():\n        # Grab the list of synthetic data points for the column names that are\n        # one-hot encoded in this group for this new synthetic data \"passenger\"\n        x = row[one_hot_col_name_list]\n        # One hot encode these columns for this new synthetic \"passenger\" using\n        # the funciton we wrote above\n        x_one_hot = make_one_hot(x)\n        # Replace the values in the columns with the one hot encoded values\n        # (overwriting the raw floating point numbers generated)\n        row[x_one_hot.index]= x_one_hot.values\n\n# Make integer as necessary by rounding the raw synthetic floating point data\n# for those columns that are supposed to be integers\nfor col in integer_cols:\n    synth_df[col] = synth_df[col].round(0)\n\n# Round binary columns and clip them so values &lt; 0 or &gt; 1 are set to 0 and 1\n# respectively (this won't happen with SMOTE, as it will only sample between the\n# two points (so points sampled between binary points will always be between 0\n# and 1) but it can happen with other methods, so it's worth getting into the\n# habit of doing this)\nfor col in binary_cols:\n    synth_df[col] = np.clip(synth_df[col],0,1).round(0)\n\n# Add the y (label) data to our synthetic dataframe.  We can leave this raw as\n# they would be floating point numbers anyway.\ny_list = list(y_synthetic)\nsynth_df[y_label] = y_list\n\n# Shuffle up the data, a bit like shuffling a pack of cards.\nsynth_df = synth_df.sample(frac=1.0)\n\nLet’s have a look at a sample of our synthetic data (remember, we’ve shuffled it, so the head will be a random sample)\n\nsynth_df.head()\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\nSurvived\n\n\n\n\n1013\n1.0\n68.0\n0.0\n0.0\n32.0\n0.0\n0.0\n0.314517\n3.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n897\n2.0\n49.0\n0.0\n0.0\n14.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1655\n2.0\n44.0\n1.0\n1.0\n26.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n1859\n1.0\n55.0\n0.0\n0.0\n34.0\n0.0\n0.0\n0.000000\n25.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n677\n2.0\n18.0\n0.0\n0.0\n11.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\n\n\n21.5.4 Remove synthetic data that is a duplication of original data or close to original data\nNow we’ve created and processed our synthetic data, we need to look for and remove data that is too close (or identical) to data in the original (real) data.\nFor each synthetic data point, we’ll find the nearest neighbour in the real data set (based on Cartesian distance of standardised data).\n\n# Standardise real and synthetic data (standardise based on data in the real\n# training data)\nX_train_std, X_synth_std = standardise_data(X_train, X_synthetic)\n\n# Get ALL real X data (combine standardised training + test data)\n# We do this because we need to check for duplicates / very close\n# values in ALL of the real data we've got\nX_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n\n# Use SciKitLearn neighbors.NearestNeighbors to find nearest neighbour\n# to each data point. First, we fit to the real standardised data\n# (all of it, train + test set).  Then we can give it the synthetic data\n# and ask it to give us the cartesian distance and ID of its nearest\n# real world data point neighbour for each synthetic data point.\nnn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\ndists, idxs = nn.kneighbors(X_synth_std)\n\n# Store the distances and data point ids (indices) in the synthetic DataFrame\n# as two new columns.  This means, for each synthetic \"passenger\", we'll be\n# able to see the closest real \"passenger\", and how close it is.\n# Flatten just reduces something in more than 1 dimension down to\n# 1 dimension (eg a list of lists becomes a single list)\nsynth_df['distance_to_closest_real'] = list(dists.flatten())\nsynth_df['closest_X_real_row_index'] = list(idxs.flatten())\n\nLet’s have a peek at our synthetic data. Observe the two new columns on the far right (scroll across), which now tells us how close the nearest neighbouring real world data point is (along with it’s id (index) so we can look it up) for each synthetic data point.\n\nsynth_df\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\nSurvived\ndistance_to_closest_real\nclosest_X_real_row_index\n\n\n\n\n1013\n1.0\n68.0\n0.0\n0.0\n32.0\n0.0\n0.0\n0.314517\n3.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.174705\n335\n\n\n897\n2.0\n49.0\n0.0\n0.0\n14.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n2.363159\n231\n\n\n1655\n2.0\n44.0\n1.0\n1.0\n26.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.176998\n504\n\n\n1859\n1.0\n55.0\n0.0\n0.0\n34.0\n0.0\n0.0\n0.000000\n25.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.329158\n499\n\n\n677\n2.0\n18.0\n0.0\n0.0\n11.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.352447\n310\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1322\n2.0\n48.0\n0.0\n0.0\n13.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.399161\n771\n\n\n896\n3.0\n22.0\n0.0\n0.0\n8.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n3.223347\n559\n\n\n789\n3.0\n27.0\n1.0\n0.0\n14.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.511640\n876\n\n\n1370\n3.0\n22.0\n0.0\n0.0\n8.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.493489\n569\n\n\n1200\n3.0\n30.0\n0.0\n0.0\n8.0\n0.0\n0.0\n1.000000\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.258553\n307\n\n\n\n\n1336 rows × 27 columns\n\n\n\n\n\n21.5.4.1 Remove identical points\nLet’s now get rid of any synthetic data points that are “identical” (or effectively identical) to real data points. You may find you don’t have, depending on how SMOTE generated the points (remember, it’s random).\n\n# Get points (\"passengers\") that are identical to real ones (use distance of\n# &lt;0.001 as effectively identical)\nidentical = synth_df['distance_to_closest_real'] &lt; 0.001\n\nprint (f\"Proportion of data points identical to real data points =\",\n       f\"{identical.mean():0.3f}\")\n\n# Remove synthetic data points considered \"identical\" (as per our definition\n# above).  We do this by setting up a mask that says we only want to see data\n# points where the \"identical\" criterion we specified above is false (ie they're\n# not identical).  Then we apply that mask and overwrite our existing synthetic\n# data DataFrame so we've now only got data points that are NOT identical to\n# real world data points.\nmask = identical == False\nsynth_df = synth_df[mask]\n\nProportion of data points identical to real data points = 0.000\n\n\n\n\n21.5.4.2 Remove closest points to original data\nNow we’ve removed points that are identical, we’re also going to remove the 10% of points that are closest to the original data.\nWe’ll sort our synthetic data points so that those with the highest distances from the nearest real data point (ie those that we want to keep) are at the top. Then we can just keep the first 90% of data points (thereby removing the 10% that are the closest). Depending on your data, you might want to play around with this in real world applications.\n\n# Proportion of points to remove\nproportion_to_remove = 0.1\n\n# Sort by distance, with highest distances (those we want to keep) at\n# the top\nsynth_by_distance = synth_df.sort_values(\n    'distance_to_closest_real', ascending=False)\n\n# Limit data.  Calculate the number of entries to keep as being the\n# total number of synthetic data points we've now got (after having\n# removed ones identical to real world data points) multiplied by\n# the proportion we want to keep (the inverse of the proportion to remove).\n# As we've sorted in descending order by distance, we can then just\n# use .head to identify how much of the top of list we want to keep\n# (90% in this case, where we're removing the 10% that are closest - at\n# the bottom)\nnumber_to_keep = int(len(synth_by_distance) * (1 - proportion_to_remove))\nsynth_by_distance = synth_by_distance.head(number_to_keep)\n\n# Shuffle and store back in synth_df (frac=1 gives us a sample size of 100%\n# (ie - all of the ones we said above we wanted to keep))\nsynth_df = synth_by_distance.sample(frac=1)\n\n\n\n\n21.5.5 Show five examples with their closest data points in the original data\nLet’s have a look at a random sample of five synthetic “passengers” and see how they compare to the nearest “real world” passenger. We use raw data here (rather than standardise data) to make it a bit more readable. Note - sometimes it may look like a synthetic passengers bears no resemblance to its nearest real world neighbour - remember, the distance is calculated in multi-dimensional space (24-dimensional in this example, as we have 24 features) taking into account all feature values, so it may not be so obvious by eye (unless you’re very good at being able to think in 24-dimensional space :))\n\n# Reproduce X_real but with non-standardised (ie the raw original) values for\n# comparison\nX_real = np.concatenate([X_train, X_test], axis=0)\n\n# Set up Data Frame for comparison\ncomparison = pd.DataFrame(index=X_col_names)\n\n# Generate five examples\nfor i in range(5):\n    # Get synthetic data sample (sample size of 1 - one data point)\n    sample = synth_df.sample(1)\n    comparison[f'Synthetic_{i+1}'] = sample[X_col_names].values[0]\n    # Get closest point from the real data (remember we stored earlier\n    # the index of the closest real world point, so we can grab it out\n    # easily here)\n    closest_id = sample['closest_X_real_row_index']\n    comparison[f'Synthetic_{i+1}_closest'] = X_real[closest_id, :][0]\n\n# Display the comparisons\ncomparison.round(0)\n\n\n\n\n\n\n\n\n\nSynthetic_1\nSynthetic_1_closest\nSynthetic_2\nSynthetic_2_closest\nSynthetic_3\nSynthetic_3_closest\nSynthetic_4\nSynthetic_4_closest\nSynthetic_5\nSynthetic_5_closest\n\n\n\n\nPclass\n2.0\n1.0\n3.0\n3.0\n1.0\n3.0\n3.0\n1.0\n3.0\n1.0\n\n\nAge\n28.0\n36.0\n59.0\n31.0\n28.0\n22.0\n28.0\n49.0\n30.0\n39.0\n\n\nSibSp\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nParch\n0.0\n2.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nFare\n13.0\n71.0\n15.0\n8.0\n79.0\n7.0\n8.0\n26.0\n7.0\n80.0\n\n\nAgeImputed\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\nEmbarkedImputed\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetterImputed\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\nCabinNumber\n0.0\n22.0\n0.0\n0.0\n85.0\n0.0\n0.0\n17.0\n0.0\n67.0\n\n\nCabinNumberImputed\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\nmale\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\nEmbarked_C\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nEmbarked_Q\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\nEmbarked_S\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\nEmbarked_missing\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_A\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_B\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_C\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_D\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\nCabinLetter_E\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nCabinLetter_F\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_G\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_T\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nCabinLetter_missing\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\n\n21.5.6 Sample from synthetic data to get same size/balance as the original data\nAs the purpose of generating synthetic data in this example is to use it instead of the real data, we want to try to keep the survived : died ratio the same as in the real data. We originally kept the ratio in terms of total numbers of points to generate (albeit double for each), but since then, we’ve removed points that are close or identical. This may well have thrown our ratios out. So let’s now randomly sample from the synthetic data such that we sample the same number of survived and died as in the original data (again, this is why we asked for double the data points for each class, so we’ve got room to do this after getting rid of points that are too close).\nThe sampled synthetic data points will then represent our synthetic data that we will use.\n\n# Randomly sample from the synthetic data those who died,\n# and sample this the same number of times as we had number\n# who died in the real data\nmask = synth_df['Survived'] == 0\nsynth_died = synth_df[mask].sample(number_died)\n\n# The same as above, but for those who survived\nmask = synth_df['Survived'] == 1\nsynth_survived = synth_df[mask].sample(number_survived)\n\n# Reconstruct into synth_df and shuffle\nsynth_df = pd.concat([synth_died, synth_survived], axis=0)\nsynth_df = synth_df.sample(frac=1.0, )\n\nCompare counts with original data. These should be identical for real vs synthetic if the above cell has worked.\n\nprint ('Number of real data survived: ', np.sum(data['Survived'] == 1))\nprint ('Number of synthetic data survived: ', np.sum(synth_df['Survived'] == 1))\nprint ('Number of real data died: ', np.sum(data['Survived'] == 0))\nprint ('Number of synthetic data died: ', np.sum(synth_df['Survived'] == 0))\n\nNumber of real data survived:  342\nNumber of synthetic data survived:  342\nNumber of real data died:  549\nNumber of synthetic data died:  549",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#test-synthetic-data-for-training-a-logistic-regression-model",
    "href": "4i_synthetic_data_SMOTE.html#test-synthetic-data-for-training-a-logistic-regression-model",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "21.6 Test synthetic data for training a logistic regression model",
    "text": "21.6 Test synthetic data for training a logistic regression model\nNote that we created synthetic data using the training portion of our orginal train/test split. We then test the model on the original (real) test data. The data used to create synthetic data is not present in the test data (this would cause leakage of test data into the training data and over-estimate performance - basically, aspects of the test data would be used to create training data points if we did that. So our model would have access to information it shouldn’t).\n\n21.6.1 Fit model using synthetic data and check accuracy\nNow let’s use our synthetic data to train our Logistic Regression model, and compare performance on the one trained with the real data (that we fitted earlier), and the one trained on synthetic data. If this has worked well, then performance for both should be very close (which implies we can use the synthetic data as a suitable replacement for the real data)\n\n# Get X data and standardised\nX_synth = synth_df[X_col_names]\ny_synth = synth_df['Survived'].values\nX_synth_std, X_test_std = standardise_data(X_synth, X_test)\n\n# Fit model\nmodel_synth = LogisticRegression()\nmodel_synth.fit(X_synth_std,y_synth)\n\n# Get predictions of test set\ny_pred_test_synth = model_synth.predict(X_test_std)\n\n# Report accuracy\naccuracy_test_synth = np.mean(y_pred_test_synth == y_test)\n\nprint (f'Accuracy of predicting test data from model trained on real data = {accuracy_test:0.3f}')\nprint (f'Accuracy of predicting test data from model trained on synthetic data = {accuracy_test_synth:0.3f}')\n\nAccuracy of predicting test data from model trained on real data = 0.821\nAccuracy of predicting test data from model trained on synthetic data = 0.785\n\n\n\n\n21.6.2 Receiver Operator Characteristic curves\nNow let’s generate our ROC curves and compare them for the model using real data and the one using synthetic data (refer back to your notes from session 4B : Logistic Regression - Who Would Survive the Titanic?), and also consult Mike Allen’s excellent materials here : https://michaelallen1966.github.io/titanic/08_roc.html\n\ny_probs = model.predict_proba(X_test_std)[:,1]\ny_probs_synthetic = model_synth.predict_proba(X_test_std)[:,1]\n\n\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\nfpr_synth, tpr_synth, thresholds_synth = roc_curve(y_test, y_probs_synthetic)\nroc_auc = auc(fpr, tpr)\nroc_auc_snth = auc(fpr_synth, tpr_synth)\nprint (f'ROC AUC real training data: {roc_auc:0.2f}')\nprint (f'ROC AUC synthetic training data: {roc_auc_snth:0.2f}')\n\nROC AUC real training data: 0.88\nROC AUC synthetic training data: 0.86\n\n\n\nfig, ax = plt.subplots()\n\n# Plot ROC\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nax.plot(fpr,tpr, color='green', label = 'Real training data')\nax.plot(fpr_synth,tpr_synth, color='red', label = 'Synthetic training data')\n\nax.set_title('Titanic survival Receiver Operator Characteristic curve')\n\nax.legend()\n\nfig.savefig('images/synthetic_roc.png')\nfig.show()",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_synthetic_data_SMOTE.html#conclusions",
    "href": "4i_synthetic_data_SMOTE.html#conclusions",
    "title": "21  Creating synthetic Titanic passenger data with SMOTE",
    "section": "21.7 Conclusions",
    "text": "21.7 Conclusions\nHere we have used the SMOTE method to create synthetic data to replace the real Titanic data. We have removed any data points that are identical to the original data, and have also removed 10% of synthetic data points that are closest to original data. We found that SMOTE generated synthetic data that could train a logistic regression model for our problem with minimal loss of accuracy when compared with training with the original data.",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Creating synthetic Titanic passenger data with SMOTE</span>"
    ]
  },
  {
    "objectID": "4i_Exercise_1_Solution.html",
    "href": "4i_Exercise_1_Solution.html",
    "title": "22  Exercise Solution: Synthetic Data (Stroke Thromobolysis Dataset)",
    "section": "",
    "text": "22.0.1 Load Packages\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Import machine learning methods\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.neighbors import NearestNeighbors\n\n# Import package for SMOTE\nimport imblearn\n\n# import SMOTE from imblearn so we can use it\nfrom imblearn.over_sampling import SMOTE\n\n# Turn warnings off to keep notebook clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n22.0.2 Read CSV into Pandas DataFrame\n\ndata = pd.read_csv('data/processed_stroke.csv')\n# Make all data 'float' type and drop ID\ndata = data.astype(float)\ndata.drop('id', axis=1, inplace=True) # Remove ID column\n\n\n\n22.0.3 Record number in each class\n\n# Record number in each class\nnumber_positive_stroke = np.sum(data['stroke'] == 1)\nnumber_negative_stroke = np.sum(data['stroke'] == 0)\n\nprint (f\"Positives : {number_positive_stroke}\")\nprint (f\"Negatives : {number_negative_stroke}\")\n\nPositives : 209\nNegatives : 4700\n\n\n\n\n22.0.4 Divide into X (features) and y (labels)\n\nX = data.drop('stroke',axis=1) # X = all 'data' except the 'stroke' column\ny = data['stroke'] # y = 'stroke' column from 'data'\n\n\n\n22.0.5 Divide into training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\n\n\n22.0.6 Show examples from the training data\n\nX_train.head()\n\n\n\n\n\n\n\n\n\ngender_male\ngender_female\ngender_other\nage\nhypertension\nheart_disease\never_married\nwork_type_private\nwork_type_self_employed\nwork_type_govt_job\nwork_type_children\nwork_type_never_worked\nresidence_type_rural\navg_glucose_level\nbmi\nsmoking_status_smokes\nsmoking_status_formerly_smoked\nsmoking_status_never_smoked\nsmoking_status_unknown\n\n\n\n\n4502\n1.0\n0.0\n0.0\n55.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n97.68\n47.1\n0.0\n1.0\n0.0\n0.0\n\n\n1752\n1.0\n0.0\n0.0\n18.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n89.61\n22.0\n0.0\n0.0\n1.0\n0.0\n\n\n4317\n1.0\n0.0\n0.0\n3.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n72.76\n18.8\n0.0\n0.0\n0.0\n1.0\n\n\n1538\n0.0\n1.0\n0.0\n53.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n83.79\n44.0\n0.0\n0.0\n0.0\n1.0\n\n\n1912\n0.0\n1.0\n0.0\n50.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n95.01\n26.2\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n22.0.7 Standardise data\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Set up the scaler just on the training set\n    sc.fit(X_train)\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.transform(X_train)\n    test_std=sc.transform(X_test)\n\n    return train_std, test_std\n\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)\n\n\n\n22.0.8 Fit Logistic Regression model\n\nmodel = LogisticRegression()\nmodel.fit(X_train_std,y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\n22.0.9 Use fitted model to make predictions on training and test set data\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train_std)\ny_pred_test = model.predict(X_test_std)\n\n\n\n22.0.10 Calculate accuracy\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train:0.3f}')\nprint (f'Accuracy of predicting test data = {accuracy_test:0.3f}')\n\nAccuracy of predicting training data = 0.958\nAccuracy of predicting test data = 0.957\n\n\n\n\n22.0.11 Function to create synthetic data\n\ndef make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n    \"\"\"\n    Synthetic data generation for two classes.\n\n    Inputs\n    ------\n    original_data: X, y numpy arrays (y should have label 0 and 1)\n    number_of_samples: number of samples to generate (list for y=0, y=1)\n    (Note - number_of_samples has default of 1000 samples for each class\n    if no numbers are specified at the point of calling the function)\n\n    Returns\n    -------\n    X_synthetic: NumPy array\n    y_synthetic: NumPy array\n\n    \"\"\"\n\n    # Count instances in each class\n    count_label_0 = np.sum(y==0)\n    count_label_1 = np.sum(y==1)\n\n    # SMOTE requires final class counts; add current counts to required counts\n    # (which are passed into the function)\n    n_class_0 = number_of_samples[0] + count_label_0\n    n_class_1 = number_of_samples[1] + count_label_1\n\n    # Use SMOTE to sample data points.  The number of points that we pass over\n    # to SMOTE is calculated above (the number of synthetic data samples we\n    # want, which we passed into the function + the counts from the original\n    # data).  This tells SMOTE how many TOTAL data points are needed (original\n    # + synthetic) for each class.  It then uses the original data to generate\n    # new synthetic data points.\n    # For example, imagine our original data has 100 samples for class 0 and 50\n    # for class 1, and we tell SMOTE we want 100 synthetic data points for\n    # class 0 and 150 synthetic data points for class 1.  We tell SMOTE that we\n    # need a total of 200 data points for class 0 (100 original + 100 synthetic)\n    # and 200 data points for class 1 (50 original + 150 synthetic).  It will\n    # then fill those data points by taking the original data (which will fill\n    # up the first 100 \"slots\" for class 0, and the first 50 \"slots\" for class 1)\n    # and then use these original data points to sample new synthetic data points\n    # to fill the remaining \"slots\" in each class.\n    X_resampled, y_resampled = SMOTE(\n        sampling_strategy = {0:n_class_0, 1:n_class_1}).fit_resample(X, y)\n\n    # Get just the additional (synthetic) data points.  By using len(X) for the\n    # X (input feature) data, and len(y) for the y (output label) data, we skip\n    # the original data, and just start from the newly created synthetic data,\n    # generated by SMOTE (above)\n    X_synthetic = X_resampled[len(X):]\n    y_synthetic = y_resampled[len(y):]\n\n    return X_synthetic, y_synthetic\n\n\n\n22.0.12 Generate raw synthetic data\n\n# Get counts of classes from y_train\nunique, original_frequency = np.unique(y_train, return_counts = True)\nrequired_smote_count = list(original_frequency * 2)\n\n\n# Call the function we wrote above to generate and extract the synthetic data\nX_synthetic, y_synthetic = make_synthetic_data_smote(\n        X_train, y_train, number_of_samples=required_smote_count)\n\n\nX_synthetic.head()\n\n\n\n\n\n\n\n\n\ngender_male\ngender_female\ngender_other\nage\nhypertension\nheart_disease\never_married\nwork_type_private\nwork_type_self_employed\nwork_type_govt_job\nwork_type_children\nwork_type_never_worked\nresidence_type_rural\navg_glucose_level\nbmi\nsmoking_status_smokes\nsmoking_status_formerly_smoked\nsmoking_status_never_smoked\nsmoking_status_unknown\n\n\n\n\n3681\n1.000000\n0.000000\n0.0\n5.000000\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.000000\n1.0\n0.0\n1.000000\n99.779906\n17.148605\n0.0\n0.0\n0.0\n1.0\n\n\n3682\n0.431466\n0.568534\n0.0\n13.000000\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.000000\n1.0\n0.0\n0.568534\n76.816507\n20.868534\n0.0\n0.0\n0.0\n1.0\n\n\n3683\n0.688153\n0.311847\n0.0\n70.311847\n0.311847\n0.0\n1.0\n0.0\n0.688153\n0.311847\n0.0\n0.0\n0.000000\n222.470035\n34.268815\n0.0\n1.0\n0.0\n0.0\n\n\n3684\n0.375244\n0.624756\n0.0\n1.529825\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.000000\n1.0\n0.0\n0.000000\n164.704902\n16.825731\n0.0\n0.0\n0.0\n1.0\n\n\n3685\n0.000000\n1.000000\n0.0\n33.765655\n0.000000\n0.0\n1.0\n1.0\n0.000000\n0.000000\n0.0\n0.0\n0.234345\n133.224763\n21.501422\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\n\n22.0.13 Prepare lists of categorical, integer and binary features\n\n# Get full list of column names (the names of our features)\nX_col_names = list(X_train)\n\n# Set categorical one-hots cols using common prefix\ncategorical = ['gender_', 'work_type_', 'smoking_status_']\n\none_hot_cols = []\nfor col in categorical:\n    one_hot_cols.append([x for x in X_col_names if x[0:len(col)] == col])\n\n# Set integer columns\ninteger_cols = ['age']\n\n# Set binary columns\nbinary_cols = ['hypertension',\n               'heart_disease',\n               'ever_married',\n               'residence_type_rural']\n\n\n\n22.0.14 Function to process raw synthetic categorical data to one-hot encoded\n\ndef make_one_hot(x):\n    \"\"\"\n    Takes a list/array/series and turns it into a one-hot encoded\n    list/array series, by setting 1 for highest value and 0 for all\n    others\n\n    \"\"\"\n    # Get argmax (this returns the index of the highest values in\n    # the list / array / series passed in to the function)\n    highest = np.argmax(x)\n    # Set all values to zero (just multiply all values by 0)\n    x *= 0.0\n    # Set the value that was found to be the highest to 1, by\n    # using the index we found using argmax above\n    x[highest] = 1.0\n\n    return x\n\n\n\n22.0.15 Process raw synthetic data and show a sample\n\n# Set y_label\ny_label = 'stroke'\n\n# Create a data frame with id to store the synthetic data\nsynth_df = pd.DataFrame()\n\n# Transfer X values to the new DataFrame\nsynth_df=pd.concat([synth_df,\n                    pd.DataFrame(X_synthetic, columns=X_col_names)],\n                    axis=1)\n\n# Make columns (that need to be) one hot encoded using the\n# function we wrote above, using the raw synthetic data\nfor one_hot_list in one_hot_cols:\n    for index, row in synth_df.iterrows():\n        x = row[one_hot_list]\n        x_one_hot = make_one_hot(x)\n        row[x_one_hot.index]= x_one_hot.values\n\n# Make integer as necessary by rounding the raw synthetic data\nfor col in integer_cols:\n    synth_df[col] = synth_df[col].round(0)\n\n# Round binary cols and clip so values under 0 or above 1\n# are set to 0 and 1 respectively (this won't happen with\n# SMOTE, as it will only sample between the two points (so\n# points sampled between binary points will always be\n# between 0 and 1) but it can happen with other methods)\nfor col in binary_cols:\n    synth_df[col] = np.clip(synth_df[col],0,1).round(0)\n\n# Add y data with a label\ny_list = list(y_synthetic)\nsynth_df[y_label] = y_list\n\n# Shuffle data\nsynth_df = synth_df.sample(frac=1.0)\n\n\nsynth_df.head()\n\n\n\n\n\n\n\n\n\ngender_male\ngender_female\ngender_other\nage\nhypertension\nheart_disease\never_married\nwork_type_private\nwork_type_self_employed\nwork_type_govt_job\nwork_type_children\nwork_type_never_worked\nresidence_type_rural\navg_glucose_level\nbmi\nsmoking_status_smokes\nsmoking_status_formerly_smoked\nsmoking_status_never_smoked\nsmoking_status_unknown\nstroke\n\n\n\n\n11029\n1.0\n0.0\n0.0\n66.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n76.595595\n21.341845\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n8917\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n86.177242\n16.819550\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n7515\n1.0\n0.0\n0.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n98.342804\n19.927970\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n10261\n0.0\n1.0\n0.0\n68.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n111.693441\n26.023267\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n3956\n1.0\n0.0\n0.0\n57.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n71.070423\n25.928229\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n22.0.16 Find nearest original data point to each synthetic data point\n\n# Standardise synthetic data (based on real training data)\nX_train_std, X_synth_std = standardise_data(X_train, X_synthetic)\n\n# Get ALL real X data (combine standardised training + test data)\n# We do this because we need to check for duplicates / very close\n# values in all of the real data we've got\nX_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n\n# Use SciKitLearn neighbors.NearestNeighbors to find nearest neighbour\n# to each data point. First, we fit to the real standardised data\n# (all of it, train + test set).  Then we can give it the synthetic data\n# and ask it to give us the cartesian distance and ID of its nearest\n# real world data point neighbour for each synthetic data point.\nnn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\ndists, idxs = nn.kneighbors(X_synth_std)\n\n# Store the index and ids (indices) in the synthetic data DataFrame\n# Flatten just reduces something in more than 1 dimension down to\n# 1 dimension (eg a list of lists becomes a single list)\nsynth_df['distance_to_closest_real'] = list(dists.flatten())\nsynth_df['closest_X_real_row_index'] = list(idxs.flatten())\n\n\nsynth_df\n\n\n\n\n\n\n\n\n\ngender_male\ngender_female\ngender_other\nage\nhypertension\nheart_disease\never_married\nwork_type_private\nwork_type_self_employed\nwork_type_govt_job\n...\nresidence_type_rural\navg_glucose_level\nbmi\nsmoking_status_smokes\nsmoking_status_formerly_smoked\nsmoking_status_never_smoked\nsmoking_status_unknown\nstroke\ndistance_to_closest_real\nclosest_X_real_row_index\n\n\n\n\n11029\n1.0\n0.0\n0.0\n66.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n1.0\n76.595595\n21.341845\n0.0\n1.0\n0.0\n0.0\n1.0\n0.017891\n3246\n\n\n8917\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n86.177242\n16.819550\n0.0\n0.0\n0.0\n1.0\n0.0\n1.512034\n527\n\n\n7515\n1.0\n0.0\n0.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n1.0\n98.342804\n19.927970\n0.0\n0.0\n0.0\n1.0\n0.0\n1.901454\n2592\n\n\n10261\n0.0\n1.0\n0.0\n68.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n1.0\n111.693441\n26.023267\n0.0\n0.0\n1.0\n0.0\n0.0\n1.088711\n2726\n\n\n3956\n1.0\n0.0\n0.0\n57.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n1.0\n71.070423\n25.928229\n0.0\n1.0\n0.0\n0.0\n0.0\n0.489179\n3500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6383\n0.0\n1.0\n0.0\n27.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n131.403461\n31.684226\n0.0\n0.0\n1.0\n0.0\n0.0\n0.295965\n3612\n\n\n9031\n0.0\n1.0\n0.0\n36.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n60.107014\n24.100000\n0.0\n0.0\n0.0\n1.0\n0.0\n0.378650\n2951\n\n\n5755\n0.0\n1.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n96.518722\n33.282594\n0.0\n0.0\n0.0\n1.0\n0.0\n1.831434\n3521\n\n\n9023\n0.0\n1.0\n0.0\n66.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n1.0\n210.940018\n23.934760\n0.0\n0.0\n1.0\n0.0\n0.0\n0.758766\n3469\n\n\n4264\n0.0\n1.0\n0.0\n54.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n...\n0.0\n107.439642\n29.097616\n0.0\n1.0\n0.0\n0.0\n0.0\n0.721686\n217\n\n\n\n\n7362 rows × 22 columns\n\n\n\n\n\n\n22.0.17 Remove identical points\n\n# Get points with zero distance to real (use distance of &lt;0.001 as effectively identical)\nidentical = synth_df['distance_to_closest_real'] &lt; 0.001\n\nprint (f'Proportion of data points identical to real data points = {identical.mean():0.3f}')\n# Remove points with zero (or effectively zero) distance to a real data point.  We\n# do this by setting up a mask that says we only want to see data points where the \"identical\"\n# criterion we specified above is false (ie they're not identical).  Then we apply that\n# mask and overwrite our existing synthetic data DataFrame so we've now only got data points\n# that are not identical to real world data points.\nmask = identical == False\nsynth_df = synth_df[mask]\n\nProportion of data points identical to real data points = 0.001\n\n\n\n\n22.0.18 Remove points closest to original data\n\n# Proportion of points to remove\nproportion_to_remove = 0.1\n\n# Sort by distance, with highest distances (those we want to keep) at\n# the top\nsynth_by_distance = synth_df.sort_values(\n    'distance_to_closest_real', ascending=False)\n\n# Limit data.  Calculate the number of entries to keep as being the\n# total number of synthetic data points we've now got (after having\n# removed ones identical to real world data points) multiplied by\n# the proportion we want to keep (the inverse of the proportion to remove).\n# As we've sorted in descending order by distance, we can then just\n# use .head to identify how much of the top of list we want to keep\n# (90% in this case, where we're removing the 10% that are closest - at\n# the bottom)\nnumber_to_keep = int(len(synth_by_distance) * (1 - proportion_to_remove))\nsynth_by_distance = synth_by_distance.head(number_to_keep)\n\n# Shuffle and store back in synth_df (frac=1 gives us a sample size of 100%\n# (ie - all of the ones we said above we wanted to keep))\nsynth_df = synth_by_distance.sample(frac=1)\n\n\n\n22.0.19 Show five examples with their closest data points in the original data\n\n# Reproduce X_real but with non-standardised (ie the raw original) values for\n# comparison\nX_real = np.concatenate([X_train, X_test], axis=0)\n\n# Set up Data Frame for comparison\ncomparison = pd.DataFrame(index=X_col_names)\n\n# Generate five examples\nfor i in range(5):\n    # Get synthetic data sample (sample size of 1 - one data point)\n    sample = synth_df.sample(1)\n    comparison[f'Synthetic_{i+1}'] = sample[X_col_names].values[0]\n    # Get closest point from the real data (remember we stored earlier\n    # the index of the closest real world point, so we can grab it out\n    # easily here)We remove\n    closest_id = sample['closest_X_real_row_index']\n    comparison[f'Synthetic_{i+1}_closest'] = X_real[closest_id, :][0]\n\n# Display the comparisons\ncomparison.round(0)\n\n\n\n\n\n\n\n\n\nSynthetic_1\nSynthetic_1_closest\nSynthetic_2\nSynthetic_2_closest\nSynthetic_3\nSynthetic_3_closest\nSynthetic_4\nSynthetic_4_closest\nSynthetic_5\nSynthetic_5_closest\n\n\n\n\ngender_male\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n\n\ngender_female\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\ngender_other\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nage\n19.0\n80.0\n58.0\n79.0\n43.0\n67.0\n70.0\n59.0\n23.0\n48.0\n\n\nhypertension\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n\n\nheart_disease\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\never_married\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n\n\nwork_type_private\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\nwork_type_self_employed\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\nwork_type_govt_job\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\nwork_type_children\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nwork_type_never_worked\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nresidence_type_rural\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\navg_glucose_level\n67.0\n104.0\n69.0\n74.0\n93.0\n82.0\n251.0\n82.0\n63.0\n147.0\n\n\nbmi\n30.0\n24.0\n28.0\n30.0\n23.0\n14.0\n27.0\n33.0\n25.0\n22.0\n\n\nsmoking_status_smokes\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nsmoking_status_formerly_smoked\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nsmoking_status_never_smoked\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n\n\nsmoking_status_unknown\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n\n\n22.0.20 Sample from synthetic data to get same size / balance as the original data, and compare counts to ensure identical to original data\n\n# Randomly sample from the synthetic data those who had a stroke,\n# and sample this the same number of times as we had number\n# who had a stroke in the real data\nmask = synth_df['stroke'] == 1\nsynth_positive_stroke = synth_df[mask].sample(number_positive_stroke)\n\n# The same as above, but for those who didn't have a stroke\nmask = synth_df['stroke'] == 0\nsynth_negative_stroke = synth_df[mask].sample(number_negative_stroke)\n\n# Reconstruct into synth_df and shuffle\nsynth_df = pd.concat([synth_positive_stroke, synth_negative_stroke], axis=0)\nsynth_df = synth_df.sample(frac=1.0, )\n\n\nprint ('Number of real data stroke : ', np.sum(data['stroke'] == 1))\nprint ('Number of synthetic data stroke : ', np.sum(synth_df['stroke'] == 1))\nprint ('Number of real data non-stroke : ', np.sum(data['stroke'] == 0))\nprint ('Number of synthetic data non-stroke : ', np.sum(synth_df['stroke'] == 0))\n\nNumber of real data stroke :  209\nNumber of synthetic data stroke :  209\nNumber of real data non-stroke :  4700\nNumber of synthetic data non-stroke :  4700\n\n\n\n\n22.0.21 Fit Logistic Regression model using synthetic data and compare accuracy with model trained on original data\n\n# Get X data and standardised\nX_synth = synth_df[X_col_names]\ny_synth = synth_df['stroke'].values\nX_synth_std, X_test_std = standardise_data(X_synth, X_test)\n\n# Fit model\nmodel_synth = LogisticRegression()\nmodel_synth.fit(X_synth_std,y_synth)\n\n# Get predictions of test set\ny_pred_test_synth = model_synth.predict(X_test_std)\n\n# Report accuracy\naccuracy_test_synth = np.mean(y_pred_test_synth == y_test)\n\nprint (f'Accuracy of predicting test data from model trained on real data = {accuracy_test:0.3f}')\nprint (f'Accuracy of predicting test data from model trained on synthetic data = {accuracy_test_synth:0.3f}')\n\nAccuracy of predicting test data from model trained on real data = 0.957\nAccuracy of predicting test data from model trained on synthetic data = 0.957\n\n\n\n\n22.0.22 ROC Curves\n\ny_probs = model.predict_proba(X_test_std)[:,1]\ny_probs_synthetic = model_synth.predict_proba(X_test_std)[:,1]\n\n\nfpr, tpr, thresholds = roc_curve(y_test, y_probs)\nfpr_synth, tpr_synth, thresholds_synth = roc_curve(y_test, y_probs_synthetic)\nroc_auc = auc(fpr, tpr)\nroc_auc_snth = auc(fpr_synth, tpr_synth)\nprint (f'ROC AUC real training data: {roc_auc:0.2f}')\nprint (f'ROC AUC synthetic training data: {roc_auc_snth:0.2f}')\n\nROC AUC real training data: 0.83\nROC AUC synthetic training data: 0.83\n\n\n\nfig, ax = plt.subplots()\n\n# Plot ROC\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\n\nax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nax.plot(fpr,tpr, color='green', label = 'Real training data')\nax.plot(fpr_synth,tpr_synth, color='red', label = 'Synthetic training data')\n\nax.set_title('Stroke Data Receiver Operator Characteristic curve')\n\nax.legend()\n\nfig.savefig('synthetic_roc.png')\nfig.show()\n\n\n\n\n\n\n\n\n\n\n22.0.23 Use synthetic data for positive class to generate another 2000 for this class.\n\n# Generate synthetic data again, but this time with 4000 extra synthetic data\n# points for the positive class (double what we need), and 0 for the negative\n# class\nX_synthetic, y_synthetic = make_synthetic_data_smote(\n    X_train, y_train, number_of_samples=[0, 4000]\n)\n\n\n# Set y_label\ny_label = 'stroke'\n\n# Create a data frame with id to store the synthetic data\nsynth_df = pd.DataFrame()\n\n# Transfer X values to the new DataFrame\nsynth_df=pd.concat([synth_df,\n                    pd.DataFrame(X_synthetic, columns=X_col_names)],\n                    axis=1)\n\n# Make columns (that need to be) one hot encoded using the\n# function we wrote above, using the raw synthetic data\nfor one_hot_list in one_hot_cols:\n    for index, row in synth_df.iterrows():\n        x = row[one_hot_list]\n        x_one_hot = make_one_hot(x)\n        row[x_one_hot.index]= x_one_hot.values\n\n# Make integer as necessary by rounding the raw synthetic data\nfor col in integer_cols:\n    synth_df[col] = synth_df[col].round(0)\n\n# Round binary cols and clip so values under 0 or above 1\n# are set to 0 and 1 respectively (this won't happen with\n# SMOTE, as it will only sample between the two points (so\n# points sampled between binary points will always be\n# between 0 and 1) but it can happen with other methods)\nfor col in binary_cols:\n    synth_df[col] = np.clip(synth_df[col],0,1).round(0)\n\n# Add y data with a label\ny_list = list(y_synthetic)\nsynth_df[y_label] = y_list\n\n# Shuffle data\nsynth_df = synth_df.sample(frac=1.0)\n\n\n# Standardise synthetic data (based on real training data)\nX_train_std, X_synth_std = standardise_data(X_train, X_synthetic)\n\n# Get ALL real X data (combine standardised training + test data)\n# We do this because we need to check for duplicates / very close\n# values in all of the real data we've got\nX_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n\n# Use SciKitLearn neighbors.NearestNeighbors to find nearest neighbour\n# to each data point. First, we fit to the real standardised data\n# (all of it, train + test set).  Then we can give it the synthetic data\n# and ask it to give us the cartesian distance and ID of its nearest\n# real world data point neighbour for each synthetic data point.\nnn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\ndists, idxs = nn.kneighbors(X_synth_std)\n\n# Store the index and ids (indices) in the synthetic data DataFrame\n# Flatten just reduces something in more than 1 dimension down to\n# 1 dimension (eg a list of lists becomes a single list)\nsynth_df['distance_to_closest_real'] = list(dists.flatten())\nsynth_df['closest_X_real_row_index'] = list(idxs.flatten())\n\n\n# Get points with zero distance to real (use distance of &lt;0.001 as effectively identical)\nidentical = synth_df['distance_to_closest_real'] &lt; 0.001\n\nprint (f'Proportion of data points identical to real data points = {identical.mean():0.3f}')\n# Remove points with zero (or effectively zero) distance to a real data point.  We\n# do this by setting up a mask that says we only want to see data points where the \"identical\"\n# criterion we specified above is false (ie they're not identical).  Then we apply that\n# mask and overwrite our existing synthetic data DataFrame so we've now only got data points\n# that are not identical to real world data points.\nmask = identical == False\nsynth_df = synth_df[mask]\n\nProportion of data points identical to real data points = 0.001\n\n\n\n# Proportion of points to remove\nproportion_to_remove = 0.1\n\n# Sort by distance, with highest distances (those we want to keep) at\n# the top\nsynth_by_distance = synth_df.sort_values(\n    'distance_to_closest_real', ascending=False)\n\n# Limit data.  Calculate the number of entries to keep as being the\n# total number of synthetic data points we've now got (after having\n# removed ones identical to real world data points) multiplied by\n# the proportion we want to keep (the inverse of the proportion to remove).\n# As we've sorted in descending order by distance, we can then just\n# use .head to identify how much of the top of list we want to keep\n# (90% in this case, where we're removing the 10% that are closest - at\n# the bottom)\nnumber_to_keep = int(len(synth_by_distance) * (1 - proportion_to_remove))\nsynth_by_distance = synth_by_distance.head(number_to_keep)\n\n# Shuffle and store back in synth_df (frac=1 gives us a sample size of 100%\n# (ie - all of the ones we said above we wanted to keep))\nsynth_df = synth_by_distance.sample(frac=1)\n\n\n# Keep only a random sample of 2000 of the remaining synthetic datapoints\n# We don't need a mask here as ALL our synthetic datapoints are for class 1\n# (positive).\nsynth_df = synth_df.sample(2000)\n\n\n# Add synthetic data for positive class (class 1) to real data\n# We'll make a separate copy of the original dataframe with the new synthetic\n# data points added, keeping our original data intact.\naugmented_data = pd.concat([data, synth_df])\n\n# We'll also get rid of the two columns we added -\n# distance_to_closest_real and closest_X_real_row_index as we do not want these\n# to be used in a Logistic Regression model.\naugmented_data.drop('distance_to_closest_real', axis=1, inplace=True)\naugmented_data.drop('closest_X_real_row_index', axis=1, inplace=True)\n\n\n# Let's have a look at our new dataframe.  We should have 6,909 records (the\n# original 4,909 + 2,000 additional ones for class 1 (positive)) and 20 columns\naugmented_data\n\n\n\n\n\n\n\n\n\ngender_male\ngender_female\ngender_other\nage\nhypertension\nheart_disease\never_married\nwork_type_private\nwork_type_self_employed\nwork_type_govt_job\nwork_type_children\nwork_type_never_worked\nresidence_type_rural\navg_glucose_level\nbmi\nsmoking_status_smokes\nsmoking_status_formerly_smoked\nsmoking_status_never_smoked\nsmoking_status_unknown\nstroke\n\n\n\n\n0\n1.0\n0.0\n0.0\n67.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n228.690000\n36.600000\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n0.0\n0.0\n80.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n105.920000\n32.500000\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n0.0\n1.0\n0.0\n49.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n171.230000\n34.400000\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n1.0\n0.0\n79.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n174.120000\n24.000000\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n4\n1.0\n0.0\n0.0\n81.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n186.210000\n29.000000\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5376\n1.0\n0.0\n0.0\n76.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n130.633158\n25.249173\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n4560\n1.0\n0.0\n0.0\n81.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n97.955702\n33.567697\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n4276\n1.0\n0.0\n0.0\n50.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n101.679233\n29.784617\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n7584\n0.0\n1.0\n0.0\n78.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n107.656949\n30.885320\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n3688\n0.0\n1.0\n0.0\n69.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n73.619327\n35.449577\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n6909 rows × 20 columns\n\n\n\n\n\n# Let's also check that the class splits are as expected (we should have 2,209\n# positives, and 4,700 negatives)\nnumber_positive_stroke = np.sum(augmented_data['stroke'] == 1)\nnumber_negative_stroke = np.sum(augmented_data['stroke'] == 0)\n\nprint (f\"Positives : {number_positive_stroke}\")\nprint (f\"Negatives : {number_negative_stroke}\")\n\nPositives : 2209\nNegatives : 4700\n\n\n\nX = augmented_data.drop('stroke',axis=1) # X = all 'data' except the 'stroke' column\ny = augmented_data['stroke'] # y = 'stroke' column from 'data'\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)\n\n\nmodel = LogisticRegression()\nmodel.fit(X_train_std,y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Predict training and test set labels\ny_pred_train = model.predict(X_train_std)\ny_pred_test = model.predict(X_test_std)\n\n\naccuracy_train = np.mean(y_pred_train == y_train)\naccuracy_test = np.mean(y_pred_test == y_test)\n\nprint (f'Accuracy of predicting training data = {accuracy_train:0.3f}')\nprint (f'Accuracy of predicting test data = {accuracy_test:0.3f}')\n\nAccuracy of predicting training data = 0.794\nAccuracy of predicting test data = 0.789\n\n\nIt looks like adding 2,000 extra synthetic datapoints to our positive class has worsened model performance in this instance. We probably didn’t need to do in this example - we already had very good performance. And 2,000 may have been too many to generate from a sample of around 200.",
    "crumbs": [
      "4I - Synthetic Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Exercise Solution: Synthetic Data (Stroke Thromobolysis Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_auto_ml.html",
    "href": "4j_auto_ml.html",
    "title": "23  Automated Machine Learning (Titanic Dataset)",
    "section": "",
    "text": "23.1 Simple initial auto ML training\nLet’s try out a simple instance of automated training with flaml.\nWe need to pass in training and testing data, tell it what kind of task we’re conducting (e.g. classification, regression), how long to keep trying different models (here, we’ve gone for 60 seconds), and set a random seed for reproducibility.\nautoml = AutoML()\nautoml.fit(X_train, y_train,\n           task=\"classification\",\n           time_budget=60,\n           seed=42)\nWe can run this line to see what model it selected.\nautoml.model\n\n&lt;flaml.automl.model.CatBoostEstimator at 0x28d4942f150&gt;\nAnd this will give us the parameters it chose.\nautoml.best_config\n\n{'early_stopping_rounds': 10,\n 'learning_rate': 0.08181308076296119,\n 'n_estimators': 21}\nFinally, we can output the best configuration for each of the estimators it tried.\nautoml.best_config_per_estimator\n\n{'lgbm': {'n_estimators': 5,\n  'num_leaves': 35,\n  'min_child_samples': 12,\n  'learning_rate': 0.6152167721646307,\n  'log_max_bin': 9,\n  'colsample_bytree': 0.9236754665076755,\n  'reg_alpha': 0.0009765625,\n  'reg_lambda': 2.5956586580400387},\n 'rf': {'n_estimators': 8,\n  'max_features': 0.3522661192428198,\n  'max_leaves': 11,\n  'criterion': 'gini'},\n 'catboost': {'early_stopping_rounds': 10,\n  'learning_rate': 0.08181308076296119,\n  'n_estimators': 21},\n 'xgboost': {'n_estimators': 49,\n  'max_leaves': 8,\n  'min_child_weight': 1.6432463021924941,\n  'learning_rate': 0.1014993900453572,\n  'subsample': 0.892732835389516,\n  'colsample_bylevel': 1.0,\n  'colsample_bytree': 1.0,\n  'reg_alpha': 0.26675517308462793,\n  'reg_lambda': 0.023187687303412453},\n 'extra_tree': {'n_estimators': 9,\n  'max_features': 0.21337293465256343,\n  'max_leaves': 14,\n  'criterion': 'gini'},\n 'xgb_limitdepth': {'n_estimators': 9,\n  'max_depth': 7,\n  'min_child_weight': 1.5981523613778321,\n  'learning_rate': 0.4433369998455015,\n  'subsample': 0.9186551030103914,\n  'colsample_bylevel': 1.0,\n  'colsample_bytree': 0.9480491857726955,\n  'reg_alpha': 0.0009765625,\n  'reg_lambda': 0.15817424342486922},\n 'lrl1': None}\nNow let’s evaluate this model and put the results into a dataframe.\nWe can use our automl variable where we’d usually use model.\ny_pred_train = automl.predict(X_train)\ny_pred_val = automl.predict(X_validate)\n\ntn, fp, fn, tp = confusion_matrix(y_validate, y_pred_val, labels=[0, 1]).ravel()\n\nresults_df = pd.DataFrame({\n          'Accuracy (training)': np.mean(y_pred_train == y_train),\n          'Accuracy (validation)': np.mean(y_pred_val == y_validate),\n          'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro'),\n          'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro'),\n          \"AUC\": roc_auc_score(y_validate, y_pred_val),\n          \"Training AUC\": roc_auc_score(y_train, y_pred_train),\n          \"f1\": f1_score(y_validate, y_pred_val, average='macro'),\n          \"Training f1\": f1_score(y_train, y_pred_train, average='macro'),\n          \"FP\": fp,\n          \"FN\": fn\n\n          }, index=[\"Auto ML - Default Parameters - Scoring on ROC AUC\"]\n).round(3)\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nTraining AUC\nf1\nTraining f1\nFP\nFN\n\n\n\n\nAuto ML - Default Parameters - Scoring on ROC AUC\n0.863\n0.811\n0.802\n0.8\n0.8\n0.846\n0.801\n0.851\n13\n14\nThis seems like pretty reasonable performance, based on our previous interactions with the titanic dataset, though not as good as we’ve seen sometimes.",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Automated Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_auto_ml.html#customisation",
    "href": "4j_auto_ml.html#customisation",
    "title": "23  Automated Machine Learning (Titanic Dataset)",
    "section": "23.2 Customisation",
    "text": "23.2 Customisation\nLet’s define a function that will allow us to quickly calculate and store metrics when assessing the automl library.\n\ndef auto_ml_get_results(name):\n    y_pred_train = automl.predict(X_train)\n    y_pred_val = automl.predict(X_validate)\n\n    tn, fp, fn, tp = confusion_matrix(y_validate, y_pred_val, labels=[0, 1]).ravel()\n\n    return pd.DataFrame({\n            'Accuracy (training)': np.mean(y_pred_train == y_train),\n            'Accuracy (validation)': np.mean(y_pred_val == y_validate),\n            'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro'),\n            'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro'),\n            \"AUC\": roc_auc_score(y_validate, y_pred_val),\n            \"Training AUC\": roc_auc_score(y_train, y_pred_train),\n            \"f1\": f1_score(y_validate, y_pred_val, average='macro'),\n            \"Training f1\": f1_score(y_train, y_pred_train, average='macro'),\n            \"FP\": fp,\n            \"FN\": fn\n\n            }, index=[name]\n    ).round(3)\n\nNow let’s try training again, this time asking it to score on a different metric - the f1 score.\n\nautoml = AutoML()\nautoml.fit(X_train, y_train,\n           task=\"classification\",\n           time_budget=60,\n           metric=\"f1\",\n           seed=42)\nresults_df = pd.concat(\n    [results_df,\n    auto_ml_get_results(name=\"Auto ML - Scoring on f1\")]\n)",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Automated Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_auto_ml.html#extending-the-time",
    "href": "4j_auto_ml.html#extending-the-time",
    "title": "23  Automated Machine Learning (Titanic Dataset)",
    "section": "23.3 Extending the time",
    "text": "23.3 Extending the time\nWhen we ran it with the default settings previously (our first run), the fifth line of the output gave us the estimated time required to find an optimal model.\n\n[flaml.automl.logger: 07-29 12:25:03] {2345} INFO - Estimated sufficient time budget=658s. Estimated necessary time budget=16s.\n\nLet’s allocate this length of time.\n\nautoml = AutoML()\n\nautoml.fit(X_train, y_train,\n           task=\"classification\",\n           time_budget=658,\n           seed=42)\n\nresults_df = pd.concat(\n    [results_df,\n    auto_ml_get_results(name=\"Auto ML - scoring on ROC AUC - Training for ~11 minutes\")]\n)\n\nLet’s view our updated results table.\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nTraining AUC\nf1\nTraining f1\nFP\nFN\n\n\n\n\nAuto ML - Default Parameters - Scoring on ROC AUC\n0.863\n0.811\n0.802\n0.800\n0.800\n0.846\n0.801\n0.851\n13\n14\n\n\nAuto ML - Scoring on f1\n0.914\n0.825\n0.820\n0.809\n0.809\n0.897\n0.813\n0.906\n10\n15\n\n\nAuto ML - scoring on ROC AUC - Training for ~11 minutes\n0.884\n0.797\n0.787\n0.792\n0.792\n0.863\n0.789\n0.872\n16\n13",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Automated Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_auto_ml.html#compare-this-with-some-other-ml-models",
    "href": "4j_auto_ml.html#compare-this-with-some-other-ml-models",
    "title": "23  Automated Machine Learning (Titanic Dataset)",
    "section": "23.4 Compare this with some other ML models",
    "text": "23.4 Compare this with some other ML models\nLet’s define a function to fit and train any provided model, then return the required metrics to be added onto our table from above.\nThis will allow us to compare the performance of flaml with specifying the model ourselves.\n\ndef fit_train(name=\"XGBoost\", X_train=X_train, X_validate=X_validate,\n              y_train=y_train, y_validate=y_validate,\n              model=XGBClassifier(random_state=42)\n              ):\n\n     model.fit(X_train, y_train)\n\n     y_pred_train = model.predict(X_train)\n     y_pred_val = model.predict(X_validate)\n\n     tn, fp, fn, tp = confusion_matrix(y_validate, y_pred_val, labels=[0, 1]).ravel()\n\n     return pd.DataFrame({\n            'Accuracy (training)': np.mean(y_pred_train == y_train),\n            'Accuracy (validation)': np.mean(y_pred_val == y_validate),\n            'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro'),\n            'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro'),\n            \"AUC\": roc_auc_score(y_validate, y_pred_val),\n            \"Training AUC\": roc_auc_score(y_train, y_pred_train),\n            \"f1\": f1_score(y_validate, y_pred_val, average='macro'),\n            \"Training f1\": f1_score(y_train, y_pred_train, average='macro'),\n            \"FP\": fp,\n            \"FN\": fn\n          }, index=[name]\n).round(3)\n\nLet’s use this to quickly assess the performance of a range of other models.\n\nresults_df = pd.concat(\n    [results_df,\n     fit_train(), # This uses the default - xgboost\n     fit_train(name=\"Decision Tree (Defaults)\", model=DecisionTreeClassifier()),\n     fit_train(name=\"Random Forest (Defaults)\", model=RandomForestClassifier(random_state=42)),\n     fit_train(name=\"LightGBM (Defaults)\", model=LGBMClassifier(random_state=42)),\n     fit_train(name=\"Catboost (Defaults)\", model=CatBoostClassifier(random_state=42, verbose=False)),\n\n     ]\n)\n\nresults_df\n\nLet’s sort this by validation accuracy.\n\nresults_df.sort_values(\"Accuracy (validation)\", ascending=False)\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nTraining AUC\nf1\nTraining f1\nFP\nFN\n\n\n\n\nAuto ML - Scoring on f1\n0.914\n0.825\n0.820\n0.809\n0.809\n0.897\n0.813\n0.906\n10\n15\n\n\nLightGBM (Defaults)\n0.961\n0.818\n0.810\n0.806\n0.806\n0.954\n0.808\n0.958\n12\n14\n\n\nCatboost (Defaults)\n0.923\n0.818\n0.810\n0.806\n0.806\n0.907\n0.808\n0.915\n12\n14\n\n\nAuto ML - Default Parameters - Scoring on ROC AUC\n0.863\n0.811\n0.802\n0.800\n0.800\n0.846\n0.801\n0.851\n13\n14\n\n\nAuto ML - scoring on ROC AUC - Training for ~11 minutes\n0.884\n0.797\n0.787\n0.792\n0.792\n0.863\n0.789\n0.872\n16\n13\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.973\n0.787\n0.977\n14\n15\n\n\nDecision Tree (Defaults)\n0.988\n0.769\n0.758\n0.763\n0.763\n0.983\n0.760\n0.987\n18\n15\n\n\nRandom Forest (Defaults)\n0.988\n0.762\n0.751\n0.757\n0.757\n0.984\n0.753\n0.987\n19\n15\n\n\n\n\n\n\n\n\n\n23.4.1 Ranking model performance\nLet’s look at another way to quickly compare the performance of the different models.\nWe will rank each model by its performance against the other models, with a lower number indicating better performance relative to the other models (e.g. the model with the highest precision will be ranked 1; the model with the lowest number of false negatives will be ranked 1).\nWe will omit training accuracy from our calculations as we are more interested in its likely ‘real-world’ performance on unseen data.\n\nranking_df_high_good = results_df[['Accuracy (validation)', 'Precision (validation)', 'Recall (validation)', 'AUC', 'f1']].rank(method='dense', ascending=False).convert_dtypes()\nranking_df_low_good = results_df[['FP', 'FN']].rank(method='dense', ascending=True).convert_dtypes()\n\nranking_df = ranking_df_high_good.merge(ranking_df_low_good, left_index=True, right_index=True)\n\nranking_df['Rank Sum'] = ranking_df.sum(axis=1)\nranking_df = ranking_df.sort_values('Rank Sum', ascending=True).convert_dtypes()\nranking_df\n\n\n\n\n\n\n\n\n\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\nRank Sum\n\n\n\n\nAuto ML - Scoring on f1\n1\n1\n1\n1\n1\n1\n3\n9\n\n\nLightGBM (Defaults)\n2\n2\n2\n2\n2\n2\n2\n14\n\n\nCatboost (Defaults)\n2\n2\n2\n2\n2\n2\n2\n14\n\n\nAuto ML - Default Parameters - Scoring on ROC AUC\n3\n3\n3\n3\n3\n3\n2\n20\n\n\nAuto ML - scoring on ROC AUC - Training for ~11 minutes\n4\n5\n4\n4\n4\n5\n1\n27\n\n\nXGBoost\n4\n4\n5\n5\n5\n4\n3\n30\n\n\nDecision Tree (Defaults)\n5\n6\n6\n6\n6\n6\n3\n38\n\n\nRandom Forest (Defaults)\n6\n7\n7\n7\n7\n7\n3\n44\n\n\n\n\n\n\n\n\nWe could plot this output as well as everything is on the same scale (though we will omit the rank sum) as that’s much larger.\n\nranking_df.drop(columns=\"Rank Sum\").plot(\n    kind=\"barh\",\n    title=\"Performance Ranking (Higher Rank Value = Worse)\",\n    xlabel=\"Rank of Performance\"\n    )",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Automated Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html",
    "href": "4j_data_preprocessing_and_eda.html",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "",
    "text": "24.1 Initial exploration\nLet’s first check how many values we have.\nlen(data)\n\n891\nView the first five rows.\ndata.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nCheck the ranges and distributions of the values.\ndata.describe().round(3)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000\n891.000\n891.000\n714.000\n891.000\n891.000\n891.000\n\n\nmean\n446.000\n0.384\n2.309\n29.699\n0.523\n0.382\n32.204\n\n\nstd\n257.354\n0.487\n0.836\n14.526\n1.103\n0.806\n49.693\n\n\nmin\n1.000\n0.000\n1.000\n0.420\n0.000\n0.000\n0.000\n\n\n25%\n223.500\n0.000\n2.000\n20.125\n0.000\n0.000\n7.910\n\n\n50%\n446.000\n0.000\n3.000\n28.000\n0.000\n0.000\n14.454\n\n\n75%\n668.500\n1.000\n3.000\n38.000\n1.000\n0.000\n31.000\n\n\nmax\n891.000\n1.000\n3.000\n80.000\n8.000\n6.000\n512.329\nWe can use the following code to quickly view histograms for every column in the dataset.\ndata.hist(bins=30, figsize=(15, 10))\n\narray([[&lt;Axes: title={'center': 'PassengerId'}&gt;,\n        &lt;Axes: title={'center': 'Survived'}&gt;,\n        &lt;Axes: title={'center': 'Pclass'}&gt;],\n       [&lt;Axes: title={'center': 'Age'}&gt;,\n        &lt;Axes: title={'center': 'SibSp'}&gt;,\n        &lt;Axes: title={'center': 'Parch'}&gt;],\n       [&lt;Axes: title={'center': 'Fare'}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],\n      dtype=object)\nGet a full list of column names.\ndata.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#initial-exploration",
    "href": "4j_data_preprocessing_and_eda.html#initial-exploration",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "",
    "text": "24.1.1 Check for missing data\nThis snippet gives us a quick overview of which columns have missing data and how much is missing.\n\npd.DataFrame(\n    data.isna().mean().round(4),\n    columns=[\"Percentage of Values Missing\"]\n    )\n\n\n\n\n\n\n\n\n\nPercentage of Values Missing\n\n\n\n\nPassengerId\n0.0000\n\n\nSurvived\n0.0000\n\n\nPclass\n0.0000\n\n\nName\n0.0000\n\n\nSex\n0.0000\n\n\nAge\n0.1987\n\n\nSibSp\n0.0000\n\n\nParch\n0.0000\n\n\nTicket\n0.0000\n\n\nFare\n0.0000\n\n\nCabin\n0.7710\n\n\nEmbarked\n0.0022",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#exploring-simple-patterns",
    "href": "4j_data_preprocessing_and_eda.html#exploring-simple-patterns",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "29.1 Exploring Simple Patterns",
    "text": "29.1 Exploring Simple Patterns\nLet’s first just look at average values for each column depending on whether people survived or not.\nThink about how you’d interpret this for binary columns like ‘IsMale’ and ‘Embarked_Cherbourg’.\n\ndata.groupby('Survived').mean(numeric_only=True).round(4)\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\n\n\nSurvived\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n447.0164\n2.5319\n0.8525\n30.6262\n0.5537\n0.3297\n22.1179\n0.1366\n0.0856\n0.7778\n\n\n1\n444.3684\n1.9503\n0.3187\n28.3437\n0.4737\n0.4649\n48.3954\n0.2719\n0.0877\n0.6345\n\n\n\n\n\n\n\n\nWe can visualise the differences in distributions across these groups using the pandas boxplot method.\nWe end up with one plot per column, with two boxplots for each column - one for those who died on the left of each plot, and one for those who survived on the right.\nThey’re not that informative where we have binary columns, unfortunately!\n\ndata.boxplot(\n    by='Survived', # Column to segregate by\n    figsize = (15, 20), # adjust overall size of output figure\n    sharey=False, # Allows each variable to be plotted on its own scale\n    sharex=False # Not strictly necessary but a nice way to get the 0/1 labels on every plot\n    )\n\narray([[&lt;Axes: title={'center': 'Age'}, xlabel='[Survived]'&gt;,\n        &lt;Axes: title={'center': 'Embarked_Cherbourg'}, xlabel='[Survived]'&gt;,\n        &lt;Axes: title={'center': 'Embarked_Queenstown'}, xlabel='[Survived]'&gt;],\n       [&lt;Axes: title={'center': 'Embarked_Southampton'}, xlabel='[Survived]'&gt;,\n        &lt;Axes: title={'center': 'Fare'}, xlabel='[Survived]'&gt;,\n        &lt;Axes: title={'center': 'IsMale'}, xlabel='[Survived]'&gt;],\n       [&lt;Axes: title={'center': 'Parch'}, xlabel='[Survived]'&gt;,\n        &lt;Axes: title={'center': 'PassengerId'}, xlabel='[Survived]'&gt;,\n        &lt;Axes: title={'center': 'Pclass'}, xlabel='[Survived]'&gt;],\n       [&lt;Axes: title={'center': 'SibSp'}, xlabel='[Survived]'&gt;, &lt;Axes: &gt;,\n        &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\nWe can’t use the pandas .hist() column with the ‘by’ parameter as it plots every column on the same plot! So we’ll create the plot we want by running the hist command on two separate filtered dataframes - one containing those who survived and one containing those who didn’t.\n\nfig, axs = plt.subplots(11,1, figsize=(8, 26))\n\ndata[data['Survived'] == 0].hist(bins=30, color='blue', alpha=0.7, density=True, ax=axs)\ndata[data['Survived'] == 1].hist(bins=30, color='red', alpha=0.7, density=True, ax=axs)\n\narray([&lt;Axes: title={'center': 'PassengerId'}&gt;,\n       &lt;Axes: title={'center': 'Survived'}&gt;,\n       &lt;Axes: title={'center': 'Pclass'}&gt;,\n       &lt;Axes: title={'center': 'IsMale'}&gt;,\n       &lt;Axes: title={'center': 'Age'}&gt;, &lt;Axes: title={'center': 'SibSp'}&gt;,\n       &lt;Axes: title={'center': 'Parch'}&gt;,\n       &lt;Axes: title={'center': 'Fare'}&gt;,\n       &lt;Axes: title={'center': 'Embarked_Cherbourg'}&gt;,\n       &lt;Axes: title={'center': 'Embarked_Queenstown'}&gt;,\n       &lt;Axes: title={'center': 'Embarked_Southampton'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n29.1.1 Look at survival by different groupings\nBy using the pandas groupby method along with functions like mean, we can start to explore the % of people in different categories who survived.\nLet’s first quickly remind ourselves of what the ‘survived’ column looks like.\n\ndata['Survived']\n\n0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n886    0\n887    1\n888    0\n889    1\n890    0\nName: Survived, Length: 891, dtype: int64\n\n\nAs it’s a series of 1’s and 0’s, taking the mean (adding up the values and dividing by the number of values) gives us the proportion of people who survived.\nLet’s start by grouping by sex and seeing who survived.\n\ndata.groupby('IsMale')['Survived'].mean()\n\nIsMale\n0    0.742038\n1    0.188908\nName: Survived, dtype: float64\n\n\nWe can interpret this as 74.2% of women surviving, and 18.8% of men surviving.\nLet’s try with a different grouping - does it work where there are multiple categories?\nWe can try tidying it up a bit too.\n\n(data.groupby('Pclass')['Survived'].mean()*100).round(2)\n\nPclass\n1    62.96\n2    47.28\n3    24.24\nName: Survived, dtype: float64\n\n\nYes! It does work where there are multiple options within a category.\nLet’s take it further - by passing in a list of columns to group by, we can explore multiple layers of detail.\nLet’s turn the output into a dataframe while we’re at it.\n\npd.DataFrame(\n    data.groupby(['Pclass', 'IsMale'])['Survived'].mean().round(4)\n    )\n\n\n\n\n\n\n\n\n\n\nSurvived\n\n\nPclass\nIsMale\n\n\n\n\n\n1\n0\n0.9681\n\n\n1\n0.3689\n\n\n2\n0\n0.9211\n\n\n1\n0.1574\n\n\n3\n0\n0.5000\n\n\n1\n0.1354\n\n\n\n\n\n\n\n\nFor some instances where we think we might have a low number of people belonging to each column and want to check this, we can use the .describe() method instead of the .mean() and manually pull back the columns we are interested in.\nHere, we can see that very few people were travelling with 3 or more parents or children so we might not place too much stock in the associated chances of survival.\n\ndata.groupby('Parch')['Survived'].describe()[['mean','count']].round(4)\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nParch\n\n\n\n\n\n\n0\n0.3437\n678.0\n\n\n1\n0.5508\n118.0\n\n\n2\n0.5000\n80.0\n\n\n3\n0.6000\n5.0\n\n\n4\n0.0000\n4.0\n\n\n5\n0.2000\n5.0\n\n\n6\n0.0000\n1.0\n\n\n\n\n\n\n\n\nFor numeric columns, we can create a new grouping ourselves to investigate the differences.\n\nunder_18 = np.where(data['Age'] &lt; 18 , 'Under 18', 'Over 18')\ndata.groupby(under_18)['Survived'].mean()\n\nOver 18     0.361183\nUnder 18    0.539823\nName: Survived, dtype: float64\n\n\n\nunder_30 = np.where(data['Age'] &lt; 30 , 'Under 30', 'Over 30')\ndata.groupby(under_30)['Survived'].mean()\n\nOver 30     0.366864\nUnder 30    0.406250\nName: Survived, dtype: float64\n\n\nWe can use this new grouping together with a column from our dataframe.\n\ndata.groupby([under_18, 'IsMale'])['Survived'].mean()\n\n          IsMale\nOver 18   0         0.752896\n          1         0.165703\nUnder 18  0         0.690909\n          1         0.396552\nName: Survived, dtype: float64\n\n\n\nover_60 = np.where(data['Age'] &gt;60 , 'Over 60', 'Under 60')\ndata.groupby([over_60, 'Pclass'])['Survived'].mean()\n\n          Pclass\nOver 60   1         0.214286\n          2         0.333333\n          3         0.200000\nUnder 60  1         0.658416\n          2         0.475138\n          3         0.242798\nName: Survived, dtype: float64\n\n\nIn some cases, we may want to assign the results of that column back to our dataframe to use as a new bit of data. Let’s do that with a column that tells us whether someone is travelling with family.\n\ndata['TravellingWithFamily'] = np.where((data['SibSp'] + data['Parch']) &gt;= 1 , 'Travelling with Family', 'Travelling Alone')\ndata.groupby('TravellingWithFamily')['Survived'].mean()\n\nTravellingWithFamily\nTravelling Alone          0.303538\nTravelling with Family    0.505650\nName: Survived, dtype: float64\n\n\n\ndata.groupby(['IsMale', 'TravellingWithFamily'])['Survived'].mean()\n\nIsMale  TravellingWithFamily  \n0       Travelling Alone          0.785714\n        Travelling with Family    0.712766\n1       Travelling Alone          0.155718\n        Travelling with Family    0.271084\nName: Survived, dtype: float64\n\n\n\ndata.groupby(['Pclass', 'TravellingWithFamily'])['Survived'].mean()\n\nPclass  TravellingWithFamily  \n1       Travelling Alone          0.532110\n        Travelling with Family    0.728972\n2       Travelling Alone          0.346154\n        Travelling with Family    0.637500\n3       Travelling Alone          0.212963\n        Travelling with Family    0.299401\nName: Survived, dtype: float64",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#correlation",
    "href": "4j_data_preprocessing_and_eda.html#correlation",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "30.1 Correlation",
    "text": "30.1 Correlation\nWe can create a correlation matrix to look at associations between different columns and also between the column of interest (survived) and each column.\nThe diagonal shows a column’s correlation with itself, which will always be 1!\nRed squares indicate a strong positive correlation between features - as one increases, the other increases.\nBlue squares indicate a strong negative correlation - as one increases, the other decreases.\nPaler squares indicate less strong correlations.\nWe use the seaborn (sns) library here instead of matplotlib (plt) as it has a really nice built-in correlation plot.\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = data.select_dtypes(['number']) .corr()\nsns.heatmap(corr,\n    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n    vmin=-1.0, vmax=1.0,\n    square=True, ax=ax)",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#predictive-power",
    "href": "4j_data_preprocessing_and_eda.html#predictive-power",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "30.2 Predictive Power",
    "text": "30.2 Predictive Power\nAn interesting alternative to pure correlations may be the predictive power score.\nHowever, this doesn’t work with pandas versions &gt;2, which we have in our environment, so we won’t cover this today.\nHowever, if you are interested, take a look at the repository: https://github.com/8080labs/ppscore",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#tidying-up-column-names",
    "href": "4j_data_preprocessing_and_eda.html#tidying-up-column-names",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "31.1 Tidying up Column Names",
    "text": "31.1 Tidying up Column Names\nIf we don’t find the column names of a provided dataset very intuitive, we can tidy these up ourselves.\nLet’s first remind ourselves of the names we have.\n\ndata.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'IsMale', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked_Cherbourg',\n       'Embarked_Queenstown', 'Embarked_Southampton', 'TravellingWithFamily'],\n      dtype='object')\n\n\n\ndata = data.rename(columns={\n            'Parch': 'ParentsOrChildren',\n            'SibSp': 'SiblingsOrSpouses',\n            'Pclass': 'PClass'\n            }\n    )\n\ndata\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPClass\nName\nIsMale\nAge\nSiblingsOrSpouses\nParentsOrChildren\nTicket\nFare\nCabin\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nTravellingWithFamily\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\n1\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\n0\n0\n1\nTravelling with Family\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n0\n38.0\n1\n0\nPC 17599\n71.2833\nC85\n1\n0\n0\nTravelling with Family\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\n0\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\n0\n0\n1\nTravelling Alone\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n0\n35.0\n1\n0\n113803\n53.1000\nC123\n0\n0\n1\nTravelling with Family\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\n1\n35.0\n0\n0\n373450\n8.0500\nNaN\n0\n0\n1\nTravelling Alone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\n1\n27.0\n0\n0\n211536\n13.0000\nNaN\n0\n0\n1\nTravelling Alone\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\n0\n19.0\n0\n0\n112053\n30.0000\nB42\n0\n0\n1\nTravelling Alone\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\n0\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\n0\n0\n1\nTravelling with Family\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\n1\n26.0\n0\n0\n111369\n30.0000\nC148\n1\n0\n0\nTravelling Alone\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\n1\n32.0\n0\n0\n370376\n7.7500\nNaN\n0\n1\n0\nTravelling Alone\n\n\n\n\n891 rows × 15 columns",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#getting-rid-of-columns-we-wont-use-and-setting-the-datatype",
    "href": "4j_data_preprocessing_and_eda.html#getting-rid-of-columns-we-wont-use-and-setting-the-datatype",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "31.2 Getting rid of columns we won’t use and setting the datatype",
    "text": "31.2 Getting rid of columns we won’t use and setting the datatype\nWe can drop ‘Name’, ‘Ticket’, ‘Cabin’ and ‘TravellingWithFamily’ here - though look at the feature_engineering notebook for how we could make better use of each of these!\nFor now, we’re just trying to create something that is similar - though slightly nicer to work with - than the processed dataset we’ve used.\n\ndata = data.drop(columns=['Name', 'Ticket', 'Cabin', 'TravellingWithFamily'])\ndata.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPClass\nIsMale\nAge\nSiblingsOrSpouses\nParentsOrChildren\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\n\n\n\n\n0\n1\n0\n3\n1\n22.0\n1\n0\n7.2500\n0\n0\n1\n\n\n1\n2\n1\n1\n0\n38.0\n1\n0\n71.2833\n1\n0\n0\n\n\n2\n3\n1\n3\n0\n26.0\n0\n0\n7.9250\n0\n0\n1\n\n\n3\n4\n1\n1\n0\n35.0\n1\n0\n53.1000\n0\n0\n1\n\n\n4\n5\n0\n3\n1\n35.0\n0\n0\n8.0500\n0\n0\n1\n\n\n\n\n\n\n\n\nAll of our columns here are integers or floats so can be dealt with by the models.\nIf they were not, we could use\ndata = data.astype('float')",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_data_preprocessing_and_eda.html#a-quick-note-on-scaling-and-imputation",
    "href": "4j_data_preprocessing_and_eda.html#a-quick-note-on-scaling-and-imputation",
    "title": "24  Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)",
    "section": "32.1 A quick note on scaling and imputation",
    "text": "32.1 A quick note on scaling and imputation\nDepending on the type of model we choose to use, we may need to scale the data and/or we may need to manually deal with missing data.\nGenerally we don’t need to do these steps for tree-based models.\nFirst, let’s deal with the missing data. Here, we are just using a simple imputation method, but the options are discussed in more depth in the relevant notebook.\nThere are some debates about whether to impute first or standardise first.\n\nfrom sklearn.impute import SimpleImputer\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nX_train = imp_mean.fit_transform(X_train)\nX_val = imp_mean.fit_transform(X_val)\n\nNow let’s scale the data - as mentioned, this isn’t necessary for tree-based algorithms.\nFor other non-tree algorithms (that aren’t deep learning), we tend to use standardisation.\nFor deep learning models, we tend to use normalisation, which can be applied very similarly to the below.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\n# Apply the scaler to the training and test sets\nX_train_standardised = sc.fit_transform(X_train)\nX_val_standardised = sc.fit_transform(X_val)\nX_test_standardised = sc.fit_transform(X_test)\n\nFinally we can fit a model to our standardised data - here, we’re going to use a logistic regression - and explore its performance.\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train_standardised, y_train)\n\ny_val_pred = model.predict(X_val_standardised)\n\npd.DataFrame(\n    classification_report(y_val, y_val_pred, output_dict=True)\n)\n\n\n\n\n\n\n\n\n\n0\n1\naccuracy\nmacro avg\nweighted avg\n\n\n\n\nprecision\n0.842697\n0.777778\n0.818182\n0.810237\n0.817274\n\n\nrecall\n0.862069\n0.750000\n0.818182\n0.806034\n0.818182\n\n\nf1-score\n0.852273\n0.763636\n0.818182\n0.807955\n0.817562\n\n\nsupport\n87.000000\n56.000000\n0.818182\n143.000000\n143.000000\n\n\n\n\n\n\n\n\nWe could then undertake further hyperparameter optimisation, feature selection and feature engineering, testing all of this with our validation dataset, before doing a final test of our resulting model on the test dataset we created.",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Data Preprocessing and Exploratory Data Analysis (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_ensembles_voting.html",
    "href": "4j_ensembles_voting.html",
    "title": "25  Ensembles (Titanic Dataset)",
    "section": "",
    "text": "25.1 Creating an Ensemble: Using VotingClassifier\nFirst, let’s try creating an ensemble of these two models.\nWe pass in a list containing a tuple per model; the tuple needs to have a name for the model, and the model object itself.\nHere, we’ve chosen ‘hard’ voting, which means it just looks at the prediction from each model and uses the majority vote.\nvoting_classifier_1 = VotingClassifier(\n    estimators=[('dt', clf1), ('xGB', clf2)],\n    voting='hard')\nWe then just use our fit_train function on this, appending the results to the end of our existing results_df.\nThe output of VotingClassifier is a model object - just as if we’d created a RandomForestClassifier() or XGBClassifier(). This means we can use all of the normal features like .fit() and .predict().\nresults_df = pd.concat(\n    [results_df,\n     fit_train(model=voting_classifier_1, name=\"DT, XGB: hard\")]\n     )\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nDecision Tree\n0.886\n0.818\n0.814\n0.800\n0.800\n0.805\n10\n16\n\n\nDT, XGB: hard\n0.898\n0.832\n0.836\n0.808\n0.808\n0.817\n7\n17",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Ensembles (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_ensembles_voting.html#creating-an-ensemble-using-votingclassifier",
    "href": "4j_ensembles_voting.html#creating-an-ensemble-using-votingclassifier",
    "title": "25  Ensembles (Titanic Dataset)",
    "section": "",
    "text": "25.1.1 Working with more classifiers\nNow let’s try this with some additional models.\nWe’re going to use some additional models that require the data to be standardised, so let’s do that first.\n\n\n25.1.2 Standardisation\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\n# Apply the scaler to the training and test sets\nX_train_standardised = sc.fit_transform(X_train)\nX_validate_standardised = sc.fit_transform(X_validate)\nX_test_standardised = sc.fit_transform(X_test)\n\n\n\n25.1.3 Creating Additional Models\n\nclf3 = KNeighborsClassifier(n_neighbors=7)\n\nclf4 = SVC(kernel='rbf', probability=True)\n\nclf5 = LogisticRegression()\n\n\n\n25.1.4 A more complex voting classifier\n\nvoting_classifier_2 = VotingClassifier(estimators=[\n    ('XGBoost', clf1),\n    ('Decision Tree', clf2),\n    ('K-Nearest Neighbours', clf3),\n    ('SVC', clf4),\n    ('Logistic Regression', clf5)\n    ],\n    voting='hard')\n\nLet’s now just append our results and view our updated table.\n\n\nresults_df = pd.concat([\n    results_df,\n    fit_train(\n        X_train=X_train_standardised,\n        X_validate=X_validate_standardised,\n        model=voting_classifier_2,\n        name=\"DT, XGBoost, KNN, LogReg + SVC: hard\")\n        ])\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nDecision Tree\n0.886\n0.818\n0.814\n0.800\n0.800\n0.805\n10\n16\n\n\nDT, XGB: hard\n0.898\n0.832\n0.836\n0.808\n0.808\n0.817\n7\n17\n\n\nDT, XGBoost, KNN, LogReg + SVC: hard\n0.880\n0.797\n0.788\n0.782\n0.782\n0.785\n13\n16",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Ensembles (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_ensembles_voting.html#hard-and-soft-voting",
    "href": "4j_ensembles_voting.html#hard-and-soft-voting",
    "title": "25  Ensembles (Titanic Dataset)",
    "section": "25.2 Hard and Soft Voting",
    "text": "25.2 Hard and Soft Voting\nWe previously used the ‘hard’ voting parameter, which looks at the predicted class from each classifier and takes the majority vote.\nInstead, the ‘soft’ classifier looks at the predicted probabilities from each classifier and averages them.\nThis does mean that each model that is passed in must have a .predict_proba() method - most do.\n\nvoting_classifier_1 = VotingClassifier(\n    estimators=[('dt', clf1), ('xGB', clf2)],\n    voting='soft')\n\nresults_df = pd.concat(\n    [results_df,\n     fit_train(model=voting_classifier_1, name=\"DT, XGB: soft\")]\n     )\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nDecision Tree\n0.886\n0.818\n0.814\n0.800\n0.800\n0.805\n10\n16\n\n\nDT, XGB: hard\n0.898\n0.832\n0.836\n0.808\n0.808\n0.817\n7\n17\n\n\nDT, XGBoost, KNN, LogReg + SVC: hard\n0.880\n0.797\n0.788\n0.782\n0.782\n0.785\n13\n16\n\n\nDT, XGB: soft\n0.951\n0.804\n0.795\n0.795\n0.795\n0.795\n14\n14\n\n\n\n\n\n\n\n\n\n25.2.1 Weighting classifiers\nWhether working with hard or soft voting, we can also weight the predictions of different models.\nHere, we give the prediction of the decision tree twice the weight of the XGBoost Model.\n\nvoting_classifier_1 = VotingClassifier(\n    estimators=[('dt', clf1), ('xGB', clf2)],\n    voting='soft',\n    weights=[1, 2])\n\nresults_df = pd.concat(\n    [results_df,\n     fit_train(model=voting_classifier_1, name=\"DT, XGB: soft, 2:1\")]\n     )\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nDecision Tree\n0.886\n0.818\n0.814\n0.800\n0.800\n0.805\n10\n16\n\n\nDT, XGB: hard\n0.898\n0.832\n0.836\n0.808\n0.808\n0.817\n7\n17\n\n\nDT, XGBoost, KNN, LogReg + SVC: hard\n0.880\n0.797\n0.788\n0.782\n0.782\n0.785\n13\n16\n\n\nDT, XGB: soft\n0.951\n0.804\n0.795\n0.795\n0.795\n0.795\n14\n14\n\n\nDT, XGB: soft, 2:1\n0.924\n0.818\n0.810\n0.806\n0.806\n0.808\n12\n14\n\n\n\n\n\n\n\n\nWe can also apply each of these to more complex ensembles. Here, let’s try soft voting with our 5-model ensemble.\n\nvoting_classifier_2 = VotingClassifier(estimators=[\n    ('XGBoost', clf1),\n    ('Decision Tree', clf2),\n    ('K-Nearest Neighbours', clf3),\n    ('SVC', clf4),\n    ('Logistic Regression', clf5)\n    ],\n    voting='soft')\n\nresults_df = pd.concat([\n    results_df,\n    fit_train(\n        X_train=X_train_standardised,\n        X_validate=X_validate_standardised,\n        model=voting_classifier_2,\n        name=\"DT, XGBoost, KNN + SVC: soft\")])\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nDecision Tree\n0.886\n0.818\n0.814\n0.800\n0.800\n0.805\n10\n16\n\n\nDT, XGB: hard\n0.898\n0.832\n0.836\n0.808\n0.808\n0.817\n7\n17\n\n\nDT, XGBoost, KNN, LogReg + SVC: hard\n0.880\n0.797\n0.788\n0.782\n0.782\n0.785\n13\n16\n\n\nDT, XGB: soft\n0.951\n0.804\n0.795\n0.795\n0.795\n0.795\n14\n14\n\n\nDT, XGB: soft, 2:1\n0.924\n0.818\n0.810\n0.806\n0.806\n0.808\n12\n14\n\n\nDT, XGBoost, KNN + SVC: soft\n0.916\n0.811\n0.803\n0.797\n0.797\n0.800\n12\n15\n\n\n\n\n\n\n\n\nNow let’s try weighting this and see the impact.\n\nvoting_classifier_2 = VotingClassifier(estimators=[\n    ('XGBoost', clf1),\n    ('Decision Tree', clf2),\n    ('K-Nearest Neighbours', clf3),\n    ('SVC', clf4),\n    ('Logistic Regression', clf5)\n    ],\n    voting='soft',\n    weights=[2,2,1,1,2])\n\nresults_df = pd.concat([\n    results_df,\n    fit_train(\n        X_train=X_train_standardised,\n        X_validate=X_validate_standardised,\n        model=voting_classifier_2,\n        name=\"DT, XGBoost, KNN + SVC: soft, 2:2:1:1:2\")])\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nXGBoost\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nDecision Tree\n0.886\n0.818\n0.814\n0.800\n0.800\n0.805\n10\n16\n\n\nDT, XGB: hard\n0.898\n0.832\n0.836\n0.808\n0.808\n0.817\n7\n17\n\n\nDT, XGBoost, KNN, LogReg + SVC: hard\n0.880\n0.797\n0.788\n0.782\n0.782\n0.785\n13\n16\n\n\nDT, XGB: soft\n0.951\n0.804\n0.795\n0.795\n0.795\n0.795\n14\n14\n\n\nDT, XGB: soft, 2:1\n0.924\n0.818\n0.810\n0.806\n0.806\n0.808\n12\n14\n\n\nDT, XGBoost, KNN + SVC: soft\n0.916\n0.811\n0.803\n0.797\n0.797\n0.800\n12\n15\n\n\nDT, XGBoost, KNN + SVC: soft, 2:2:1:1:2\n0.923\n0.818\n0.812\n0.803\n0.803\n0.807\n11\n15",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Ensembles (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html",
    "href": "4j_feature_engineering.html",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "",
    "text": "26.1 Binary features - Simple Conditions\nFirst, let’s make a feature that reflects whether the passenger is under 16.\ndata['Under18'] = np.where(data['Age'] &lt; 18 , 1, 0)\ndata.head(10)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n0\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n0\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n1\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n0\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n1",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html#binary-features---using-a-combination-of-other-columns",
    "href": "4j_feature_engineering.html#binary-features---using-a-combination-of-other-columns",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "26.2 Binary Features - Using a combination of other columns",
    "text": "26.2 Binary Features - Using a combination of other columns\nNext, let’s make a column that reflects whether the passenger is travelling alone or with any family.\n\ndata['TravellingWithFamily'] = np.where(\n    (data['SibSp'] + data['Parch']) &gt;= 1, # Condition\n    1, # Value if True\n    0 # Value if False\n    )\n\ndata.head(10)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n1\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n1\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n0\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n0\n0\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n0\n0\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n1\n1\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n0\n1\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n1\n1",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html#features-from-complex-string-patterns---regular-expressions",
    "href": "4j_feature_engineering.html#features-from-complex-string-patterns---regular-expressions",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "26.3 Features from Complex String Patterns - Regular Expressions",
    "text": "26.3 Features from Complex String Patterns - Regular Expressions\nNow let’s extract groups of titles from the name column.\nWe could extract each individual title, but if there are a lot, this could lead to a very high number of columns when we go to one-hot encode our data. What we want to mainly explore from titles is whether people were noble, general populace, or young.\nThe easiest way to do this here is to use something called Regular Expressions.\nRegular expressions allow us to match varying strings, like telephone numbers that may have different spacings or numbers of digits. We won’t be using them to their full potential here! But they have the benefit of being usable with the contains method to allow us to pass a series of strings we would like to match with.\nFirst, let’s import the re library.\n\nimport re\n\nBefore turning this into a column, let’s explore how we would match strings for a couple of categories of passengers.\nLet’s start with looking at upper-class titles.\nHere, we’re going to filter to rows where the ‘name’ column contains any one of\n\nCol.\nCapt.\nDon.\nCountess.\nDr.\nLady.\nSir.\nMajor.\nRev.\nJonkheer.\nDona.\n\nWe want to ensure we look out for a . after these as in this dataset, a . is consistently used after a title. This is handy for us!\nHowever, in regex, a . has a special meaning - so we need to pass a \\ before it to tell the regular expression we’re looking for an actual full stop.\nWe then use the pipe | to denote or.\n\ndata[data['Name'].str.contains(r\"Col\\.|Capt\\.|Don\\.|Countess\\.|Dr\\.|Lady\\.|Sir\\.|Major\\.|Rev\\.|Jonkheer\\.|Dona\\.\")]\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\n\n\n\n\n30\n31\n0\n1\nUruchurtu, Don. Manuel E\nmale\n40.0\n0\n0\nPC 17601\n27.7208\nNaN\nC\n0\n0\n\n\n149\n150\n0\n2\nByles, Rev. Thomas Roussel Davids\nmale\n42.0\n0\n0\n244310\n13.0000\nNaN\nS\n0\n0\n\n\n150\n151\n0\n2\nBateman, Rev. Robert James\nmale\n51.0\n0\n0\nS.O.P. 1166\n12.5250\nNaN\nS\n0\n0\n\n\n245\n246\n0\n1\nMinahan, Dr. William Edward\nmale\n44.0\n2\n0\n19928\n90.0000\nC78\nQ\n0\n1\n\n\n249\n250\n0\n2\nCarter, Rev. Ernest Courtenay\nmale\n54.0\n1\n0\n244252\n26.0000\nNaN\nS\n0\n1\n\n\n317\n318\n0\n2\nMoraweck, Dr. Ernest\nmale\n54.0\n0\n0\n29011\n14.0000\nNaN\nS\n0\n0\n\n\n398\n399\n0\n2\nPain, Dr. Alfred\nmale\n23.0\n0\n0\n244278\n10.5000\nNaN\nS\n0\n0\n\n\n449\n450\n1\n1\nPeuchen, Major. Arthur Godfrey\nmale\n52.0\n0\n0\n113786\n30.5000\nC104\nS\n0\n0\n\n\n536\n537\n0\n1\nButt, Major. Archibald Willingham\nmale\n45.0\n0\n0\n113050\n26.5500\nB38\nS\n0\n0\n\n\n556\n557\n1\n1\nDuff Gordon, Lady. (Lucille Christiana Sutherl...\nfemale\n48.0\n1\n0\n11755\n39.6000\nA16\nC\n0\n1\n\n\n599\n600\n1\n1\nDuff Gordon, Sir. Cosmo Edmund (\"Mr Morgan\")\nmale\n49.0\n1\n0\nPC 17485\n56.9292\nA20\nC\n0\n1\n\n\n626\n627\n0\n2\nKirkland, Rev. Charles Leonard\nmale\n57.0\n0\n0\n219533\n12.3500\nNaN\nQ\n0\n0\n\n\n632\n633\n1\n1\nStahelin-Maeglin, Dr. Max\nmale\n32.0\n0\n0\n13214\n30.5000\nB50\nC\n0\n0\n\n\n647\n648\n1\n1\nSimonius-Blumer, Col. Oberst Alfons\nmale\n56.0\n0\n0\n13213\n35.5000\nA26\nC\n0\n0\n\n\n660\n661\n1\n1\nFrauenthal, Dr. Henry William\nmale\n50.0\n2\n0\nPC 17611\n133.6500\nNaN\nS\n0\n1\n\n\n694\n695\n0\n1\nWeir, Col. John\nmale\n60.0\n0\n0\n113800\n26.5500\nNaN\nS\n0\n0\n\n\n745\n746\n0\n1\nCrosby, Capt. Edward Gifford\nmale\n70.0\n1\n1\nWE/P 5735\n71.0000\nB22\nS\n0\n1\n\n\n759\n760\n1\n1\nRothes, the Countess. of (Lucy Noel Martha Dye...\nfemale\n33.0\n0\n0\n110152\n86.5000\nB77\nS\n0\n0\n\n\n766\n767\n0\n1\nBrewe, Dr. Arthur Jackson\nmale\nNaN\n0\n0\n112379\n39.6000\nNaN\nC\n0\n0\n\n\n796\n797\n1\n1\nLeader, Dr. Alice (Farnham)\nfemale\n49.0\n0\n0\n17465\n25.9292\nD17\nS\n0\n0\n\n\n822\n823\n0\n1\nReuchlin, Jonkheer. John George\nmale\n38.0\n0\n0\n19972\n0.0000\nNaN\nS\n0\n0\n\n\n848\n849\n0\n2\nHarper, Rev. John\nmale\n28.0\n0\n1\n248727\n33.0000\nNaN\nS\n0\n1\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n0\n0\n\n\n\n\n\n\n\n\nLet’s repeat this to look at rows containing other groupings of titles.\nWhat about ‘regular’ class people?\n\ndata[data['Name'].str.contains(r\"Mrs\\.|Mlle\\.|Mr\\.|Mons\\.\")]\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n1\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n1\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n0\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n883\n884\n0\n2\nBanfield, Mr. Frederick James\nmale\n28.0\n0\n0\nC.A./SOTON 34068\n10.5000\nNaN\nS\n0\n0\n\n\n884\n885\n0\n3\nSutehall, Mr. Henry Jr\nmale\n25.0\n0\n0\nSOTON/OQ 392076\n7.0500\nNaN\nS\n0\n0\n\n\n885\n886\n0\n3\nRice, Mrs. William (Margaret Norton)\nfemale\n39.0\n0\n5\n382652\n29.1250\nNaN\nQ\n0\n1\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n0\n0\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n0\n0\n\n\n\n\n644 rows × 14 columns\n\n\n\n\nNow let’s look at instances relating to children.\n\ndata[data['Name'].str.contains(r\"Master\\.|Miss\\.\")]\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\n\n\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n0\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n1\n1\n\n\n10\n11\n1\n3\nSandstrom, Miss. Marguerite Rut\nfemale\n4.0\n1\n1\nPP 9549\n16.7000\nG6\nS\n1\n1\n\n\n11\n12\n1\n1\nBonnell, Miss. Elizabeth\nfemale\n58.0\n0\n0\n113783\n26.5500\nC103\nS\n0\n0\n\n\n14\n15\n0\n3\nVestrom, Miss. Hulda Amanda Adolfina\nfemale\n14.0\n0\n0\n350406\n7.8542\nNaN\nS\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n869\n870\n1\n3\nJohnson, Master. Harold Theodor\nmale\n4.0\n1\n1\n347742\n11.1333\nNaN\nS\n1\n1\n\n\n875\n876\n1\n3\nNajib, Miss. Adele Kiamie \"Jane\"\nfemale\n15.0\n0\n0\n2667\n7.2250\nNaN\nC\n1\n0\n\n\n882\n883\n0\n3\nDahlberg, Miss. Gerda Ulrika\nfemale\n22.0\n0\n0\n7552\n10.5167\nNaN\nS\n0\n0\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n0\n0\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n0\n1\n\n\n\n\n222 rows × 14 columns\n\n\n\n\nLet’s put in all of the ones we’ve come up with so far and check we haven’t missed anyone.\nWe’ve used the ~ operator to filter our dataframe to instances where a match has NOT been found instead.\n\ndata[~data['Name'].str.contains(r\"Master\\.|Miss\\.|Mrs\\.|Mlle\\.|Mr\\.|Mons\\.|Col\\.|Capt\\.|Don\\.|Countess\\.|Dr\\.|Lady\\.|Sir\\.|Major\\.|Rev\\.|Jonkheer\\.|Dona\\.\")]\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\n\n\n\n\n369\n370\n1\n1\nAubart, Mme. Leontine Pauline\nfemale\n24.0\n0\n0\nPC 17477\n69.3\nB35\nC\n0\n0\n\n\n443\n444\n1\n2\nReynaldo, Ms. Encarnacion\nfemale\n28.0\n0\n0\n230434\n13.0\nNaN\nS\n0\n0\n\n\n\n\n\n\n\n\nIt looks like we’ve missed Mme. and Ms. so we’ll make sure to add those into the ‘Regular’ status for our final query.\n\n26.3.1 Creating the column\nNow we’ve explored and built our query, we can turn this into our final column.\nWe’re going to use np.where again - this does get a bit complex with longer queries!\nIt effectively is structured like a big if-else clause.\nnp.where(condition, value_if_true, value_if_false)\nWe nest these by passing additional np.where statements in the value if false position.\nThis may feel familiar if you’ve had to do a big if clause in Excel!\n(note that this would be a lot easier in a newer version of pandas - they released a ‘case_when’ function in v2.2 - those of you used to SQL may be familiar with that sort of thing)\nFirst, let’s just run the statement and see the output.\nI’ve added in some extra complexity to look at the ‘age’ column as well for the first two instances.\nYou may wish to indent your code differently for readability - the indentation won’t have any impact on the functioning here.\n\nnp.where(\n    (data['Name'].str.contains(r\"Master\\.|Miss\\.\")) & (data['Age'] &lt;18), \"Young\",\n    np.where((data['Name'].str.contains(r\"Miss\\.\")) & (data['Age'] &gt;=18), \"Unmarried Woman\",\n    np.where(data['Name'].str.contains(r\"Mrs\\.|Mlle\\.|Mr\\.|Mons\\.\"), \"Regular\",\n    np.where(data['Name'].str.contains(r\"Col\\.|Capt\\.|Don\\.|Countess\\.|Dr\\.|Lady\\.|Sir\\.|Major\\.|Rev\\.|Jonkheer\\.|Dona\\.\"), \"Upper Class\",\n    \"Unknown\"\n    )))\n    )\n\narray(['Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Young', 'Regular', 'Regular', 'Young',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young', 'Regular',\n       'Young', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Young', 'Regular', 'Young', 'Regular', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Upper Class', 'Regular', 'Unknown',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Young', 'Regular', 'Regular', 'Regular',\n       'Young', 'Unmarried Woman', 'Regular', 'Regular', 'Unknown',\n       'Regular', 'Regular', 'Young', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Young',\n       'Young', 'Regular', 'Unmarried Woman', 'Regular', 'Young',\n       'Regular', 'Unknown', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Young', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Young', 'Unmarried Woman', 'Regular',\n       'Regular', 'Unknown', 'Regular', 'Young', 'Regular', 'Regular',\n       'Regular', 'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Regular', 'Unknown', 'Regular', 'Young', 'Regular',\n       'Unmarried Woman', 'Young', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Young', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Young', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Young', 'Regular', 'Upper Class',\n       'Upper Class', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Young', 'Regular', 'Regular', 'Unknown', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Young', 'Young', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Young', 'Young',\n       'Regular', 'Regular', 'Regular', 'Unknown', 'Unmarried Woman',\n       'Regular', 'Regular', 'Unknown', 'Regular', 'Young', 'Young',\n       'Young', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Young', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Unknown',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Young', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Unmarried Woman', 'Regular', 'Unmarried Woman',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Unknown', 'Regular',\n       'Regular', 'Regular', 'Young', 'Regular', 'Unknown', 'Regular',\n       'Young', 'Regular', 'Regular', 'Unknown', 'Unknown', 'Regular',\n       'Regular', 'Regular', 'Upper Class', 'Unmarried Woman', 'Regular',\n       'Regular', 'Upper Class', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Unmarried Woman',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Unknown', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unknown', 'Unmarried Woman', 'Unmarried Woman', 'Regular',\n       'Young', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Unmarried Woman', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Young',\n       'Regular', 'Regular', 'Unknown', 'Regular', 'Regular', 'Unknown',\n       'Regular', 'Young', 'Unknown', 'Regular', 'Regular',\n       'Unmarried Woman', 'Unmarried Woman', 'Unmarried Woman', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Upper Class',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Unmarried Woman',\n       'Regular', 'Regular', 'Regular', 'Young', 'Unknown', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Unmarried Woman', 'Regular', 'Young',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Unmarried Woman', 'Unmarried Woman', 'Unknown',\n       'Unknown', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unknown', 'Unknown', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Young', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Young', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Young', 'Unmarried Woman', 'Regular', 'Young',\n       'Regular', 'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Regular', 'Unmarried Woman', 'Regular', 'Upper Class', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Young', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Young',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unknown', 'Regular', 'Young', 'Young',\n       'Regular', 'Young', 'Upper Class', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Young', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Young', 'Young', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Unknown', 'Unmarried Woman', 'Young',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Unmarried Woman',\n       'Regular', 'Regular', 'Regular', 'Young', 'Regular', 'Regular',\n       'Regular', 'Unmarried Woman', 'Young', 'Upper Class',\n       'Unmarried Woman', 'Regular', 'Unmarried Woman', 'Unmarried Woman',\n       'Young', 'Young', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Young', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Unmarried Woman', 'Regular', 'Upper Class', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unknown', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unknown', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Upper Class', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Unmarried Woman', 'Regular',\n       'Regular', 'Young', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Upper Class', 'Unmarried Woman',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Upper Class',\n       'Regular', 'Young', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Young', 'Regular',\n       'Young', 'Regular', 'Regular', 'Upper Class', 'Regular',\n       'Unmarried Woman', 'Regular', 'Unmarried Woman', 'Regular',\n       'Unknown', 'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Upper Class', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Young', 'Regular', 'Young',\n       'Regular', 'Regular', 'Upper Class', 'Regular', 'Regular',\n       'Unknown', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Unmarried Woman', 'Unknown', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Unmarried Woman',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Unknown',\n       'Regular', 'Unmarried Woman', 'Unmarried Woman', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Unmarried Woman',\n       'Regular', 'Regular', 'Upper Class', 'Regular', 'Unmarried Woman',\n       'Regular', 'Regular', 'Young', 'Young', 'Regular', 'Regular',\n       'Regular', 'Young', 'Regular', 'Regular', 'Regular', 'Upper Class',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Upper Class', 'Unmarried Woman', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Young', 'Regular', 'Regular', 'Young', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unmarried Woman', 'Young',\n       'Young', 'Regular', 'Regular', 'Regular', 'Unknown', 'Regular',\n       'Regular', 'Regular', 'Upper Class', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Young', 'Young', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Young', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Upper Class', 'Regular', 'Young', 'Regular', 'Regular',\n       'Young', 'Regular', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Regular', 'Unmarried Woman', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Unmarried Woman',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Upper Class', 'Regular', 'Young', 'Regular', 'Young', 'Young',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Unknown', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Young', 'Regular',\n       'Regular', 'Regular', 'Regular', 'Regular', 'Regular',\n       'Unmarried Woman', 'Regular', 'Regular', 'Regular', 'Upper Class',\n       'Unmarried Woman', 'Unknown', 'Regular', 'Regular'], dtype='&lt;U15')\n\n\nWe can see it outputs a numpy array - effectively a big list - with the matched statement for each row.\nNow let’s assign that to a column in our dataframe and take a look at it in context.\n\ndata['Title'] = np.where(\n    (data['Name'].str.contains(r\"Master\\.|Miss\\.\")) & (data['Age'] &lt;18), \"Young\",\n    np.where((data['Name'].str.contains(r\"Miss\\.\")) & (data['Age'] &gt;=18), \"Unmarried Woman\",\n    np.where(data['Name'].str.contains(r\"Mrs\\.|Mlle\\.|Mr\\.|Mons\\.\"), \"Regular\",\n    np.where(data['Name'].str.contains(r\"Col\\.|Capt\\.|Don\\.|Countess\\.|Dr\\.|Lady\\.|Sir\\.|Major\\.|Rev\\.|Jonkheer\\.|Dona\\.\"), \"Upper Class\",\n    \"Unknown\"\n    ))))\n\n# Select just the name, age and title columns\n# Return a random sample of 15 rows\ndata[['Name', 'Age', 'Title']].sample(15)\n\n\n\n\n\n\n\n\n\nName\nAge\nTitle\n\n\n\n\n691\nKarun, Miss. Manca\n4.0\nYoung\n\n\n194\nBrown, Mrs. James Joseph (Margaret Tobin)\n44.0\nRegular\n\n\n719\nJohnson, Mr. Malkolm Joackim\n33.0\nRegular\n\n\n805\nJohansson, Mr. Karl Johan\n31.0\nRegular\n\n\n125\nNicola-Yarred, Master. Elias\n12.0\nYoung\n\n\n441\nHampe, Mr. Leon\n20.0\nRegular\n\n\n225\nBerglund, Mr. Karl Ivar Sven\n22.0\nRegular\n\n\n354\nYousif, Mr. Wazli\nNaN\nRegular\n\n\n260\nSmith, Mr. Thomas\nNaN\nRegular\n\n\n409\nLefebre, Miss. Ida\nNaN\nUnknown\n\n\n444\nJohannesen-Bratthammer, Mr. Bernt\nNaN\nRegular\n\n\n560\nMorrow, Mr. Thomas Rowan\nNaN\nRegular\n\n\n739\nNankoff, Mr. Minko\nNaN\nRegular\n\n\n175\nKlasen, Mr. Klas Albin\n18.0\nRegular\n\n\n289\nConnolly, Miss. Kate\n22.0\nUnmarried Woman\n\n\n\n\n\n\n\n\nFinally, let’s see how many people fall into each category.\n\ndata['Title'].value_counts()\n\nRegular            644\nUnmarried Woman     95\nYoung               87\nUnknown             42\nUpper Class         23\nName: Title, dtype: int64",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html#feature-from-simple-string-patterns---getting-the-deck-from-the-cabin-number",
    "href": "4j_feature_engineering.html#feature-from-simple-string-patterns---getting-the-deck-from-the-cabin-number",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "26.4 Feature from Simple String Patterns - Getting the Deck from the Cabin Number",
    "text": "26.4 Feature from Simple String Patterns - Getting the Deck from the Cabin Number\nLet’s also work out which deck they were staying on based on the cabin number.\nLet’s first look at a single cabin number from our dataframe.\n\ndata['Cabin'][1]\n\n'C85'\n\n\nOn further inspection, it turns out that the first letter of the cabin string is always the deck number.\nWe can just pull back the .str attribute in each case and pull out the 1st letter (in position 0 - remember Python counts from 0).\n\ndata['Deck'] = data['Cabin'].str[0]\ndata.head(10)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\nTitle\nDeck\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n1\nRegular\nNaN\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n1\nRegular\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n0\nUnmarried Woman\nNaN\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n1\nRegular\nC\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n0\nRegular\nNaN\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n0\n0\nRegular\nNaN\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n0\n0\nRegular\nE\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n1\n1\nYoung\nNaN\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n0\n1\nRegular\nNaN\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n1\n1\nRegular\nNaN\n\n\n\n\n\n\n\n\nLet’s see how many people were on each deck, and how many didn’t have a cabin number.\n\ndata['Deck'].value_counts(dropna=False)\n\nNaN    687\nC       59\nB       47\nD       33\nE       32\nA       15\nF       13\nG        4\nT        1\nName: Deck, dtype: int64\n\n\n\n26.4.1 Numeric Columns - Adding Values - Family Size\nLet’s calculate the family size by adding two numeric columns together.\n\ndata['FamilySize'] = data['SibSp'] + data['Parch']\n\ndata.head(10)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\nTitle\nDeck\nFamilySize\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n1\nRegular\nNaN\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n1\nRegular\nC\n1\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n0\nUnmarried Woman\nNaN\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n1\nRegular\nC\n1\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n0\nRegular\nNaN\n0\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n0\n0\nRegular\nNaN\n0\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n0\n0\nRegular\nE\n0\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n1\n1\nYoung\nNaN\n4\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n0\n1\nRegular\nNaN\n2\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n1\n1\nRegular\nNaN\n1",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html#joining-to-counts---shared-tickets-and-the-impact-on-fare",
    "href": "4j_feature_engineering.html#joining-to-counts---shared-tickets-and-the-impact-on-fare",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "26.5 Joining to Counts - Shared Tickets and the Impact on Fare",
    "text": "26.5 Joining to Counts - Shared Tickets and the Impact on Fare\nApparently tickets were shared between families. Let’s explore this.\nBy using the ‘value_counts’ method twice in a row - we first get a count of how many times each ticket number appeared, - and next we get a count of how many times a ticket was shared between 1 person, how many times a ticket was shared between 2 people, and so on.\n\ndata['Ticket'].value_counts().value_counts()\n\n1    547\n2     94\n3     21\n4     11\n7      3\n6      3\n5      2\nName: Ticket, dtype: int64\n\n\nIt looks like this might be interesting and might have some impact on our fare value as well - so let’s create a column for fare per person.\nFirst, though, we’ll need to add a column for the counts of how many times a ticket does appear in the dataset.\nWe start by creating a dataframe from the counts.\n\nticket_counts_df = (pd.DataFrame(data['Ticket'].value_counts())\n                    .reset_index()\n                    .rename(columns={\"Ticket\": \"NumPeopleTicketShared\"})\n                    )\n\n\nticket_counts_df\n\n\n\n\n\n\n\n\n\nindex\nNumPeopleTicketShared\n\n\n\n\n0\n347082\n7\n\n\n1\nCA. 2343\n7\n\n\n2\n1601\n7\n\n\n3\n3101295\n6\n\n\n4\nCA 2144\n6\n\n\n...\n...\n...\n\n\n676\n9234\n1\n\n\n677\n19988\n1\n\n\n678\n2693\n1\n\n\n679\nPC 17612\n1\n\n\n680\n370376\n1\n\n\n\n\n681 rows × 2 columns\n\n\n\n\nNow we can join this back to our main dataframe.\n\ndata = pd.merge(\n    left=data, # Original Dataframe\n    right=ticket_counts_df, # Count Dataframe\n    left_on=\"Ticket\", # Common column from original dataframe\n    right_on=\"index\", # Common column from count dataframe\n    how=\"left\" # Keep all rows from the original dataframe\n).drop(columns='index') # Get rid of the extra column\n\ndata.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\nTitle\nDeck\nFamilySize\nNumPeopleTicketShared\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n1\nRegular\nNaN\n1\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n1\nRegular\nC\n1\n1\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n0\nUnmarried Woman\nNaN\n0\n1\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n1\nRegular\nC\n1\n2\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n0\nRegular\nNaN\n0\n1\n\n\n\n\n\n\n\n\nNow we can create a ‘fare per person’ column by dividing the fare by the number of people it’s shared between.\n\ndata['FarePerPerson'] = data['Fare'] / data['NumPeopleTicketShared']\ndata\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nUnder18\nTravellingWithFamily\nTitle\nDeck\nFamilySize\nNumPeopleTicketShared\nFarePerPerson\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n1\nRegular\nNaN\n1\n1\n7.2500\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n0\n1\nRegular\nC\n1\n1\n71.2833\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n0\nUnmarried Woman\nNaN\n0\n1\n7.9250\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n0\n1\nRegular\nC\n1\n2\n26.5500\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n0\nRegular\nNaN\n0\n1\n8.0500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n0\n0\nUpper Class\nNaN\n0\n1\n13.0000\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n0\n0\nUnmarried Woman\nB\n0\n1\n30.0000\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n0\n1\nUnknown\nNaN\n3\n2\n11.7250\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n0\n0\nRegular\nC\n0\n1\n30.0000\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n0\n0\nRegular\nNaN\n0\n1\n7.7500\n\n\n\n\n891 rows × 19 columns",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html#indicating-missing-values",
    "href": "4j_feature_engineering.html#indicating-missing-values",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "26.6 Indicating Missing Values",
    "text": "26.6 Indicating Missing Values\nWe will be one-hot encoding the ‘embarked’ column. It can be helpful to provide an extra column to indcate when the relevant column contains a missing value.\n\ndata['Embarked_Missing'] = data['Embarked'].isna().astype('int')\ndata['Embarked_Missing'].value_counts()\n\n0    889\n1      2\nName: Embarked_Missing, dtype: int64\n\n\nIn a later step, we are going to impute values where existing values are missing.\nIt can help to have features indicating that a value was missing and replaced.\n\ndata['Age_Imputed'] = data['Age'].isna().astype('int')\ndata['Age_Imputed'].value_counts()\n\n0    714\n1    177\nName: Age_Imputed, dtype: int64\n\n\nLet’s remind ourselves what the dataframe looks like at this stage.\n\ndata\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\n...\nEmbarked\nUnder18\nTravellingWithFamily\nTitle\nDeck\nFamilySize\nNumPeopleTicketShared\nFarePerPerson\nEmbarked_Missing\nAge_Imputed\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\n...\nS\n0\n1\nRegular\nNaN\n1\n1\n7.2500\n0\n0\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\n...\nC\n0\n1\nRegular\nC\n1\n1\n71.2833\n0\n0\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n...\nS\n0\n0\nUnmarried Woman\nNaN\n0\n1\n7.9250\n0\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\n...\nS\n0\n1\nRegular\nC\n1\n2\n26.5500\n0\n0\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\n...\nS\n0\n0\nRegular\nNaN\n0\n1\n8.0500\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\n...\nS\n0\n0\nUpper Class\nNaN\n0\n1\n13.0000\n0\n0\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\n...\nS\n0\n0\nUnmarried Woman\nB\n0\n1\n30.0000\n0\n0\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\n...\nS\n0\n1\nUnknown\nNaN\n3\n2\n11.7250\n0\n1\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\n...\nC\n0\n0\nRegular\nC\n0\n1\n30.0000\n0\n0\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\n...\nQ\n0\n0\nRegular\nNaN\n0\n1\n7.7500\n0\n0\n\n\n\n\n891 rows × 21 columns",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_engineering.html#performance-with-engineered-features",
    "href": "4j_feature_engineering.html#performance-with-engineered-features",
    "title": "26  Feature Engineering (Titanic Dataset)",
    "section": "27.1 Performance with engineered features",
    "text": "27.1 Performance with engineered features\n\nafter_engineering = fit_assess(\n    X_train=X_train,\n    X_validate=X_validate,\n    name=\"After Feature Engineering\"\n)\n\nresults_df = pd.concat([\n    results_df,\n    after_engineering[0]\n])\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nWithout Feature Engineering\n0.969\n0.804\n0.799\n0.795\n0.795\n0.797\n16\n19\n\n\nBefore Feature Engineering - subset of columns\n0.963\n0.810\n0.806\n0.800\n0.800\n0.803\n15\n19\n\n\nAfter Feature Engineering\n0.975\n0.782\n0.776\n0.774\n0.774\n0.775\n19\n20\n\n\n\n\n\n\n\n\n\nresult_pfi = permutation_importance(\n    after_engineering[1], X_validate, y_validate,\n    n_repeats=10, random_state=42, n_jobs=2\n)\n\nfeature_importances_pfi = pd.Series(result_pfi.importances_mean, index=X_train.columns.tolist())\n\nfig, ax = plt.subplots(figsize=(15,10))\nfeature_importances_pfi.plot.barh(yerr=result_pfi.importances_std, ax=ax)\nax.set_title(\"Feature importances using permutation on full model\")\nax.set_xlabel(\"Mean accuracy decrease\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLet’s rerun with just some of the features that are the most impactful.\n\nselected_columns = ['Pclass', 'IsMale', 'Age', 'TravellingWithFamily', 'NumPeopleTicketShared', 'FarePerPerson']\n\nafter_engineering_2 = fit_assess(\n    X_train=X_train[selected_columns],\n    X_validate=X_validate[selected_columns],\n    name=\"After Feature Engineering - subset of columns\"\n)\n\nresults_df = pd.concat(\n    [results_df,\n     after_engineering_2[0]\n     ]\n)\n\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nWithout Feature Engineering\n0.969\n0.804\n0.799\n0.795\n0.795\n0.797\n16\n19\n\n\nBefore Feature Engineering - subset of columns\n0.963\n0.810\n0.806\n0.800\n0.800\n0.803\n15\n19\n\n\nAfter Feature Engineering\n0.975\n0.782\n0.776\n0.774\n0.774\n0.775\n19\n20\n\n\nAfter Feature Engineering - subset of columns\n0.969\n0.821\n0.816\n0.816\n0.816\n0.816\n16\n16\n\n\n\n\n\n\n\n\nWhile minor in this case - and potentially further influenced by the number of features we have chosen and the method by which we have selected them - feature engineering can have a positive impact on your models, and this notebook has hopefully given you an insight into different strategies for creating additional columns in pandas.",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Feature Engineering (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_selection.html",
    "href": "4j_feature_selection.html",
    "title": "27  Feature Selection Methods for Machine Learning (Titanic Dataset)",
    "section": "",
    "text": "27.1 Useful Function: Model evaluation\nLet’s create a function that can take a model as the input and\nWe also have created an additional parameter that will be used to store the runtime for different instances; some of these might be quite long as feature selection can take a while!\ndef fit_train(X_train, X_validate, y_train, y_validate,\n              name,\n              feature_selection_runtime=\"N/A\",\n              model=XGBClassifier(random_state=42),\n              show_confusion_matrix=False\n              ):\n\n     model.fit(X_train, y_train)\n\n     y_pred_train = model.predict(X_train)\n     y_pred_val = model.predict(X_validate)\n\n     if show_confusion_matrix:\n          confusion_matrix_titanic = ConfusionMatrixDisplay(\n               confusion_matrix=confusion_matrix(\n                    y_true=y_validate,\n                    y_pred=y_pred_val\n                    ),\n                    display_labels=[\"Died\", \"Survived\"]\n               )\n\n          confusion_matrix_titanic.plot()\n\n     return pd.DataFrame({\n          'Accuracy (training)': np.mean(y_pred_train == y_train).round(4),\n          'Accuracy (validation)': np.mean(y_pred_val == y_validate).round(4),\n          'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro').round(4),\n          'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro').round(4),\n          'features': \", \".join(X_train.columns.tolist()),\n          'feature_selection_runtime': feature_selection_runtime\n\n          }, index=[name]\n)\nLet’s first use this to create a dataframe of results that just contains the results from running an xgboost model with all available features in use.\nexperiment_results_df = fit_train(X_train=X_train,\n          X_validate=X_validate,\n          y_train=y_train,\n          y_validate=y_validate,\n          name=f\"XGBoost - all features ({len(X.columns)})\")\n\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Feature Selection Methods for Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_selection.html#forward-feature-selection",
    "href": "4j_feature_selection.html#forward-feature-selection",
    "title": "27  Feature Selection Methods for Machine Learning (Titanic Dataset)",
    "section": "27.2 Forward Feature Selection",
    "text": "27.2 Forward Feature Selection\nForward feature selection starts by finding which single feature produces the best model.\nIt then iteratively goes through and adds additional features, in each case keeping the feature that adds the most predictive power.\nWe specify the model first, then record the time feature selection began.\nWe then create a SequentialFeatureSelector, passing in the model, how many features we want to end up with, and whether to do ‘forward’ or ‘backward’ selection.\nWe then run fit on this, and when this has completed, it will record the duration.\n\nmodel = XGBClassifier(random_state=42)\n\nstart_time = time()\n\nsfs_forward = SequentialFeatureSelector(\n    model, n_features_to_select=3, direction=\"forward\"\n)\n\nsfs_forward.fit(X_train, y_train)\n\nduration = time() - start_time\n\nsfs_forward\n\nSequentialFeatureSelector(estimator=XGBClassifier(base_score=None, booster=None,\n                                                  callbacks=None,\n                                                  colsample_bylevel=None,\n                                                  colsample_bynode=None,\n                                                  colsample_bytree=None,\n                                                  device=None,\n                                                  early_stopping_rounds=None,\n                                                  enable_categorical=False,\n                                                  eval_metric=None,\n                                                  feature_types=None,\n                                                  gamma=None, grow_policy=None,\n                                                  importance_type=None,\n                                                  interaction_constraints=None,\n                                                  learning_rate=None,\n                                                  max_bin=None,\n                                                  max_cat_threshold=None,\n                                                  max_cat_to_onehot=None,\n                                                  max_delta_step=None,\n                                                  max_depth=None,\n                                                  max_leaves=None,\n                                                  min_child_weight=None,\n                                                  missing=nan,\n                                                  monotone_constraints=None,\n                                                  multi_strategy=None,\n                                                  n_estimators=None,\n                                                  n_jobs=None,\n                                                  num_parallel_tree=None,\n                                                  random_state=42, ...),\n                          n_features_to_select=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SequentialFeatureSelector?Documentation for SequentialFeatureSelectoriFittedSequentialFeatureSelector(estimator=XGBClassifier(base_score=None, booster=None,\n                                                  callbacks=None,\n                                                  colsample_bylevel=None,\n                                                  colsample_bynode=None,\n                                                  colsample_bytree=None,\n                                                  device=None,\n                                                  early_stopping_rounds=None,\n                                                  enable_categorical=False,\n                                                  eval_metric=None,\n                                                  feature_types=None,\n                                                  gamma=None, grow_policy=None,\n                                                  importance_type=None,\n                                                  interaction_constraints=None,\n                                                  learning_rate=None,\n                                                  max_bin=None,\n                                                  max_cat_threshold=None,\n                                                  max_cat_to_onehot=None,\n                                                  max_delta_step=None,\n                                                  max_depth=None,\n                                                  max_leaves=None,\n                                                  min_child_weight=None,\n                                                  missing=nan,\n                                                  monotone_constraints=None,\n                                                  multi_strategy=None,\n                                                  n_estimators=None,\n                                                  n_jobs=None,\n                                                  num_parallel_tree=None,\n                                                  random_state=42, ...),\n                          n_features_to_select=3) estimator: XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...) XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...) \n\n\nLet’s explore what the output of this is.\n\nsfs_forward.get_support()\n\narray([False, False, False, False,  True, False, False, False,  True,\n       False,  True, False, False, False, False, False, False, False,\n       False, False, False, False, False, False])\n\n\nIt looks like it’s a MASK. This list of true and false values could be applied to an array so that we only keep the values in the array that match up with an instance of ‘true’ in the mask.\nLet’s use the mask to get the actual feature names out instead and print these.\n\nfeature_names_selected_ff = np.array(feature_names)[sfs_forward.get_support()]\n\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{feature_names_selected_ff}\"\n)\n\nFeatures selected by forward sequential selection: ['Fare' 'CabinNumber' 'male']\n\n\nLet’s now assess the performance of a model trained using just these features and als pass in the duration of the feature selection step (calculated two cells previously).\nWe pass the feature names we have obtained to the training and validation datasets to filter them down to just these cells.\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_ff],\n          X_validate=X_validate[feature_names_selected_ff],\n          y_train=y_train,\n          y_validate=y_validate,\n          feature_selection_runtime=f\"{duration:.3f}s\",\n          name=\"Forward Feature Selection - 3\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\n\n\n\n\n\n\n\n27.2.0.1 Repeat with 5 features\nLet’s do it again, seeing how much better 5 features perform and how long it takes.\n\nmodel = XGBClassifier(random_state=42)\n\nstart_time = time()\n\nsfs_forward_5 = SequentialFeatureSelector(\n    model, n_features_to_select=5, direction=\"forward\"\n)\n\nsfs_forward_5.fit(X_train, y_train)\n\nduration = time() - start_time\n\nfeature_names_selected_ff_5 = np.array(feature_names)[sfs_forward_5.get_support()]\n\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{feature_names_selected_ff}\"\n)\n\nFeatures selected by forward sequential selection: ['Fare' 'CabinNumber' 'male']\n\n\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_ff_5],\n          X_validate=X_validate[feature_names_selected_ff_5],\n          y_train=y_train,\n          y_validate=y_validate,\n          feature_selection_runtime=f\"{duration:.3f}s\",\n          name=\"Forward Feature Selection - 5\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Feature Selection Methods for Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_selection.html#backward-feature-selection",
    "href": "4j_feature_selection.html#backward-feature-selection",
    "title": "27  Feature Selection Methods for Machine Learning (Titanic Dataset)",
    "section": "27.3 Backward Feature Selection",
    "text": "27.3 Backward Feature Selection\nThe code is almost identical when we want to perform backward selection - we just use direction=\"backward\" instead.\n\nmodel = XGBClassifier(random_state=42)\n\nstart_time = time()\n\nsfs_backward_3 = SequentialFeatureSelector(\n    model, n_features_to_select=3, direction=\"backward\"\n)\n\nsfs_backward_3.fit(X_train, y_train)\n\nduration = time() - start_time\n\nfeature_names_selected_bf_3 = np.array(feature_names)[sfs_backward_3.get_support()]\n\nprint(\n    \"Features selected by backward sequential selection: \"\n    f\"{feature_names_selected_bf_3}\"\n)\n\nFeatures selected by backward sequential selection: ['Fare' 'CabinNumber' 'male']\n\n\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_bf_3],\n          X_validate=X_validate[feature_names_selected_bf_3],\n          y_train=y_train,\n          y_validate=y_validate,\n          feature_selection_runtime=f\"{duration:.3f}s\",\n          name=\"Backward Feature Selection - 3\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s\n\n\nBackward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n268.981s\n\n\n\n\n\n\n\n\nLet’s repeat with 5 and assess the performance.\n\nmodel = XGBClassifier(random_state=42)\n\nstart_time = time()\n\nsfs_backward_5 = SequentialFeatureSelector(\n    model, n_features_to_select=5, direction=\"backward\"\n)\n\nsfs_backward_5.fit(X_train, y_train)\n\nduration = time() - start_time\n\nfeature_names_selected_bf_5 = np.array(feature_names)[sfs_backward_5.get_support()]\n\nprint(\n    \"Features selected by backward sequential selection: \"\n    f\"{feature_names_selected_bf_5}\"\n)\n\nFeatures selected by backward sequential selection: ['Pclass' 'Fare' 'CabinNumber' 'male' 'CabinLetter_E']\n\n\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_bf_5],\n          X_validate=X_validate[feature_names_selected_bf_5],\n          y_train=y_train,\n          y_validate=y_validate,\n          feature_selection_runtime=f\"{duration:.3f}s\",\n          name=\"Backward Feature Selection - 5\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s\n\n\nBackward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n268.981s\n\n\nBackward Feature Selection - 5\n0.9262\n0.8042\n0.7945\n0.7945\nPclass, Fare, CabinNumber, male, CabinLetter_E\n179.093s\n\n\n\n\n\n\n\n\n\n27.3.1 The ‘auto’ parameter\nWe can also let SequentialFeatureSelector decide how many features to use.\n\nmodel = XGBClassifier(random_state=42)\n\nstart_time = time()\n\nsfs_backward_auto = SequentialFeatureSelector(\n    model,\n    n_features_to_select=\"auto\",\n    direction=\"backward\"\n)\n\nsfs_backward_auto.fit(X_train, y_train)\n\nduration = time() - start_time\n\nfeature_names_selected_bf_auto = np.array(feature_names)[sfs_backward_auto.get_support()]\n\nprint(\n    \"Features selected by backward sequential selection: \"\n    f\"{feature_names_selected_bf_auto}\"\n)\n\nFeatures selected by backward sequential selection: ['Pclass' 'SibSp' 'Parch' 'Fare' 'CabinNumber' 'male' 'Embarked_C'\n 'Embarked_Q' 'Embarked_S' 'CabinLetter_D' 'CabinLetter_E'\n 'CabinLetter_missing']\n\n\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_bf_auto],\n          X_validate=X_validate[feature_names_selected_bf_auto],\n          y_train=y_train,\n          y_validate=y_validate,\n          feature_selection_runtime=f\"{duration:.3f}s\",\n          name=\"Backward Feature Selection - auto\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s\n\n\nBackward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n268.981s\n\n\nBackward Feature Selection - 5\n0.9262\n0.8042\n0.7945\n0.7945\nPclass, Fare, CabinNumber, male, CabinLetter_E\n179.093s\n\n\nBackward Feature Selection - auto\n0.9385\n0.8042\n0.7942\n0.7977\nPclass, SibSp, Parch, Fare, CabinNumber, male,...\n74.881s",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Feature Selection Methods for Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_feature_selection.html#using-feature-importance-for-feature-selection",
    "href": "4j_feature_selection.html#using-feature-importance-for-feature-selection",
    "title": "27  Feature Selection Methods for Machine Learning (Titanic Dataset)",
    "section": "27.4 Using Feature Importance for Feature Selection",
    "text": "27.4 Using Feature Importance for Feature Selection\nWe could also use some of the feature importance metrics we’ve explored in previous sessions to make a judgment of which features to keep in our model.\nThe SelectFromModel function works with any model that has a featur_importances_ attribute.\ne.g.\nFor logistic regression, this will be model coefficients.\nFor decision trees, this will be mean decrease in impurity.\nWe can determine the threshold we are going to use for keeping features in the model.\n\nstart_time = time()\n\nmodel = XGBClassifier(random_state=42)\n\nselector = SelectFromModel(\n    estimator=model,\n    threshold=0.03\n    )\n\nselector.fit(X_train, y_train)\n\nduration = time() - start_time\n\n\nselector.estimator_.feature_importances_\n\narray([0.15078323, 0.03309093, 0.04509973, 0.03712323, 0.03330522,\n       0.01645992, 0.        , 0.02637349, 0.0527233 , 0.        ,\n       0.38024062, 0.03597496, 0.02359743, 0.03333741, 0.        ,\n       0.        , 0.        , 0.02592567, 0.04060741, 0.06535744,\n       0.        , 0.        , 0.        , 0.        ], dtype=float32)\n\n\n\nselector.threshold_\n\n0.03\n\n\nSimilarly to before, we can use ‘get_support’.\n\nselector.get_support()\n\narray([ True,  True,  True,  True,  True, False, False, False,  True,\n       False,  True,  True, False,  True, False, False, False, False,\n        True,  True, False, False, False, False])\n\n\n\nfeature_names_selected_fi_03 = np.array(feature_names)[selector.get_support()]\nfeature_names_selected_fi_03\n\narray(['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'CabinNumber', 'male',\n       'Embarked_C', 'Embarked_S', 'CabinLetter_D', 'CabinLetter_E'],\n      dtype='&lt;U19')\n\n\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_fi_03],\n          X_validate=X_validate[feature_names_selected_fi_03],\n          y_train=y_train,\n          y_validate=y_validate,\n          feature_selection_runtime=f\"{duration:.3f}s\",\n          name=\"Feature Importance Selection - Threshold 0.03\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s\n\n\nBackward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n268.981s\n\n\nBackward Feature Selection - 5\n0.9262\n0.8042\n0.7945\n0.7945\nPclass, Fare, CabinNumber, male, CabinLetter_E\n179.093s\n\n\nBackward Feature Selection - auto\n0.9385\n0.8042\n0.7942\n0.7977\nPclass, SibSp, Parch, Fare, CabinNumber, male,...\n74.881s\n\n\nFeature Importance Selection - Threshold 0.03\n0.9807\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, CabinNumber, ...\n0.193s\n\n\n\n\n\n\n\n\nLet’s try tweaking with a different model.\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train,\n          X_validate=X_validate,\n          y_train=y_train,\n          y_validate=y_validate,\n          model=RandomForestClassifier(random_state=42, max_depth=6),\n          name=f\"Random Forest - all features ({len(X_train.columns)})\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s\n\n\nBackward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n268.981s\n\n\nBackward Feature Selection - 5\n0.9262\n0.8042\n0.7945\n0.7945\nPclass, Fare, CabinNumber, male, CabinLetter_E\n179.093s\n\n\nBackward Feature Selection - auto\n0.9385\n0.8042\n0.7942\n0.7977\nPclass, SibSp, Parch, Fare, CabinNumber, male,...\n74.881s\n\n\nFeature Importance Selection - Threshold 0.03\n0.9807\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, CabinNumber, ...\n0.193s\n\n\nRandom Forest - all features (24)\n0.8822\n0.7972\n0.7870\n0.7888\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\n\n\n\n\n\n\n\nselector = SelectFromModel(estimator=RandomForestClassifier(random_state=42, max_depth=6), threshold=\"mean\").fit(X_train, y_train)\nselector.estimator_.feature_importances_\n\narray([6.66890927e-02, 1.03772923e-01, 4.78576994e-02, 3.96696333e-02,\n       1.29373457e-01, 1.25008170e-02, 1.55485193e-04, 3.27266742e-02,\n       8.05041446e-02, 4.58115404e-02, 3.09684642e-01, 1.31952009e-02,\n       7.36228867e-03, 2.11471973e-02, 9.83537258e-06, 2.28944280e-03,\n       1.19636370e-02, 1.08809761e-02, 5.33431862e-03, 1.15495213e-02,\n       2.40979415e-03, 8.87648714e-04, 0.00000000e+00, 4.42240310e-02])\n\n\n\nfeature_names_selected_fi_rf_mean = np.array(feature_names)[selector.get_support()]\nfeature_names_selected_fi_rf_mean\n\nexperiment_results_df = pd.concat([experiment_results_df,\n    fit_train(X_train=X_train[feature_names_selected_fi_rf_mean],\n          X_validate=X_validate[feature_names_selected_fi_rf_mean],\n          y_train=y_train,\n          y_validate=y_validate,\n          model=RandomForestClassifier(random_state=42, max_depth=6),\n          name=f\"Random Forest - Mean Feature Importance Threshold ({len(feature_names_selected_fi_rf_mean)})\")]\n)\n\nexperiment_results_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nfeatures\nfeature_selection_runtime\n\n\n\n\nXGBoost - all features (24)\n0.9789\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nForward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n203.124s\n\n\nForward Feature Selection - 5\n0.9262\n0.8042\n0.7967\n0.7882\nFare, CabinLetterImputed, CabinNumber, male, C...\n106.305s\n\n\nBackward Feature Selection - 3\n0.9244\n0.8112\n0.8052\n0.7939\nFare, CabinNumber, male\n268.981s\n\n\nBackward Feature Selection - 5\n0.9262\n0.8042\n0.7945\n0.7945\nPclass, Fare, CabinNumber, male, CabinLetter_E\n179.093s\n\n\nBackward Feature Selection - auto\n0.9385\n0.8042\n0.7942\n0.7977\nPclass, SibSp, Parch, Fare, CabinNumber, male,...\n74.881s\n\n\nFeature Importance Selection - Threshold 0.03\n0.9807\n0.7972\n0.7875\n0.7856\nPclass, Age, SibSp, Parch, Fare, CabinNumber, ...\n0.193s\n\n\nRandom Forest - all features (24)\n0.8822\n0.7972\n0.7870\n0.7888\nPclass, Age, SibSp, Parch, Fare, AgeImputed, E...\nN/A\n\n\nRandom Forest - Mean Feature Importance Threshold (8)\n0.8946\n0.8322\n0.8239\n0.8239\nPclass, Age, SibSp, Fare, CabinNumber, CabinNu...\nN/A",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Feature Selection Methods for Machine Learning (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_hyperparameter_optimisation.html",
    "href": "4j_hyperparameter_optimisation.html",
    "title": "28  Hyperparameter Optimisation (Titanic Dataset)",
    "section": "",
    "text": "Hyperparameter optimisation with grid search or frameworks like Optuna can be more efficient than manually trying out combinations of hyperparameters.\n\nimport pandas as pd\nimport numpy as np\nfrom time import time\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score, precision_score, \\\n                            recall_score\n\n# Imports for hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\nimport optuna\nfrom optuna.visualization import plot_optimization_history, \\\n                                plot_param_importances, plot_rank, plot_slice\n\n# Import scipy random integer function and give it an alias so we don't overwrite\n# other functions\nfrom scipy.stats import randint as sp_randint\n\nFirst let’s import the processed titanic dataset and split it into training and testing datasets.\nWe will be using cross-validated models so rather than separating out a validation dataset, it will be taken care of when we pass in the training dataset.\n\ntry:\n    data = pd.read_csv(\"data/processed_data.csv\")\n\nexcept FileNotFoundError:\n    # Download processed data:\n    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n\n    data = pd.read_csv(address)\n\n    # Create a data subfolder if one does not already exist\n    import os\n    data_directory ='./data/'\n    if not os.path.exists(data_directory):\n        os.makedirs(data_directory)\n\n    # Save data\n    data.to_csv(data_directory + 'processed_data.csv', index=False)\n\ndata = data.astype(float)\n\n# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n# We drop passenger ID as it is not original data\n\ndata.drop('PassengerId', inplace=True, axis=1)\n\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'\n\nfeature_names = X.columns.tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training Dataset Samples: {len(X_train)}\")\nprint(f\"Testing Dataset Samples: {len(X_test)}\")\n\nTraining Dataset Samples: 712\nTesting Dataset Samples: 179\n\n\n\n29 Exhaustive\nLet’s begin by doing an exhaustive search - evaluating every possible combination of parameters - using GridSearchCV.\nWe create a dictionary containing the parameter names on the left - these must match how they are referred to in the model we are working with, which is a decision tree in this case - and the possible values on the right.\nWe can pass in a list of possible options, or generate a range object and pass that in.\nRemember that the ‘up to’ parameter of a range isn’t inclusive. The third parameter is the step/gap between numbers.\nVerbose controls the level of output - higher numbers will give more detail in the cell output.\n\nparams = [\n    {'criterion': ['gini', 'entropy', 'log_loss'],\n     'max_depth': range(1, 16, 1),\n     'min_samples_split': range(2, 16, 1),\n     'min_samples_leaf': range(1, 16, 1)\n    }]\n\n\ngridsearch_dt = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                      param_grid=params,\n                      scoring='accuracy',\n                      cv=5,\n                      verbose=2)\n\nWe then fit our grid search object and return the best parameters.\nWe can also calculate the score.\n\nstart_time = time()\n\ngridsearch_dt.fit(X_train, y_train)\n\nend_time = time()\n\n\nprint(f\"Time taken: {(end_time - start_time):.3f}s\")\n\nprint(f\"Best parameters{gridsearch_dt.best_params_}\")\n\nprint(f\"Training Set Score: {gridsearch_dt.score(X_train, y_train)}\")\n\nprint(f\"Test Set Score: {gridsearch_dt.score(X_test, y_test)}\")\n\nTime taken: 515.106s\nBest parameters{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 9}\nTraining Set Score: 0.8904494382022472\nTest Set Score: 0.7988826815642458\n\n\n\n\n30 Randomised\nRandomised grid search will instead do as many different combinations as we specify with the ‘n_iter’ parameter.\nRemember that there will be cross-validations - so with 5-fold cross validations, there will actually be 5 * 500 different model fits: 2500 model fits!\nNote that we have to specify the parameter grid slightly differently when using randomised search. Scipy’s randomint function, which we’ve imported as sp_randint, provides the parameter values in the format RandomizedSearchCV requires.\n\nparams_dist = [\n    {'criterion': ['gini', 'entropy', 'log_loss'],\n     'max_depth': sp_randint(1, 16),\n     'min_samples_split': sp_randint(2, 16),\n     'min_samples_leaf': sp_randint(1, 16)\n    }]\n\nstart_time = time()\n\nrgs_dt = RandomizedSearchCV(\n                      DecisionTreeClassifier(random_state=42),\n                      n_iter=500,\n                      param_distributions=params_dist,\n                      scoring='accuracy',\n                      cv=5)\n\n\n\nrgs_dt.fit(X_train, y_train)\n\nprint(f\"Best parameters{rgs_dt.best_params_}\")\n\nprint(f\"Training Set Score: {rgs_dt.score(X_train, y_train)}\")\n\nprint(f\"Test Set Score: {rgs_dt.score(X_test, y_test)}\")\n\nprint(f\"Time taken: {(time() - start_time):.3f}s\")\n\nBest parameters{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 7}\nTraining Set Score: 0.8974719101123596\nTest Set Score: 0.8268156424581006\nTime taken: 17.146s\n\n\n\n\n31 Optuna\nThe optuna framework more intelligently finds the best combinations of parameters.\nWe need to set this up slightly differently - we need to define an objective function that gets passed the parameter trial.\nWithin this, we set up our parameter values; the first argument is the actual name of the parameter in the relevant model.\nThen we set up the model, and ensure that the return value from the objective function will be a numeric value representing a score - here we’ve chosen average accuracy over 3 cross-validated folds.\n\ndef objective(trial):\n\n    # Set Optuna trial parameters and ranges\n    rf_criterion = trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss'])\n    rf_max_depth = trial.suggest_int('max_depth', 1, 32, log=True)\n    rf_min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 32, log=True)\n    rf_min_samples_split = trial.suggest_int('min_samples_split', 2, 32, log=True)\n\n    # Set up model\n    model = DecisionTreeClassifier(\n       criterion=rf_criterion,\n       max_depth=rf_max_depth,\n       min_samples_leaf=rf_min_samples_leaf,\n       min_samples_split=rf_min_samples_split,\n       random_state=42\n       )\n\n    # Assess accuracy with sklearn.model_selection.cross_val_score\n    accuracy = cross_val_score(\n        model, X_train, y_train, n_jobs=-1, cv=3).mean()\n\n    return accuracy\n\nWe then want to set up a study using the optuna_create_study() function.\n\nstart_time = time()\n\n# Set up Optuna study - we need to specifiy that we wish to maximise objective\nstudy = optuna.create_study(direction='maximize')\n\n# Run optimisation\nstudy.optimize(objective, n_trials=1000)\n\n# Get best model run\ntrial = study.best_trial\n\nprint(f'Accuracy: {trial.value:0.3f}')\nprint(f'Best hyperparameters: {trial.params}')\nprint(f\"Time taken: {(time() - start_time):.3f}s\")\n\nFinally, we can explore the plots that Optuna provides.\nThe optimization history shows how the best value - in this case, accuracy - increased as additional ‘trials’ (parameter combinations) were tried. Each dot is the result from a single trial.\n\nplot_optimization_history(study)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nThe slice plot shows the range of values achieved with different values, and where most of the search time was spent.\n\nplot_slice(study)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nFinally, we can get some sense of how important different hyperparameters were for the final output.\n\nplot_param_importances(study)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nWe can also look at different importances - like how much impact on the model fit time different parameters had.\n\noptuna.visualization.plot_param_importances(\n    study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nThe rank plot is another option available but isn’t the easiest to interpret!\n\nplot_rank(study)\n\nC:\\Users\\Sammi\\AppData\\Local\\Temp\\ipykernel_45892\\3430271029.py:1: ExperimentalWarning:\n\nplot_rank is experimental (supported from v3.2.0). The interface can change in the future.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Hyperparameter Optimisation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_imbalanced_data.html",
    "href": "4j_imbalanced_data.html",
    "title": "29  Imbalanced Data (Titanic Dataset)",
    "section": "",
    "text": "29.1 In-model options\nCertain models have options to allow us to account for imbalanced data automatically.\nresults_df = fit_train(\"Standard\", model=XGBClassifier())\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nStandard\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\nmodel = XGBClassifier(\n    random_state=42,\n    scale_pos_weight=number_negative_class/number_positive_class\n    )\nresults_df = pd.concat([results_df,\n    fit_train(\"With Imbalanced Data Parameter\", model=model)]\n)\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nStandard\n0.979\n0.797\n0.788\n0.786\n0.786\n0.787\n14\n15\n\n\nWith Imbalanced Data Parameter\n0.988\n0.811\n0.802\n0.800\n0.800\n0.801\n13\n14",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Imbalanced Data (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_imbalanced_data.html#in-model-options",
    "href": "4j_imbalanced_data.html#in-model-options",
    "title": "29  Imbalanced Data (Titanic Dataset)",
    "section": "",
    "text": "29.1.1 Logistic Regression\n\ndef standardise_data(X_train, X_test):\n\n    # Initialise a new scaling object for normalising input data\n    sc = StandardScaler()\n\n    # Set up the scaler just on the training set\n    sc.fit(X_train)\n\n    # Apply the scaler to the training and test sets\n    train_std=sc.transform(X_train)\n    test_std=sc.transform(X_test)\n\n    return train_std, test_std\n\n\nX_train_std, X_test_std = standardise_data(X_train, X_test)\n\n\nresults_df = fit_train(\"Logistic Regression\", model=LogisticRegression(),\n                       X_train=X_train_std,\n                       X_validate=X_test_std,\n                       y_train=y_train,\n                       y_validate=y_test)\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nLogistic Regression\n0.8\n0.832\n0.827\n0.827\n0.827\n0.827\n15\n15\n\n\n\n\n\n\n\n\nAccording to the documentation, “The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).”\nWhat will this be in our case?\n\nlen(X_train_std) / (2 * np.bincount(y_train))\n\narray([0.79691877, 1.34198113])\n\n\n\nmodel_lr = LogisticRegression(\n    class_weight=\"balanced\"\n)\n\nresults_df = pd.concat([results_df,\n    fit_train(\"With Imbalanced Data Parameter\", model=model_lr,\n                       X_train=X_train_std,\n                       X_validate=X_test_std,\n                       y_train=y_train,\n                       y_validate=y_test)\n\n    ]\n)\nresults_df\n\n\n\n\n\n\n\n\n\nAccuracy (training)\nAccuracy (validation)\nPrecision (validation)\nRecall (validation)\nAUC\nf1\nFP\nFN\n\n\n\n\nLogistic Regression\n0.800\n0.832\n0.827\n0.827\n0.827\n0.827\n15\n15\n\n\nWith Imbalanced Data Parameter\n0.803\n0.804\n0.800\n0.807\n0.807\n0.801\n22\n13\n\n\n\n\n\n\n\n\nIn our case, this doesn’t seem to have helped - but it’s good to know and interesting to see the strong impact on false positives and false negatives. If we were interested in maximising some aspect of our model, the slight loss in overall performance may feel worthwhile to us.\n\n\n29.1.2 Upsampling using synthetic data generators like SMOTE\n\ndef make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n    \"\"\"\n    Synthetic data generation for two classes.\n\n    Inputs\n    ------\n    original_data: X, y numpy arrays (y should have label 0 and 1)\n    number_of_samples: number of samples to generate (list for y=0, y=1)\n    (Note - number_of_samples has default of 1000 samples for each class\n    if no numbers are specified at the point of calling the function)\n\n    Returns\n    -------\n    X_synthetic: NumPy array\n    y_synthetic: NumPy array\n\n    \"\"\"\n\n    # Count instances in each class\n    count_label_0 = np.sum(y==0)\n    count_label_1 = np.sum(y==1)\n\n    # SMOTE requires final class counts; add current counts to required counts\n    # (which are passed into the function)\n    n_class_0 = number_of_samples[0] + count_label_0\n    n_class_1 = number_of_samples[1] + count_label_1\n\n    # Use SMOTE to sample data points.  The number of points that we pass over\n    # to SMOTE is calculated above (the number of synthetic data samples we\n    # want, which we passed into the function + the counts from the original\n    # data).  This tells SMOTE how many TOTAL data points are needed (original\n    # + synthetic) for each class.  It then uses the original data to generate\n    # new synthetic data points.\n    # For example, imagine our original data has 100 samples for class 0 and 50\n    # for class 1, and we tell SMOTE we want 100 synthetic data points for\n    # class 0 and 150 synthetic data points for class 1.  We tell SMOTE that we\n    # need a total of 200 data points for class 0 (100 original + 100 synthetic)\n    # and 200 data points for class 1 (50 original + 150 synthetic).  It will\n    # then fill those data points by taking the original data (which will fill\n    # up the first 100 \"slots\" for class 0, and the first 50 \"slots\" for class 1)\n    # and then use these original data points to sample new synthetic data points\n    # to fill the remaining \"slots\" in each class.\n    X_resampled, y_resampled = SMOTE(\n        sampling_strategy = {0:n_class_0, 1:n_class_1}).fit_resample(X, y)\n\n    # Get just the additional (synthetic) data points.  By using len(X) for the\n    # X (input feature) data, and len(y) for the y (output label) data, we skip\n    # the original data, and just start from the newly created synthetic data,\n    # generated by SMOTE (above)\n    X_synthetic = X_resampled[len(X):]\n    y_synthetic = y_resampled[len(y):]\n\n    return X_synthetic, y_synthetic\n\n\nX.head()\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n0\n3.0\n22.0\n1.0\n0.0\n7.2500\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n38.0\n1.0\n0.0\n71.2833\n0.0\n0.0\n0.0\n85.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n3.0\n26.0\n0.0\n0.0\n7.9250\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n1.0\n35.0\n1.0\n0.0\n53.1000\n0.0\n0.0\n0.0\n123.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n3.0\n35.0\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\n# Get full list of column names (the names of our features)\nX_col_names = X.columns.tolist()\n\n# Set categorical one-hots cols using common prefix\ncategorical = ['CabinLetter_', 'Embarked_']\n\none_hot_cols = []\nfor col in categorical:\n    one_hot_cols.append([x for x in X_col_names if x[0:len(col)] == col])\n\n# Set integer columns\ninteger_cols = ['Age', 'Pclass']\n\n# Don't need to explicitly set float cols\n\n# Set binary columns\nbinary_cols = ['SibSp', 'Parch', 'AgeImputed', 'EmbarkedImputed']\n\n\n# Generate synthetic data again, but this time with 250 extra synthetic data\n# points for the positive class (double what we need), and 0 for the negative\n# class\nX_synthetic, y_synthetic = make_synthetic_data_smote(\n    X_train, y_train, number_of_samples=[0, 350]\n)\n\n# Set y_label\ny_label = \"Survived\"\n\n# Create a data frame with id to store the synthetic data\nsynth_df = pd.DataFrame()\n\n# Transfer X values to the new DataFrame\nsynth_df=pd.concat([synth_df,\n                    pd.DataFrame(X_synthetic, columns=X.columns.to_list())],\n                    axis=1)\n\n\nsynth_df\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n569\n1.000000\n34.674002\n1.000000\n0.000000\n90.000000\n0.0\n0.0\n0.0\n90.555015\n0.0\n...\n0.0\n0.0\n0.000000\n1.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n\n\n570\n1.000000\n35.938509\n0.000000\n0.000000\n223.817179\n0.0\n0.0\n0.0\n35.837817\n0.0\n...\n0.0\n0.0\n0.229055\n0.770945\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n\n\n571\n1.000000\n55.300072\n0.000000\n0.850004\n153.462500\n0.0\n0.0\n0.0\n125.000000\n0.0\n...\n0.0\n0.0\n0.000000\n1.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n\n\n572\n1.000000\n22.943377\n0.000000\n0.528311\n61.745149\n0.0\n0.0\n0.0\n33.943377\n0.0\n...\n0.0\n0.0\n0.471689\n0.000000\n0.0\n0.528311\n0.0\n0.0\n0.0\n0.0\n\n\n573\n1.025236\n27.050471\n0.025236\n0.000000\n30.386439\n0.0\n0.0\n1.0\n0.000000\n1.0\n...\n0.0\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n914\n2.000000\n25.995775\n1.000000\n0.751056\n29.004225\n0.0\n0.0\n1.0\n0.000000\n1.0\n...\n0.0\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n1.0\n\n\n915\n2.945161\n62.945161\n0.000000\n0.000000\n9.637540\n0.0\n0.0\n1.0\n0.000000\n1.0\n...\n0.0\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n1.0\n\n\n916\n3.000000\n28.000000\n0.000000\n0.000000\n7.750000\n1.0\n0.0\n1.0\n0.000000\n1.0\n...\n0.0\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n1.0\n\n\n917\n1.773067\n33.067333\n0.226933\n0.000000\n22.100026\n0.0\n0.0\n0.0\n105.992533\n0.0\n...\n0.0\n0.0\n0.000000\n0.226933\n0.0\n0.773067\n0.0\n0.0\n0.0\n0.0\n\n\n918\n3.000000\n15.809182\n0.000000\n0.000000\n7.892575\n0.0\n0.0\n1.0\n0.000000\n1.0\n...\n0.0\n0.0\n0.000000\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n350 rows × 24 columns\n\n\n\n\n\n# Make integer as necessary by rounding the raw synthetic data\nfor col in integer_cols:\n    synth_df[col] = synth_df[col].round(0)\n\n# Round binary cols and clip so values under 0 or above 1\n# are set to 0 and 1 respectively (this won't happen with\n# SMOTE, as it will only sample between the two points (so\n# points sampled between binary points will always be\n# between 0 and 1) but it can happen with other methods)\nfor col in binary_cols:\n    synth_df[col] = np.clip(synth_df[col],0,1).round(0)\n\n# Add y data with a label\ny_list = list(y_synthetic)\nsynth_df[y_label] = y_list\n\n# Shuffle data\nsynth_df = synth_df.sample(frac=1.0)\n\n\n# Standardise synthetic data (based on real training data)\nX_train_std, X_synth_std = standardise_data(X_train, X_synthetic)\n\n# Get ALL real X data (combine standardised training + test data)\n# We do this because we need to check for duplicates / very close\n# values in all of the real data we've got\nX_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n\n# Use SciKitLearn neighbors.NearestNeighbors to find nearest neighbour\n# to each data point. First, we fit to the real standardised data\n# (all of it, train + test set).  Then we can give it the synthetic data\n# and ask it to give us the cartesian distance and ID of its nearest\n# real world data point neighbour for each synthetic data point.\nnn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\ndists, idxs = nn.kneighbors(X_synth_std)\n\n# Store the index and ids (indices) in the synthetic data DataFrame\n# Flatten just reduces something in more than 1 dimension down to\n# 1 dimension (eg a list of lists becomes a single list)\nsynth_df['distance_to_closest_real'] = list(dists.flatten())\nsynth_df['closest_X_real_row_index'] = list(idxs.flatten())\n\n\n# Get points with zero distance to real (use distance of &lt;0.001 as effectively identical)\nidentical = synth_df['distance_to_closest_real'] &lt; 0.001\n\nprint (f'Proportion of data points identical to real data points = {identical.mean():0.3f}')\n# Remove points with zero (or effectively zero) distance to a real data point.  We\n# do this by setting up a mask that says we only want to see data points where the \"identical\"\n# criterion we specified above is false (ie they're not identical).  Then we apply that\n# mask and overwrite our existing synthetic data DataFrame so we've now only got data points\n# that are not identical to real world data points.\nmask = identical == False\nsynth_df = synth_df[mask]\n\nProportion of data points identical to real data points = 0.074\n\n\n\n# Proportion of points to remove\nproportion_to_remove = 0.1\n\n# Sort by distance, with highest distances (those we want to keep) at\n# the top\nsynth_by_distance = synth_df.sort_values(\n    'distance_to_closest_real', ascending=False)\n\n# Limit data.  Calculate the number of entries to keep as being the\n# total number of synthetic data points we've now got (after having\n# removed ones identical to real world data points) multiplied by\n# the proportion we want to keep (the inverse of the proportion to remove).\n# As we've sorted in descending order by distance, we can then just\n# use .head to identify how much of the top of list we want to keep\n# (90% in this case, where we're removing the 10% that are closest - at\n# the bottom)\nnumber_to_keep = int(len(synth_by_distance) * (1 - proportion_to_remove))\nsynth_by_distance = synth_by_distance.head(number_to_keep)\n\n# Shuffle and store back in synth_df (frac=1 gives us a sample size of 100%\n# (ie - all of the ones we said above we wanted to keep))\nsynth_df = synth_by_distance.sample(frac=1)\n\n\n# Keep only a random sample of 150 of the remaining synthetic datapoints\n# We don't need a mask here as ALL our synthetic datapoints are for class 1\n# (positive).\nsynth_df = synth_df.sample(150)\nsynth_df\n\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\nCabinNumberImputed\n...\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\nSurvived\ndistance_to_closest_real\nclosest_X_real_row_index\n\n\n\n\n812\n1.0\n40.0\n0.0\n0.0\n79.770103\n0.0\n1.0\n0.000000\n26.938916\n0.000000\n...\n0.000000\n0.132635\n0.0\n0.0\n0.0\n0.0\n0.000000\n1.0\n3.069914\n214\n\n\n634\n2.0\n32.0\n1.0\n1.0\n26.175431\n0.0\n0.0\n1.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n0.328739\n550\n\n\n720\n3.0\n2.0\n0.0\n1.0\n12.870792\n0.0\n0.0\n1.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n0.156506\n266\n\n\n886\n1.0\n41.0\n0.0\n0.0\n79.085888\n0.0\n1.0\n0.000000\n29.397383\n0.000000\n...\n0.000000\n0.279477\n0.0\n0.0\n0.0\n0.0\n0.000000\n1.0\n0.645736\n101\n\n\n881\n1.0\n11.0\n1.0\n1.0\n147.110861\n0.0\n0.0\n0.000000\n24.788982\n0.000000\n...\n1.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n1.0\n3.136116\n372\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n635\n2.0\n1.0\n1.0\n1.0\n18.613431\n0.0\n0.0\n1.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n0.311295\n639\n\n\n789\n1.0\n13.0\n0.0\n1.0\n79.786677\n0.0\n0.0\n0.000000\n20.720365\n0.000000\n...\n0.000000\n0.531185\n0.0\n0.0\n0.0\n0.0\n0.000000\n1.0\n0.378668\n429\n\n\n702\n2.0\n24.0\n1.0\n1.0\n65.049929\n0.0\n0.0\n0.968794\n0.062412\n0.968794\n...\n0.031206\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.968794\n1.0\n0.565451\n282\n\n\n755\n2.0\n31.0\n0.0\n0.0\n13.000000\n0.0\n0.0\n1.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n0.314695\n301\n\n\n890\n3.0\n4.0\n0.0\n1.0\n22.490998\n0.0\n0.0\n1.000000\n0.000000\n1.000000\n...\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n1.0\n0.677551\n160\n\n\n\n\n150 rows × 27 columns\n\n\n\n\n\n# Add synthetic data for positive class (class 1) to real data\n# We'll make a separate copy of the original dataframe with the new synthetic\n# data points added, keeping our original data intact.\naugmented_data = pd.concat([data, synth_df])\n\n# We'll also get rid of the two columns we added -\n# distance_to_closest_real and closest_X_real_row_index as we do not want these\n# to be used in a Logistic Regression model.\naugmented_data.drop('distance_to_closest_real', axis=1, inplace=True)\naugmented_data.drop('closest_X_real_row_index', axis=1, inplace=True)\n\n\n# Let's have a look at our new dataframe\naugmented_data\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nAgeImputed\nEmbarkedImputed\nCabinLetterImputed\nCabinNumber\n...\nEmbarked_missing\nCabinLetter_A\nCabinLetter_B\nCabinLetter_C\nCabinLetter_D\nCabinLetter_E\nCabinLetter_F\nCabinLetter_G\nCabinLetter_T\nCabinLetter_missing\n\n\n\n\n0\n0.0\n3.0\n22.0\n1.0\n0.0\n7.250000\n0.0\n0.0\n1.000000\n0.000000\n...\n0.0\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n1\n1.0\n1.0\n38.0\n1.0\n0.0\n71.283300\n0.0\n0.0\n0.000000\n85.000000\n...\n0.0\n0.000000\n0.0\n1.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n2\n1.0\n3.0\n26.0\n0.0\n0.0\n7.925000\n0.0\n0.0\n1.000000\n0.000000\n...\n0.0\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n3\n1.0\n1.0\n35.0\n1.0\n0.0\n53.100000\n0.0\n0.0\n0.000000\n123.000000\n...\n0.0\n0.000000\n0.0\n1.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n4\n0.0\n3.0\n35.0\n0.0\n0.0\n8.050000\n0.0\n0.0\n1.000000\n0.000000\n...\n0.0\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n635\n1.0\n2.0\n1.0\n1.0\n1.0\n18.613431\n0.0\n0.0\n1.000000\n0.000000\n...\n0.0\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n789\n1.0\n1.0\n13.0\n0.0\n1.0\n79.786677\n0.0\n0.0\n0.000000\n20.720365\n...\n0.0\n0.468815\n0.0\n0.000000\n0.531185\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n702\n1.0\n2.0\n24.0\n1.0\n1.0\n65.049929\n0.0\n0.0\n0.968794\n0.062412\n...\n0.0\n0.000000\n0.0\n0.031206\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.968794\n\n\n755\n1.0\n2.0\n31.0\n0.0\n0.0\n13.000000\n0.0\n0.0\n1.000000\n0.000000\n...\n0.0\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n890\n1.0\n3.0\n4.0\n0.0\n1.0\n22.490998\n0.0\n0.0\n1.000000\n0.000000\n...\n0.0\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n1.000000\n\n\n\n\n1041 rows × 25 columns\n\n\n\n\n\n# Let's also check that the class splits are as expected\nnumber_positive_class = np.sum(augmented_data['Survived'] == 1)\nnumber_negative_class = np.sum(augmented_data['Survived'] == 0)\n\nprint (f\"Positives : {number_positive_class}\")\nprint (f\"Negatives : {number_negative_class}\")\n\nPositives : 492\nNegatives : 549\n\n\n\nX_aug = augmented_data.drop('Survived',axis=1) # X = all 'data' except the 'stroke' column\ny_aug = augmented_data['Survived'] # y = 'stroke' column from 'data'\nX_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_aug, y_aug, test_size = 0.25, random_state=42)\nX_train_std_aug, X_test_std_aug = standardise_data(X_train_aug, X_test_aug)",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Imbalanced Data (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_k_fold_validation.html",
    "href": "4j_k_fold_validation.html",
    "title": "30  K-fold validation (Titanic Dataset)",
    "section": "",
    "text": "31 Prediction\nWe can still create predictions when using cross validation.\ny_pred_10 = cross_val_predict(\n    model,\n    X_train_val,\n    y_train_val,\n    cv=10,\n)\n\ny_pred_5 = cross_val_predict(\n    model,\n    X_train_val,\n    y_train_val,\n    cv=5,\n)\nThis allows us to create outputs like confusion matrices.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n\nconfusion_matrix_10 = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_train_val,\n        y_pred=y_pred_10\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n\n)\n\nconfusion_matrix_10.plot(ax=ax1)\nax1.set_title(\"10-fold CV\")\n\nconfusion_matrix_5 = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_train_val,\n        y_pred=y_pred_5\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n\n)\n\nconfusion_matrix_5.plot(ax=ax2)\nax2.set_title(\"5-fold CV\")\n\nText(0.5, 1.0, '5-fold CV')\nNote the slight variation in performance here.\nWe can also generate performance reports.\npd.DataFrame(\n    classification_report(y_train_val, y_pred_5, output_dict=True)\n).round(3)\n\n\n\n\n\n\n\n\n\n0.0\n1.0\naccuracy\nmacro avg\nweighted avg\n\n\n\n\nprecision\n0.844\n0.764\n0.815\n0.804\n0.813\n\n\nrecall\n0.863\n0.735\n0.815\n0.799\n0.815\n\n\nf1-score\n0.853\n0.749\n0.815\n0.801\n0.814\n\n\nsupport\n444.000\n268.000\n0.815\n712.000\n712.000",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-fold validation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_k_fold_validation.html#leave-one-out",
    "href": "4j_k_fold_validation.html#leave-one-out",
    "title": "30  K-fold validation (Titanic Dataset)",
    "section": "32.1 Leave-one-out",
    "text": "32.1 Leave-one-out\nAn extreme version of cross validation is leave-one-out validation, where only one datapoint at a time is used for evaluating the model, and the rest is used as training data.\nThis can be useful for very small datasets, but is more computationally intensive.\nLeave-one-out tends to give us a more realistic idea of the performance in the real world, but can also lead to higher variance.\n\nscores = cross_validate(\n    model, X_train_val, y_train_val,\n    scoring=['accuracy', 'f1', 'roc_auc', 'precision_macro', 'recall_macro'],\n    n_jobs=-1,\n    cv=KFold(len(X_train_val)-1)\n    )\n\nscores_df_leave_one_out = pd.DataFrame(scores)\n\nscores_df_leave_one_out\n\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_accuracy\ntest_f1\ntest_roc_auc\ntest_precision_macro\ntest_recall_macro\n\n\n\n\n0\n1.777401\n2.717442\n1.0\n0.0\nNaN\n1.0\n1.0\n\n\n1\n3.770272\n2.818617\n0.0\n0.0\nNaN\n0.0\n0.0\n\n\n2\n3.055812\n0.985970\n1.0\n0.0\nNaN\n1.0\n1.0\n\n\n3\n4.052561\n1.256033\n1.0\n0.0\nNaN\n1.0\n1.0\n\n\n4\n3.003047\n2.907689\n0.0\n0.0\nNaN\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n706\n0.062687\n0.026082\n1.0\n1.0\nNaN\n1.0\n1.0\n\n\n707\n0.117362\n0.037119\n1.0\n0.0\nNaN\n1.0\n1.0\n\n\n708\n0.160492\n0.026065\n1.0\n0.0\nNaN\n1.0\n1.0\n\n\n709\n0.156961\n0.022564\n1.0\n1.0\nNaN\n1.0\n1.0\n\n\n710\n0.063164\n0.018045\n1.0\n0.0\nNaN\n1.0\n1.0\n\n\n\n\n711 rows × 7 columns\n\n\n\n\n\ny_pred_leave_one_out = cross_val_predict(\n    model,\n    X_train_val,\n    y_train_val,\n    cv=KFold(len(X_train_val)-1),\n)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n\nconfusion_matrix_10 = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_train_val,\n        y_pred=y_pred_leave_one_out\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n\n)\n\nconfusion_matrix_10.plot(ax=ax1)\nax1.set_title(\"Leave One Out\")\n\nconfusion_matrix_5 = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(\n        y_true=y_train_val,\n        y_pred=y_pred_5\n        ),\n        display_labels=[\"Died\", \"Survived\"]\n\n)\n\nconfusion_matrix_5.plot(ax=ax2)\nax2.set_title(\"5-fold CV\")\n\nText(0.5, 1.0, '5-fold CV')\n\n\n\n\n\n\n\n\n\nWe can also generate other reports here.\n\npd.DataFrame(\n    classification_report(y_train_val, y_pred_leave_one_out,\n                          output_dict=True)\n)\n\n\n\n\n\n\n\n\n\n0.0\n1.0\naccuracy\nmacro avg\nweighted avg\n\n\n\n\nprecision\n0.839912\n0.761719\n0.811798\n0.800816\n0.810480\n\n\nrecall\n0.862613\n0.727612\n0.811798\n0.795112\n0.811798\n\n\nf1-score\n0.851111\n0.744275\n0.811798\n0.797693\n0.810897\n\n\nsupport\n444.000000\n268.000000\n0.811798\n712.000000\n712.000000",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-fold validation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_missing_data_imputation.html",
    "href": "4j_missing_data_imputation.html",
    "title": "31  Imputation (Titanic Dataset)",
    "section": "",
    "text": "31.1 Some initial preprocessing of the raw data prior to imputation\nLet’s take a look at a sample of the data.\ndata.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nLet’s look at some of the column counts, passing in dropna=False to ensure any missing values (if any) are included in counts.\ndata['SibSp'].value_counts(dropna=False)\n\n0    608\n1    209\n2     28\n4     18\n3     16\n8      7\n5      5\nName: SibSp, dtype: int64\nWe can also get a summary of missing values.\npd.DataFrame(\n    data.isna().mean().round(4),\n    columns=[\"Percentage of Values Missing\"]\n    )\n\n\n\n\n\n\n\n\n\nPercentage of Values Missing\n\n\n\n\nPassengerId\n0.0000\n\n\nSurvived\n0.0000\n\n\nPclass\n0.0000\n\n\nName\n0.0000\n\n\nSex\n0.0000\n\n\nAge\n0.1987\n\n\nSibSp\n0.0000\n\n\nParch\n0.0000\n\n\nTicket\n0.0000\n\n\nFare\n0.0000\n\n\nCabin\n0.7710\n\n\nEmbarked\n0.0022\nLet’s do a couple of preprocessing steps.\n###############################\n# Replace unclear data values #\n###############################\nembarked_lookup = {\n    'S': 'Southampton',\n    'C':  'Cherbourg',\n    'Q': 'Queenstown'\n}\n\n# Note that 'get' defaults to 'None' if the key doesn't appear in the dictionary lookup.\ndata['Embarked'] = data['Embarked'].apply(lambda row_value: embarked_lookup.get(row_value))\n\ndata['Embarked'].value_counts(dropna=False)\n\nSouthampton    644\nCherbourg      168\nQueenstown      77\nNone             2\nName: Embarked, dtype: int64\n#######################\n# One hot encoding    #\n#######################\n# here we've asked for an additional column when no value is recorded for the embarkation point\none_hot = pd.get_dummies(data['Embarked'], prefix='Embarked', dummy_na=True).astype('int')\n# Drop the column as it is now encoded\ndata = data.drop('Embarked', axis = 1)\n# Join the encoded df\ndata = data.join(one_hot, how=\"left\")\nWe can now just confirm that the number of people missing agree with our dataset from before.\ndata.loc[:, data.columns.str.startswith('Embarked')].sum(axis=0)\n\nEmbarked_Cherbourg      168\nEmbarked_Queenstown      77\nEmbarked_Southampton    644\nEmbarked_nan              2\ndtype: int64\nNow let’s deal with our remaining preprocessing steps.\nNotice that here we don’t do anything to values that are neither male nor female so they will remain as their original value.\nFrom our initial checks, we know that in this case no values were missing in the Sex column anyway.\n#######################\n# Dichotomous columns #\n#######################\ndata['Sex'].replace('male', 1, inplace=True)\ndata['Sex'].replace('female', 0, inplace=True)\ndata = data.rename(columns={'Sex': 'IsMale'})\nNow let’s tidy up and display our final output.\n#####################################\n# Tidying up remaining column names #\n#####################################\ndata = data.drop(columns=['Name', 'Ticket', 'Cabin'])\ndata = data.rename(columns={'Embarked_nan':'Embarked_Unknown'})\n\ndata.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n0\n1\n0\n3\n1\n22.0\n1\n0\n7.2500\n0\n0\n1\n0\n\n\n1\n2\n1\n1\n0\n38.0\n1\n0\n71.2833\n1\n0\n0\n0\n\n\n2\n3\n1\n3\n0\n26.0\n0\n0\n7.9250\n0\n0\n1\n0\n\n\n3\n4\n1\n1\n0\n35.0\n1\n0\n53.1000\n0\n0\n1\n0\n\n\n4\n5\n0\n3\n1\n35.0\n0\n0\n8.0500\n0\n0\n1\n0\nNow let’s get our data ready for machine learning.\ndata = data.astype(float)\n\n# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n# We drop passenger ID as it is not original data\n\ndata.drop('PassengerId', inplace=True, axis=1)\n\nX = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\ny = data['Survived'] # y = 'survived' column from 'data'\n\nfeature_names = X.columns.tolist()\nFinally, let’s check we haven’t lost or gained any rows!\n(if we were automating this, we may turn this into a hard-coded test that will cause the notebook/script to fail if the dataset is no longer the same size as the original dataset).\nprint(f\"Total number of rows in raw dataset: {len(data)}\")\n\nTotal number of rows in raw dataset: 891",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Imputation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_missing_data_imputation.html#checking-for-missing-values",
    "href": "4j_missing_data_imputation.html#checking-for-missing-values",
    "title": "31  Imputation (Titanic Dataset)",
    "section": "31.2 Checking for Missing Values",
    "text": "31.2 Checking for Missing Values\nLet’s just remind ourselves of our columns and their missing data.\n\npd.DataFrame(\n    (data.isna().mean()*100).round(2),\n    columns=[\"Percentage of Values Missing\"]\n    )\n\n\n\n\n\n\n\n\n\nPercentage of Values Missing\n\n\n\n\nSurvived\n0.00\n\n\nPclass\n0.00\n\n\nIsMale\n0.00\n\n\nAge\n19.87\n\n\nSibSp\n0.00\n\n\nParch\n0.00\n\n\nFare\n0.00\n\n\nEmbarked_Cherbourg\n0.00\n\n\nEmbarked_Queenstown\n0.00\n\n\nEmbarked_Southampton\n0.00\n\n\nEmbarked_Unknown\n0.00\n\n\n\n\n\n\n\n\nWe could choose to impute the embarkation as well; however, in this case, as it’s a low number of samples we will just choose to stick with that being unknown.",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Imputation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_missing_data_imputation.html#a-note-on-the-order-of-steps",
    "href": "4j_missing_data_imputation.html#a-note-on-the-order-of-steps",
    "title": "31  Imputation (Titanic Dataset)",
    "section": "31.3 A note on the order of steps",
    "text": "31.3 A note on the order of steps\nTo avoid leakage - where information from the testing data, like the distribution of data, influences your training data - you should perform imputation steps after splitting your data into training and testing datasets.\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_validate, y_train, y_validate = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n\nprint(f\"Training Dataset Samples: {len(X_train)}\")\nprint(f\"Validation Dataset Samples: {len(X_validate)}\")\nprint(f\"Testing Dataset Samples: {len(X_test)}\")\n\nTraining Dataset Samples: 569\nValidation Dataset Samples: 143\nTesting Dataset Samples: 179\n\n\nThis is why imputing categorical columns can complicate things slightly - you need to do the split, but then you will need to apply some transformations - like one-hot encoding - to each dataset separately.\nGood use of functions can ensure this is a relatively painless process - but is not one we’ll cover today.",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Imputation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_missing_data_imputation.html#exploring-the-distribution-prior-to-imputation",
    "href": "4j_missing_data_imputation.html#exploring-the-distribution-prior-to-imputation",
    "title": "31  Imputation (Titanic Dataset)",
    "section": "31.4 Exploring the distribution prior to imputation",
    "text": "31.4 Exploring the distribution prior to imputation\nAs we’re just looking at age today, let’s take a look at that column and some statistics.\n\nX_train['Age'].mean()\n\n28.99313043478261\n\n\n\nX_train['Age'].median()\n\n28.0\n\n\n\nX_train['Age'].mode()\n\n0    24.0\nName: Age, dtype: float64\n\n\n\nX_train['Age'].hist()\n\n\n\n\n\n\n\n\nLet’s find the missing values and pull them out so we can track how they change.\n\nindices_missing_age = X_train[X_train['Age'].isna()].index\nindices_missing_age\n\nInt64Index([517, 792, 420,  95, 495, 384, 159, 301, 826, 274,\n            ...\n            468, 470,  64, 223, 564, 284, 888,  19, 295, 214],\n           dtype='int64', length=109)",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Imputation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_missing_data_imputation.html#simple-imputation",
    "href": "4j_missing_data_imputation.html#simple-imputation",
    "title": "31  Imputation (Titanic Dataset)",
    "section": "31.5 Simple Imputation",
    "text": "31.5 Simple Imputation\n\n31.5.1 Mean Imputation\nFirst, we’ll just show how to replace every missing value in ‘Age’ with the mean value for age using the SimpleImputer.\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_train_mean_imp = imp_mean.fit_transform(X_train)\n\nX_train_mean_imp\n\narray([[ 3.        ,  1.        , 28.99313043, ...,  1.        ,\n         0.        ,  0.        ],\n       [ 3.        ,  0.        , 28.99313043, ...,  0.        ,\n         1.        ,  0.        ],\n       [ 2.        ,  0.        , 33.        , ...,  0.        ,\n         1.        ,  0.        ],\n       ...,\n       [ 2.        ,  1.        , 29.        , ...,  0.        ,\n         1.        ,  0.        ],\n       [ 3.        ,  0.        , 27.        , ...,  0.        ,\n         1.        ,  0.        ],\n       [ 3.        ,  1.        , 20.        , ...,  0.        ,\n         1.        ,  0.        ]])\n\n\nHowever, when we do this, we find that our output is now a numpy array instead of a dataframe.\nWe can use the following notation to avoid this issue.\n\nX_train_mean_imp = X_train.copy()\nX_train_mean_imp.values[:] = SimpleImputer().fit_transform(X_train)\nX_train_mean_imp\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n28.99313\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n28.99313\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n472\n2.0\n0.0\n33.00000\n1.0\n2.0\n27.7500\n0.0\n0.0\n1.0\n0.0\n\n\n483\n3.0\n0.0\n63.00000\n0.0\n0.0\n9.5875\n0.0\n0.0\n1.0\n0.0\n\n\n9\n2.0\n0.0\n14.00000\n1.0\n0.0\n30.0708\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n79\n3.0\n0.0\n30.00000\n0.0\n0.0\n12.4750\n0.0\n0.0\n1.0\n0.0\n\n\n164\n3.0\n1.0\n1.00000\n4.0\n1.0\n39.6875\n0.0\n0.0\n1.0\n0.0\n\n\n117\n2.0\n1.0\n29.00000\n1.0\n0.0\n21.0000\n0.0\n0.0\n1.0\n0.0\n\n\n8\n3.0\n0.0\n27.00000\n0.0\n2.0\n11.1333\n0.0\n0.0\n1.0\n0.0\n\n\n131\n3.0\n1.0\n20.00000\n0.0\n0.0\n7.0500\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n569 rows × 10 columns\n\n\n\n\nLet’s see what this has done to our data distribution.\n\nX_train_mean_imp['Age'].hist()\n\n\n\n\n\n\n\n\nThis looks quite different to our original distribution.\n\nX_train['Age'].hist()\n\n\n\n\n\n\n\n\nLet’s look at the individual rows.\n\nX_train_mean_imp.loc[indices_missing_age]\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n28.99313\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n28.99313\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n420\n3.0\n1.0\n28.99313\n0.0\n0.0\n7.8958\n1.0\n0.0\n0.0\n0.0\n\n\n95\n3.0\n1.0\n28.99313\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n0.0\n\n\n495\n3.0\n1.0\n28.99313\n0.0\n0.0\n14.4583\n1.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\n1.0\n1.0\n28.99313\n0.0\n0.0\n26.0000\n0.0\n0.0\n1.0\n0.0\n\n\n888\n3.0\n0.0\n28.99313\n1.0\n2.0\n23.4500\n0.0\n0.0\n1.0\n0.0\n\n\n19\n3.0\n0.0\n28.99313\n0.0\n0.0\n7.2250\n1.0\n0.0\n0.0\n0.0\n\n\n295\n1.0\n1.0\n28.99313\n0.0\n0.0\n27.7208\n1.0\n0.0\n0.0\n0.0\n\n\n214\n3.0\n1.0\n28.99313\n1.0\n0.0\n7.7500\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n109 rows × 10 columns\n\n\n\n\nWe can see that they all have the same value!\n\nX_train_mean_imp.loc[indices_missing_age]['Age'].value_counts()\n\n28.99313    109\nName: Age, dtype: int64\n\n\nWe should round this to the nearest integer to match the original data.\n\nX_train_mean_imp['Age'] = X_train_mean_imp['Age'].round(0)\nX_train_mean_imp.head()\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n29.0\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n29.0\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n472\n2.0\n0.0\n33.0\n1.0\n2.0\n27.7500\n0.0\n0.0\n1.0\n0.0\n\n\n483\n3.0\n0.0\n63.0\n0.0\n0.0\n9.5875\n0.0\n0.0\n1.0\n0.0\n\n\n9\n2.0\n0.0\n14.0\n1.0\n0.0\n30.0708\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n31.5.2 Function: Simple Imputation\nWe could turn this into a function to make it quicker to carry out.\n\ndef impute_missing_df(df, impute_type=\"mean\"):\n    imputed_df = df.copy()\n    imputed_df.values[:] = SimpleImputer(missing_values=np.nan, strategy=impute_type).fit_transform(df)\n    return imputed_df\n\n\n\n31.5.3 Mode\nLet’s now impute using the modal (most common) value.\n\nX_train_mode_imp = impute_missing_df(X_train, impute_type=\"most_frequent\")\n\n\nX_train_mode_imp.loc[indices_missing_age].head()\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n24.0\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n24.0\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n420\n3.0\n1.0\n24.0\n0.0\n0.0\n7.8958\n1.0\n0.0\n0.0\n0.0\n\n\n95\n3.0\n1.0\n24.0\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n0.0\n\n\n495\n3.0\n1.0\n24.0\n0.0\n0.0\n14.4583\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n31.5.4 Median\n\nX_train_median_imp = impute_missing_df(X_train, impute_type=\"median\")\nX_train_median_imp.loc[indices_missing_age].head()\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n28.0\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n28.0\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n420\n3.0\n1.0\n28.0\n0.0\n0.0\n7.8958\n1.0\n0.0\n0.0\n0.0\n\n\n95\n3.0\n1.0\n28.0\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n0.0\n\n\n495\n3.0\n1.0\n28.0\n0.0\n0.0\n14.4583\n1.0\n0.0\n0.0\n0.0",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Imputation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_missing_data_imputation.html#iterative-imputation",
    "href": "4j_missing_data_imputation.html#iterative-imputation",
    "title": "31  Imputation (Titanic Dataset)",
    "section": "31.6 Iterative Imputation",
    "text": "31.6 Iterative Imputation\nIn the documentation, IterativeImputer is described as a “Multivariate imputer that estimates each feature from all the others.\nA strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.”\nLet’s go straight to writing a function for this to make it easier to apply.\n**kwargs is a special parameter in python. Here, we’re just using it to pass any number of arguments directly through to the IterativeImputer function so we don’t have to explicitly specify all of the different arguments we might want to allow someone to pass through to the IterativeImputer.\n\ndef impute_missing_df_iterative(df, **kwargs):\n    imputed_df = df.copy()\n    imputed_df.values[:] = IterativeImputer(**kwargs).fit_transform(df)\n    return imputed_df\n\n\nX_train_iterative_imp = impute_missing_df_iterative(\n    X_train,\n    missing_values=np.nan,\n    random_state=42\n    )\nX_train_iterative_imp.loc[indices_missing_age].head()\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n26.458273\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n-4.786686\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n420\n3.0\n1.0\n26.157474\n0.0\n0.0\n7.8958\n1.0\n0.0\n0.0\n0.0\n\n\n95\n3.0\n1.0\n27.785451\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n0.0\n\n\n495\n3.0\n1.0\n26.065750\n0.0\n0.0\n14.4583\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nStraight away we can see a slight problem - someone with an age of minus 4!\nWe can fix that by passing in the IterativeImputer argument min_value.\n\nX_train_iterative_imp = impute_missing_df_iterative(\n    X_train,\n    min_value=0,\n    missing_values=np.nan,\n    random_state=42\n    )\n\nX_train_iterative_imp.head()\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n26.458273\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n0.000000\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n472\n2.0\n0.0\n33.000000\n1.0\n2.0\n27.7500\n0.0\n0.0\n1.0\n0.0\n\n\n483\n3.0\n0.0\n63.000000\n0.0\n0.0\n9.5875\n0.0\n0.0\n1.0\n0.0\n\n\n9\n2.0\n0.0\n14.000000\n1.0\n0.0\n30.0708\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nLet’s just look at the values for those rows that previously had missing values in ‘age’.\n\nX_train_iterative_imp.loc[indices_missing_age].head()\n\n\n\n\n\n\n\n\n\nPclass\nIsMale\nAge\nSibSp\nParch\nFare\nEmbarked_Cherbourg\nEmbarked_Queenstown\nEmbarked_Southampton\nEmbarked_Unknown\n\n\n\n\n517\n3.0\n1.0\n26.458273\n0.0\n0.0\n24.1500\n0.0\n1.0\n0.0\n0.0\n\n\n792\n3.0\n0.0\n0.000000\n8.0\n2.0\n69.5500\n0.0\n0.0\n1.0\n0.0\n\n\n420\n3.0\n1.0\n26.157474\n0.0\n0.0\n7.8958\n1.0\n0.0\n0.0\n0.0\n\n\n95\n3.0\n1.0\n27.785451\n0.0\n0.0\n8.0500\n0.0\n0.0\n1.0\n0.0\n\n\n495\n3.0\n1.0\n26.065750\n0.0\n0.0\n14.4583\n1.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nLet’s compare this new dataframe with the original.\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\nX_train['Age'].hist(ax=ax1, density=True)\nX_train_mean_imp['Age'].hist(ax=ax2, density=True)\nX_train_iterative_imp['Age'].hist(ax=ax3, density=True)\nax1.set_title('Original Column')\nax2.set_title('Mean Imputation')\nax3.set_title('Iterative Imputation')\n\nText(0.5, 1.0, 'Iterative Imputation')\n\n\n\n\n\n\n\n\n\nLet’s overlay the original data distribution to compare the approaches.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\nX_train_mean_imp['Age'].hist(ax=ax1, density=True)\nX_train_iterative_imp['Age'].hist(ax=ax2, density=True)\nX_train['Age'].hist(ax=ax1, alpha=0.3, color=\"red\", density=True)\nX_train['Age'].hist(ax=ax2, alpha=0.3, color=\"red\", density=True)\nax1.set_title('Mean Imputation')\nax2.set_title('Iterative Imputation')\n\nText(0.5, 1.0, 'Iterative Imputation')",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Imputation (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_model_calibration.html",
    "href": "4j_model_calibration.html",
    "title": "32  Checking Model Calibration (Titanic Dataset)",
    "section": "",
    "text": "32.1 Manual Calculation - equal-width bins\nNow we can create a reliability plot, binning cases by their predicted probability fo survival.\n# Bin data with numpy digitize (this will assign a bin to each case)\nstep = 0.10\nbins = np.arange(step, 1+step, step)\ndigitized = np.digitize(y_calibrate_probabilities, bins)\n\n# Put data in DataFrame\nreliability = pd.DataFrame()\nreliability['bin'] = digitized\nreliability['probability'] = y_calibrate_probabilities\nreliability['observed'] = y_validate.values\n\n# Summarise data by bin in new dataframe\nreliability_summary = pd.DataFrame()\n\n# Add bins to summary\nreliability_summary['bin'] = bins\n\n# Calculate mean of predicted probability of survival for each bin\nreliability_summary['confidence'] = \\\n    reliability.groupby('bin').mean()['probability']\n\n# Calculate the proportion of passengers who survive in each bin\nreliability_summary['fraction_positive'] = \\\n    reliability.groupby('bin').mean()['observed']\n\nreliability_summary\n\n\n\n\n\n\n\n\n\nbin\nconfidence\nfraction_positive\n\n\n\n\n0\n0.1\n0.026164\n0.171875\n\n\n1\n0.2\n0.137591\n0.117647\n\n\n2\n0.3\n0.234026\n0.333333\n\n\n3\n0.4\n0.336645\n0.000000\n\n\n4\n0.5\n0.476205\n0.500000\n\n\n5\n0.6\n0.519817\n1.000000\n\n\n6\n0.7\n0.650163\n0.000000\n\n\n7\n0.8\n0.726074\n0.000000\n\n\n8\n0.9\n0.839273\n0.250000\n\n\n9\n1.0\n0.978974\n0.904762\nLet’s now observe this on a plot.\nplt.plot(reliability_summary['confidence'],\n         reliability_summary['fraction_positive'],\n         linestyle='-',\n         marker='o',\n         label='model')\n\nplt.plot([0,1],[0,1],\n         linestyle='--',\n         label='theoretical')\n\nplt.xlabel('Model probability')\nplt.ylabel('Fraction positive')\n\nplt.title('Reliability plot')\n\nplt.grid()\nplt.legend()\nplt.show()",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Checking Model Calibration (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_model_calibration.html#the-sklearn-reliability-plot",
    "href": "4j_model_calibration.html#the-sklearn-reliability-plot",
    "title": "32  Checking Model Calibration (Titanic Dataset)",
    "section": "32.2 The sklearn reliability plot",
    "text": "32.2 The sklearn reliability plot\n\nCalibrationDisplay.from_estimator(model, X_test, y_test)\nplt.show()\n\n\n\n\n\n\n\n\nLet’s compare this for a series of classifiers.\n\nmodel_dt = DecisionTreeClassifier(max_depth=6)\nmodel_rf = RandomForestClassifier(random_state=42, max_depth=6)\nmodel_xgb = XGBClassifier(random_state=42)\nmodel_lr = LogisticRegression()\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 7))\n\n#######################\n# Decision Tree       #\n#######################\n\nCalibrationDisplay.from_estimator(model_dt.fit(X_train, y_train), X_validate, y_validate, ax=ax1)\nax1 = ax1.set_title(\"Decision Tree\")\n\n#######################\n# Random Forest       #\n#######################\n\nCalibrationDisplay.from_estimator(model_rf.fit(X_train, y_train), X_validate, y_validate, ax=ax2)\nax2 = ax2.set_title(\"Random Forest\")\n\n#######################\n# XGBoost             #\n#######################\n\nCalibrationDisplay.from_estimator(model_xgb.fit(X_train, y_train), X_validate, y_validate, ax=ax3)\nax3 = ax3.set_title(\"XGBoost\")\n\n#######################\n# Logistic Regression #\n#######################\n\n# Initialise a new scaling object for normalising input data\nsc = StandardScaler()\n\n# Apply the scaler to the training and test sets\ntrain_std=sc.fit_transform(X_train)\nval_std = sc.fit_transform(X_validate)\ntest_std=sc.fit_transform(X_test)\n\nCalibrationDisplay.from_estimator(model_lr.fit(train_std, y_train), val_std , y_validate, ax=ax4)\nax4 = ax4.set_title(\"Logistic Regression\")\n\n\n\n\n\n\n\n\nRepeat this, plotting them overlaid.\n\nfig, ax = plt.subplots(figsize=(12,7))\nCalibrationDisplay.from_estimator(model_dt.fit(X_train, y_train), X_validate, y_validate, ax=ax)\nCalibrationDisplay.from_estimator(model_rf.fit(X_train, y_train), X_validate, y_validate, ax=ax)\nCalibrationDisplay.from_estimator(model_xgb.fit(X_train, y_train), X_validate, y_validate, ax=ax)\nCalibrationDisplay.from_estimator(model_lr.fit(train_std, y_train), val_std , y_validate, ax=ax)\n\n\n\n\n\n\n\n\n\n32.2.1 Histograms\n\nfig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6, 1, figsize=(7, 15), sharey=True, sharex=True)\n\n#######################\n# Decision Tree       #\n#######################\n\nax1.hist(model_dt.predict_proba(X_validate)[:,1], bins=np.arange(0,1.01,0.1))\nax1 = ax1.set_title(\"Decision Tree\")\n\n#######################\n# Random Forest       #\n#######################\n\nax2.hist(model_rf.predict_proba(X_validate)[:,1], bins=np.arange(0,1.01,0.1))\nax2 = ax2.set_title(\"Random Forest\")\n\n#######################\n# XGBoost             #\n#######################\n\nax3.hist(model_xgb.predict_proba(X_validate)[:,1], bins=np.arange(0,1.01,0.1))\nax3 = ax3.set_title(\"XGBoost\")\n\n#######################\n# Logistic Regression #\n#######################\n\nax4.hist(model_lr.predict_proba(val_std)[:,1], bins=np.arange(0,1.01,0.1))\nax4 = ax4.set_title(\"Logistic Regression\")\n\n#######################\n# KNN                 #\n#######################\n\nax5.hist(KNeighborsClassifier().fit(train_std, y_train).predict_proba(val_std)[:,1], bins=np.arange(0,1.01,0.1))\nax5 = ax5.set_title(\"K Nearest Neighbours\")\n\n#######################\n# Naive Bayes         #\n#######################\n\nax6.hist(GaussianNB().fit(X_train, y_train).predict_proba(X_validate)[:,1], bins=np.arange(0,1.01,0.1))\nax6 = ax6.set_title(\"Gaussian Naive Bayes\")",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Checking Model Calibration (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_pipelines.html",
    "href": "4j_pipelines.html",
    "title": "33  Pipelines (Titanic Dataset)",
    "section": "",
    "text": "33.1 Increasing the complexity of our pipe\nLet’s add in another step!\nknn_classifier = KNeighborsClassifier()\nsfs = SequentialFeatureSelector(knn_classifier,\n                                n_features_to_select='auto',\n                                tol=.01,\n                                n_jobs=-1)\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('feature_selection', sfs),\n    ('classifier', knn_classifier)\n])\npipe\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('feature_selection',\n                 SequentialFeatureSelector(estimator=KNeighborsClassifier(),\n                                           n_jobs=-1, tol=0.01)),\n                ('classifier', KNeighborsClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('scaler', StandardScaler()),\n                ('feature_selection',\n                 SequentialFeatureSelector(estimator=KNeighborsClassifier(),\n                                           n_jobs=-1, tol=0.01)),\n                ('classifier', KNeighborsClassifier())])  StandardScaler?Documentation for StandardScalerStandardScaler()  feature_selection: SequentialFeatureSelector?Documentation for feature_selection: SequentialFeatureSelectorSequentialFeatureSelector(estimator=KNeighborsClassifier(), n_jobs=-1, tol=0.01) estimator: KNeighborsClassifierKNeighborsClassifier()  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier()  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier()\npipe.fit(X_train, y_train)\nprint(f'Training set score: {pipe.score(X_train,y_train):.3f}')\nprint(f'Test set score: {pipe.score(X_test,y_test):.3f}')\n\nTraining set score: 0.787\nTest set score: 0.788",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Pipelines (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_pipelines.html#ensembles-with-pipelines",
    "href": "4j_pipelines.html#ensembles-with-pipelines",
    "title": "33  Pipelines (Titanic Dataset)",
    "section": "33.2 Ensembles with pipelines",
    "text": "33.2 Ensembles with pipelines\nEnsembles can easily be added in to the process as well - they are effectively just a classifier like a single model once we have set them up.\n\nvoting_classifier_1 = VotingClassifier(\n    estimators=[('knn', KNeighborsClassifier()),\n                ('logreg', LogisticRegression())],\n    voting='soft')\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', voting_classifier_1)\n])\n\npipe.fit(X_train, y_train)\nprint(f'Training set score: {pipe.score(X_train,y_train):.3f}')\nprint(f'Test set score: {pipe.score(X_test,y_test):.3f}')\n\nTraining set score: 0.851\nTest set score: 0.810",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Pipelines (Titanic Dataset)</span>"
    ]
  },
  {
    "objectID": "4j_pipelines.html#grid-search-with-pipelines",
    "href": "4j_pipelines.html#grid-search-with-pipelines",
    "title": "33  Pipelines (Titanic Dataset)",
    "section": "33.3 Grid search with pipelines",
    "text": "33.3 Grid search with pipelines\nTo use grid search with our pipeline, we just need to add in the step name we defined with a double underscore before each parameter.\n\nknn_classifier = KNeighborsClassifier()\nsfs = SequentialFeatureSelector(knn_classifier,\n                                direction=\"backward\",\n                                n_jobs=-1)\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('feature_selection', sfs),\n    ('classifier', knn_classifier)\n])\n\nparameter_grid = {\n    \"feature_selection__n_features_to_select\": [18, 20],\n    \"classifier__n_neighbors\": [i for i in range(1,10, 2)],\n    \"classifier__metric\": [\"minowski\", \"manhattan\", \"euclidean\"]\n}\n\n\nrandom_search = GridSearchCV(\n    estimator=pipe, # notice that we're passing our pipeline in here\n    param_grid=parameter_grid,\n    n_jobs=1, # If n_jobs is not one, you won't get the progress report during the process\n    verbose=2, # this controls the level of detail being output\n)\n\nrandom_search.fit(X_train, y_train)\n\n\nprint(\"Best parameters combination found:\")\nbest_parameters = random_search.best_estimator_.get_params()\nfor param_name in sorted(parameter_grid.keys()):\n    print(f\"{param_name}: {best_parameters[param_name]}\")\n\nBest parameters combination found:\nclassifier__metric: euclidean\nclassifier__n_neighbors: 3\nfeature_selection__n_features_to_select: 20\n\n\n\ntest_accuracy = random_search.score(X_test, y_test)\nprint(\n    \"Accuracy of the best parameters using the inner CV of \"\n    f\"the random search: {random_search.best_score_:.3f}\"\n)\nprint(f\"Accuracy on test set: {test_accuracy:.3f}\")\n\nAccuracy of the best parameters using the inner CV of the random search: 0.805\nAccuracy on test set: 0.771",
    "crumbs": [
      "4J - Model Optimisation",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Pipelines (Titanic Dataset)</span>"
    ]
  }
]